nohup: ignoring input
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
10
Epoch: 0, Loss: 237.86262208869658, Learning Rate: 0.0005
Epoch: 1, Loss: 191.1064939843603, Learning Rate: 0.0005
Mean: 0.0720386324022879, Median: 0.02010007800788882, Num: 111
Epoch: 2, Loss: 173.45167688576572, Learning Rate: 0.0005
Epoch: 3, Loss: 172.66652559946817, Learning Rate: 0.0005
Mean: 0.08761580161540411, Median: 0.038556284817962534, Num: 111
Epoch: 4, Loss: 157.76689409922403, Learning Rate: 0.0005
Epoch: 5, Loss: 157.79573619796568, Learning Rate: 0.0005
Mean: 0.1076589705365769, Median: 0.07011746409654603, Num: 111
Epoch: 6, Loss: 152.72982328483857, Learning Rate: 0.0005
Epoch: 7, Loss: 148.76719890732363, Learning Rate: 0.0005
Mean: 0.13102887119358142, Median: 0.09888402432696795, Num: 111
Epoch: 8, Loss: 132.18396827973515, Learning Rate: 0.0005
Epoch: 9, Loss: 140.5053504576166, Learning Rate: 0.0005
Mean: 0.14810329880470854, Median: 0.10547294369298883, Num: 111
Epoch: 10, Loss: 132.93826753547393, Learning Rate: 0.0005
Epoch: 11, Loss: 135.59278129669556, Learning Rate: 0.0005
Mean: 0.1681162970642815, Median: 0.14741306923790823, Num: 111
Epoch: 12, Loss: 133.15542221069336, Learning Rate: 0.0005
Epoch: 13, Loss: 126.73675959943289, Learning Rate: 0.0005
Mean: 0.16157960915120154, Median: 0.13599226243408108, Num: 111
Epoch: 14, Loss: 130.37964557739625, Learning Rate: 0.0005
Epoch: 15, Loss: 121.97700822209737, Learning Rate: 0.0005
Mean: 0.18190319695158375, Median: 0.16401054554230726, Num: 111
Epoch: 16, Loss: 123.58515668202595, Learning Rate: 0.0005
Epoch: 17, Loss: 119.85226923011872, Learning Rate: 0.0005
Mean: 0.18243098156877702, Median: 0.1613329051160117, Num: 111
Epoch: 18, Loss: 124.89220426168787, Learning Rate: 0.0005
Epoch: 19, Loss: 110.12243969469185, Learning Rate: 0.0005
Mean: 0.18082989784833314, Median: 0.1555036526459266, Num: 111
Epoch: 20, Loss: 111.93771197996944, Learning Rate: 0.0005
Epoch: 21, Loss: 106.67752091281385, Learning Rate: 0.0005
Mean: 0.19448920676447087, Median: 0.17479263492077216, Num: 111
Epoch: 22, Loss: 112.81150928175593, Learning Rate: 0.0005
Epoch: 23, Loss: 108.65200810260083, Learning Rate: 0.0005
Mean: 0.19131725337879688, Median: 0.16720260216539365, Num: 111
Epoch: 24, Loss: 108.19317544224751, Learning Rate: 0.0005
Epoch: 25, Loss: 105.2359458509698, Learning Rate: 0.0005
Mean: 0.19330522928226862, Median: 0.17109766616698205, Num: 111
Epoch: 26, Loss: 103.13455142745052, Learning Rate: 0.0005
Epoch: 27, Loss: 102.65364594057382, Learning Rate: 0.0005
Mean: 0.19484763529063004, Median: 0.1803121178794537, Num: 111
Epoch: 28, Loss: 104.13257166850998, Learning Rate: 0.0005
Epoch: 29, Loss: 108.62129093078245, Learning Rate: 0.0005
Mean: 0.19426546120193444, Median: 0.1638334373800908, Num: 111
Epoch: 30, Loss: 99.47670017380312, Learning Rate: 0.0005
Epoch: 31, Loss: 100.57928924100945, Learning Rate: 0.0005
Mean: 0.2006266391349794, Median: 0.16766686004830453, Num: 111
Epoch: 32, Loss: 97.09555382326425, Learning Rate: 0.0005
Epoch: 33, Loss: 98.41651723471033, Learning Rate: 0.0005
Mean: 0.21016295282942943, Median: 0.1844544318949015, Num: 111
Epoch: 34, Loss: 95.5266577019749, Learning Rate: 0.0005
Epoch: 35, Loss: 91.52760333325489, Learning Rate: 0.0005
Mean: 0.197123069208581, Median: 0.17108816687459402, Num: 111
Epoch: 36, Loss: 103.40165071602327, Learning Rate: 0.0005
Epoch: 37, Loss: 93.6431976686041, Learning Rate: 0.0005
Mean: 0.20298575587633225, Median: 0.19102775098074368, Num: 111
Epoch: 38, Loss: 89.7332832841988, Learning Rate: 0.0005
Epoch: 39, Loss: 92.11043704848692, Learning Rate: 0.0005
Mean: 0.20668010796491257, Median: 0.18470053953298987, Num: 111
Epoch: 40, Loss: 100.29228141508906, Learning Rate: 0.0005
Epoch: 41, Loss: 97.54935536901635, Learning Rate: 0.0005
Mean: 0.21216022426349632, Median: 0.19782333305509, Num: 111
Epoch: 42, Loss: 90.31649692949043, Learning Rate: 0.0005
Epoch: 43, Loss: 88.4102170553552, Learning Rate: 0.0005
Mean: 0.2072812292560347, Median: 0.1908462438336446, Num: 111
Epoch: 44, Loss: 92.09287222896714, Learning Rate: 0.0005
Epoch: 45, Loss: 85.6428801295269, Learning Rate: 0.0005
Mean: 0.20970876479162756, Median: 0.18682409966553093, Num: 111
Epoch: 46, Loss: 88.22279856578413, Learning Rate: 0.0005
Epoch: 47, Loss: 87.11869563826595, Learning Rate: 0.0005
Mean: 0.2196883529133636, Median: 0.20318597792830392, Num: 111
Epoch: 48, Loss: 90.26157066620976, Learning Rate: 0.0005
Epoch: 49, Loss: 90.0723527885345, Learning Rate: 0.0005
Mean: 0.21811303437962698, Median: 0.19556124352230325, Num: 111
Epoch: 50, Loss: 90.5442885663136, Learning Rate: 0.0005
Epoch: 51, Loss: 87.85100680661489, Learning Rate: 0.0005
Mean: 0.21662875870959217, Median: 0.20836697614631805, Num: 111
Epoch: 52, Loss: 91.19763652387871, Learning Rate: 0.0005
Epoch: 53, Loss: 81.42955387069519, Learning Rate: 0.0005
Mean: 0.22129865258559003, Median: 0.22342667695489118, Num: 111
Epoch: 54, Loss: 84.62173418251865, Learning Rate: 0.0005
Epoch: 55, Loss: 88.75465264377824, Learning Rate: 0.0005
Mean: 0.22212997411381794, Median: 0.21992154011184262, Num: 111
Epoch: 56, Loss: 81.87821505443159, Learning Rate: 0.0005
Epoch: 57, Loss: 85.36755869762007, Learning Rate: 0.0005
Mean: 0.22918149020247175, Median: 0.21373891070012346, Num: 111
Epoch: 58, Loss: 85.43356774801231, Learning Rate: 0.0005
Epoch: 59, Loss: 89.50971618330622, Learning Rate: 0.0005
Mean: 0.21675891270465314, Median: 0.21722126102290304, Num: 111
Epoch: 60, Loss: 84.0207989474377, Learning Rate: 0.0005
Epoch: 61, Loss: 82.76353546510259, Learning Rate: 0.0005
Mean: 0.23059871691108866, Median: 0.22298983759812324, Num: 111
Epoch: 62, Loss: 86.85010073558394, Learning Rate: 0.0005
Epoch: 63, Loss: 74.21694688911897, Learning Rate: 0.0005
Mean: 0.23198666194940107, Median: 0.21122159112090194, Num: 111
Epoch: 64, Loss: 82.28092156835349, Learning Rate: 0.0005
Epoch: 65, Loss: 79.08299788509507, Learning Rate: 0.0005
Mean: 0.22923777211085675, Median: 0.22016976729837795, Num: 111
Epoch: 66, Loss: 75.31757529385119, Learning Rate: 0.0005
Epoch: 67, Loss: 78.83449850886701, Learning Rate: 0.0005
Mean: 0.2291888185031021, Median: 0.22315607145170202, Num: 111
Epoch: 68, Loss: 75.15088628286338, Learning Rate: 0.0005
Epoch: 69, Loss: 85.84004705498018, Learning Rate: 0.0005
Mean: 0.24108000715315636, Median: 0.23674499579513855, Num: 111
Epoch: 70, Loss: 78.70269823648843, Learning Rate: 0.0005
Epoch: 71, Loss: 81.21377637012895, Learning Rate: 0.0005
Mean: 0.2428507442002182, Median: 0.22195057478138203, Num: 111
Epoch: 72, Loss: 79.69884808092232, Learning Rate: 0.0005
Epoch: 73, Loss: 77.58268559697163, Learning Rate: 0.0005
Mean: 0.23749350929564902, Median: 0.2396885745956083, Num: 111
Epoch: 74, Loss: 83.9861590833549, Learning Rate: 0.0005
Epoch: 75, Loss: 82.17733286662273, Learning Rate: 0.0005
Mean: 0.2337645819331155, Median: 0.22958584549104336, Num: 111
Epoch: 76, Loss: 77.88588482500559, Learning Rate: 0.0005
Epoch: 77, Loss: 81.24359395130571, Learning Rate: 0.0005
Mean: 0.23326413083053718, Median: 0.20350193251716242, Num: 111
Epoch: 78, Loss: 77.40911279241723, Learning Rate: 0.0005
Epoch: 79, Loss: 75.32581046690424, Learning Rate: 0.0005
Mean: 0.2409858977684177, Median: 0.22533294748534427, Num: 111
Epoch: 80, Loss: 72.85142165494253, Learning Rate: 0.0005
Epoch: 81, Loss: 72.14347094800098, Learning Rate: 0.0005
Mean: 0.23917197885166777, Median: 0.22615576617427605, Num: 111
Epoch: 82, Loss: 73.96389678587397, Learning Rate: 0.0005
Epoch: 83, Loss: 74.13509598697524, Learning Rate: 0.0005
Mean: 0.23719681595242598, Median: 0.21422268247216855, Num: 111
Epoch: 84, Loss: 71.30966943717864, Learning Rate: 0.0005
Epoch: 85, Loss: 67.1037494245782, Learning Rate: 0.0005
Mean: 0.2391224410650225, Median: 0.24247584914046377, Num: 111
Epoch: 86, Loss: 77.96708134570754, Learning Rate: 0.0005
Epoch: 87, Loss: 91.6803632529385, Learning Rate: 0.0005
Mean: 0.24325540221316577, Median: 0.2339878864802005, Num: 111
Epoch: 88, Loss: 68.39200394412121, Learning Rate: 0.0005
Epoch: 89, Loss: 74.64687250895672, Learning Rate: 0.0005
Mean: 0.24375151580556745, Median: 0.23609351308170382, Num: 111
Epoch: 90, Loss: 68.53527910163604, Learning Rate: 0.0005
Epoch: 91, Loss: 62.31038596831172, Learning Rate: 0.0005
Mean: 0.2436938830397677, Median: 0.22999682712020084, Num: 111
Epoch: 92, Loss: 68.25732375914792, Learning Rate: 0.0005
Epoch: 93, Loss: 75.9783637494926, Learning Rate: 0.0005
Mean: 0.24907122702050213, Median: 0.25179740313219223, Num: 111
Epoch: 94, Loss: 67.34771652681282, Learning Rate: 0.0005
Epoch: 95, Loss: 64.62177713233304, Learning Rate: 0.0005
Mean: 0.24601925822089213, Median: 0.22754847932505082, Num: 111
Epoch: 96, Loss: 70.6132743329887, Learning Rate: 0.0005
Epoch: 97, Loss: 74.63061946271414, Learning Rate: 0.0005
Mean: 0.24825827464021927, Median: 0.24441965415567718, Num: 111
Epoch: 98, Loss: 71.52733353534377, Learning Rate: 0.0005
Epoch: 99, Loss: 72.85875588152783, Learning Rate: 0.0005
Mean: 0.24817190314325213, Median: 0.22097654731597374, Num: 111
Epoch: 100, Loss: 74.26894762429846, Learning Rate: 0.0005
Epoch: 101, Loss: 63.462852248226305, Learning Rate: 0.0005
Mean: 0.24380915510355508, Median: 0.23598889841888995, Num: 111
Epoch: 102, Loss: 76.19930276526026, Learning Rate: 0.0005
Epoch: 103, Loss: 71.97461212112243, Learning Rate: 0.0005
Mean: 0.2529017670682714, Median: 0.24593510530935447, Num: 111
Epoch: 104, Loss: 68.77823328684612, Learning Rate: 0.0005
Epoch: 105, Loss: 64.3098259201969, Learning Rate: 0.0005
Mean: 0.24641047769892324, Median: 0.22867821988373996, Num: 111
Epoch: 106, Loss: 71.77717381213085, Learning Rate: 0.0005
Epoch: 107, Loss: 66.683970233044, Learning Rate: 0.0005
Mean: 0.24781596305752343, Median: 0.24420460163524468, Num: 111
Epoch: 108, Loss: 69.27942078372082, Learning Rate: 0.0005
Epoch: 109, Loss: 70.25626245751438, Learning Rate: 0.0005
Mean: 0.2454968730770856, Median: 0.22218719719669258, Num: 111
Epoch: 110, Loss: 59.84532969830984, Learning Rate: 0.0005
Epoch: 111, Loss: 64.76807608087378, Learning Rate: 0.0005
Mean: 0.24735237939949445, Median: 0.24073145974105853, Num: 111
Epoch: 112, Loss: 65.58933002977486, Learning Rate: 0.0005
Epoch: 113, Loss: 67.88967811630432, Learning Rate: 0.0005
Mean: 0.24573278433050857, Median: 0.22427951365119073, Num: 111
Epoch: 114, Loss: 65.25891515433071, Learning Rate: 0.0005
Epoch: 115, Loss: 73.67141627116375, Learning Rate: 0.0005
Mean: 0.25754319532560066, Median: 0.2410003103407113, Num: 111
Epoch: 116, Loss: 65.48062195835344, Learning Rate: 0.0005
Epoch: 117, Loss: 64.97448684508542, Learning Rate: 0.0005
Mean: 0.2461285613565985, Median: 0.2460989048020194, Num: 111
Epoch: 118, Loss: 73.70162639847722, Learning Rate: 0.0005
Epoch: 119, Loss: 73.94205940200622, Learning Rate: 0.0005
Mean: 0.24804062047854533, Median: 0.24096636555042286, Num: 111
Epoch: 120, Loss: 61.29364111337317, Learning Rate: 0.0005
Epoch: 121, Loss: 78.7429153028741, Learning Rate: 0.0005
Mean: 0.24910806654025217, Median: 0.23483228556226157, Num: 111
Epoch: 122, Loss: 71.90920648230127, Learning Rate: 0.0005
Epoch: 123, Loss: 63.60403249349939, Learning Rate: 0.0005
Mean: 0.24661017477121433, Median: 0.22921797789624457, Num: 111
Epoch: 124, Loss: 71.33935369928199, Learning Rate: 0.0005
Epoch: 125, Loss: 61.724250172994225, Learning Rate: 0.0005
Mean: 0.2551978838210552, Median: 0.24341535517792556, Num: 111
Epoch: 126, Loss: 68.1773314648364, Learning Rate: 0.0005
Epoch: 127, Loss: 66.49233507822795, Learning Rate: 0.0005
Mean: 0.25516129961699147, Median: 0.25572866178798864, Num: 111
Epoch: 128, Loss: 67.20902012055178, Learning Rate: 0.0005
Epoch: 129, Loss: 70.00425681148667, Learning Rate: 0.0005
Mean: 0.2545509994166422, Median: 0.26031485064127646, Num: 111
Epoch: 130, Loss: 68.58897117247065, Learning Rate: 0.0005
Epoch: 131, Loss: 60.22046291397279, Learning Rate: 0.0005
Mean: 0.2553143622638058, Median: 0.25391705603998066, Num: 111
Epoch: 132, Loss: 69.24814896411206, Learning Rate: 0.0005
Epoch: 133, Loss: 60.14354328936841, Learning Rate: 0.0005
Mean: 0.2553034161954639, Median: 0.23808185563693607, Num: 111
Epoch: 134, Loss: 63.78903433788253, Learning Rate: 0.0005
Epoch: 135, Loss: 64.24790035385683, Learning Rate: 0.0005
Mean: 0.250821043696951, Median: 0.24464854429741711, Num: 111
Epoch: 136, Loss: 63.61073294030615, Learning Rate: 0.0005
Epoch: 137, Loss: 75.73908423228436, Learning Rate: 0.0005
Mean: 0.258264656460898, Median: 0.25767309948616507, Num: 111
Epoch: 138, Loss: 62.66962076669716, Learning Rate: 0.0005
Epoch: 139, Loss: 65.17609837543533, Learning Rate: 0.0005
Mean: 0.2540254334759373, Median: 0.24443946821772045, Num: 111
Epoch: 140, Loss: 63.15426337000835, Learning Rate: 0.0005
Epoch: 141, Loss: 63.89713039168392, Learning Rate: 0.0005
Mean: 0.25398561416410337, Median: 0.2425913928033996, Num: 111
Epoch: 142, Loss: 68.71950015102524, Learning Rate: 0.0005
Epoch: 143, Loss: 61.80857497525503, Learning Rate: 0.0005
Mean: 0.2566331034926026, Median: 0.23031782004099266, Num: 111
Epoch: 144, Loss: 68.40527557464968, Learning Rate: 0.0005
Epoch: 145, Loss: 63.87353235267731, Learning Rate: 0.0005
Mean: 0.2606165287614026, Median: 0.267575664467148, Num: 111
Epoch: 146, Loss: 69.96086319383369, Learning Rate: 0.0005
Epoch: 147, Loss: 67.89074726564338, Learning Rate: 0.0005
Mean: 0.25454313710373305, Median: 0.21799296369587015, Num: 111
Epoch: 148, Loss: 70.53085702873138, Learning Rate: 0.0005
Epoch: 149, Loss: 64.32701182078166, Learning Rate: 0.0005
Mean: 0.2529925860753393, Median: 0.2303241502295231, Num: 111
Epoch: 150, Loss: 59.45221979072295, Learning Rate: 0.0005
Epoch: 151, Loss: 70.79982143999582, Learning Rate: 0.0005
Mean: 0.2597437746864534, Median: 0.2475233224315683, Num: 111
Epoch: 152, Loss: 68.96773398066142, Learning Rate: 0.0005
Epoch: 153, Loss: 61.065703937806276, Learning Rate: 0.0005
Mean: 0.2586513539322176, Median: 0.24613848891195175, Num: 111
Epoch: 154, Loss: 62.20542024129845, Learning Rate: 0.0005
Epoch: 155, Loss: 60.32270702683782, Learning Rate: 0.0005
Mean: 0.25037235558531024, Median: 0.2365208713167869, Num: 111
Epoch: 156, Loss: 66.09030760339944, Learning Rate: 0.0005
Epoch: 157, Loss: 76.73872061809861, Learning Rate: 0.0005
Mean: 0.2622129711571417, Median: 0.2484196627666711, Num: 111
Epoch: 158, Loss: 62.52727533823036, Learning Rate: 0.0005
Epoch: 159, Loss: 61.61537669078413, Learning Rate: 0.0005
Mean: 0.2557482964464157, Median: 0.2506649102625952, Num: 111
Epoch: 160, Loss: 65.79423021982952, Learning Rate: 0.0005
Epoch: 161, Loss: 63.77732022986355, Learning Rate: 0.0005
Mean: 0.26492563557529325, Median: 0.2639621446521732, Num: 111
Epoch: 162, Loss: 63.745163193668226, Learning Rate: 0.0005
Epoch: 163, Loss: 62.05215525339885, Learning Rate: 0.0005
Mean: 0.2536964921182536, Median: 0.2313895435276819, Num: 111
Epoch: 164, Loss: 67.87276364521809, Learning Rate: 0.0005
Epoch: 165, Loss: 62.713175842560915, Learning Rate: 0.0005
Mean: 0.2638902387693265, Median: 0.2558860553957492, Num: 111
Epoch: 166, Loss: 62.73808977977339, Learning Rate: 0.0005
Epoch: 167, Loss: 64.42384253352522, Learning Rate: 0.0005
Mean: 0.2595718713587935, Median: 0.24501891803946554, Num: 111
Epoch: 168, Loss: 58.954353355499634, Learning Rate: 0.0005
Epoch: 169, Loss: 61.61849858387407, Learning Rate: 0.0005
Mean: 0.26608077566821603, Median: 0.2561692009502432, Num: 111
Epoch: 170, Loss: 64.37232120927558, Learning Rate: 0.0005
Epoch: 171, Loss: 63.60081117698945, Learning Rate: 0.0005
Mean: 0.2595053187893653, Median: 0.23125281369728148, Num: 111
Epoch: 172, Loss: 62.20389273080481, Learning Rate: 0.0005
Epoch: 173, Loss: 65.32542182738523, Learning Rate: 0.0005
Mean: 0.26119776978975895, Median: 0.23911418354708708, Num: 111
Epoch: 174, Loss: 58.525528482643956, Learning Rate: 0.0005
Epoch: 175, Loss: 65.3545810285821, Learning Rate: 0.0005
Mean: 0.2636244639002348, Median: 0.25745051597361046, Num: 111
Epoch: 176, Loss: 64.38824462890625, Learning Rate: 0.0005
Epoch: 177, Loss: 60.79152304867664, Learning Rate: 0.0005
Mean: 0.26550474928359447, Median: 0.26054719961196077, Num: 111
Epoch: 178, Loss: 61.61084205558501, Learning Rate: 0.0005
Epoch: 179, Loss: 67.6825066302196, Learning Rate: 0.0005
Mean: 0.26878805973059944, Median: 0.26507215318793925, Num: 111
Epoch: 180, Loss: 62.78360717267875, Learning Rate: 0.0005
Epoch: 181, Loss: 70.45981149788362, Learning Rate: 0.0005
Mean: 0.2606831973792248, Median: 0.2663662300994868, Num: 111
Epoch: 182, Loss: 68.26543782705284, Learning Rate: 0.0005
Epoch: 183, Loss: 65.06367768437029, Learning Rate: 0.0005
Mean: 0.2554130150337897, Median: 0.22593362942142484, Num: 111
Epoch: 184, Loss: 64.59440522021558, Learning Rate: 0.0005
Epoch: 185, Loss: 59.74054980565266, Learning Rate: 0.0005
Mean: 0.26438148935162936, Median: 0.25476200065629245, Num: 111
Epoch: 186, Loss: 56.28285674589226, Learning Rate: 0.0005
Epoch: 187, Loss: 53.54147260734834, Learning Rate: 0.0005
Mean: 0.2604116291055832, Median: 0.2425542923308717, Num: 111
Epoch: 188, Loss: 68.58233081289085, Learning Rate: 0.0005
Epoch: 189, Loss: 63.844970094152245, Learning Rate: 0.0005
Mean: 0.26782063318908894, Median: 0.2587312466703784, Num: 111
Epoch: 190, Loss: 66.69888298770032, Learning Rate: 0.0005
Epoch: 191, Loss: 60.37395969069148, Learning Rate: 0.0005
Mean: 0.26729233851506073, Median: 0.2699656194843486, Num: 111
Epoch: 192, Loss: 63.71825268756913, Learning Rate: 0.0005
Epoch: 193, Loss: 65.28555993574211, Learning Rate: 0.0005
Mean: 0.25780163474501105, Median: 0.24162110643190332, Num: 111
Epoch: 194, Loss: 61.43955956884177, Learning Rate: 0.0005
Epoch: 195, Loss: 64.95207529182893, Learning Rate: 0.0005
Mean: 0.26345750044157523, Median: 0.2712188769169103, Num: 111
Epoch: 196, Loss: 61.123974972460644, Learning Rate: 0.0005
Epoch: 197, Loss: 63.78355991409486, Learning Rate: 0.0005
Mean: 0.2657616955224496, Median: 0.2495871683287968, Num: 111
Epoch: 198, Loss: 59.54683958765972, Learning Rate: 0.0005
Epoch: 199, Loss: 73.74360953181623, Learning Rate: 0.0005
Mean: 0.26393531156084143, Median: 0.25546680111617903, Num: 111
Epoch: 200, Loss: 60.910443685140955, Learning Rate: 0.0005
Epoch: 201, Loss: 68.80084684096187, Learning Rate: 0.0005
Mean: 0.2672050126772458, Median: 0.2893557307708462, Num: 111
Epoch: 202, Loss: 59.03781515718943, Learning Rate: 0.0005
Epoch: 203, Loss: 62.08288854576019, Learning Rate: 0.0005
Mean: 0.2676230299303774, Median: 0.26805352748747713, Num: 111
Epoch: 204, Loss: 57.82907584776361, Learning Rate: 0.0005
Epoch: 205, Loss: 59.43353397874947, Learning Rate: 0.0005
Mean: 0.2658369099695167, Median: 0.27794622709527217, Num: 111
Epoch: 206, Loss: 60.67577856132783, Learning Rate: 0.0005
Epoch: 207, Loss: 62.90367610196033, Learning Rate: 0.0005
Mean: 0.27097340391121716, Median: 0.26018742169235803, Num: 111
Epoch: 208, Loss: 65.84750465025385, Learning Rate: 0.0005
Epoch: 209, Loss: 66.03844422604664, Learning Rate: 0.0005
Mean: 0.2642698458053432, Median: 0.25397522304965353, Num: 111
Epoch: 210, Loss: 54.80444825413716, Learning Rate: 0.0005
Epoch: 211, Loss: 60.99219230284174, Learning Rate: 0.0005
Mean: 0.26263652827297285, Median: 0.25766017476077663, Num: 111
Epoch: 212, Loss: 67.78602618481739, Learning Rate: 0.0005
Epoch: 213, Loss: 53.228457094675086, Learning Rate: 0.0005
Mean: 0.2649315126366938, Median: 0.27099704819096737, Num: 111
Epoch: 214, Loss: 60.642040160765134, Learning Rate: 0.0005
Epoch: 215, Loss: 59.69911419052676, Learning Rate: 0.0005
Mean: 0.26401128077876723, Median: 0.25630025454260597, Num: 111
Epoch: 216, Loss: 64.54744906597827, Learning Rate: 0.0005
Epoch: 217, Loss: 61.049300894679796, Learning Rate: 0.0005
Mean: 0.26973125915876806, Median: 0.26941329603121916, Num: 111
Epoch: 218, Loss: 56.17767692473998, Learning Rate: 0.0005
Epoch: 219, Loss: 60.82831557400255, Learning Rate: 0.0005
Mean: 0.2686953462363544, Median: 0.2574720003075648, Num: 111
Epoch: 220, Loss: 62.35044935525182, Learning Rate: 0.0005
Epoch: 221, Loss: 63.549014223627296, Learning Rate: 0.0005
Mean: 0.26110659392430013, Median: 0.24020121636129985, Num: 111
Epoch: 222, Loss: 63.033257013343906, Learning Rate: 0.0005
Epoch: 223, Loss: 63.519015651151356, Learning Rate: 0.0005
Mean: 0.26846362343465313, Median: 0.2533804269684478, Num: 111
Epoch: 224, Loss: 65.40555526549558, Learning Rate: 0.0005
Epoch: 225, Loss: 62.19117340409612, Learning Rate: 0.0005
Mean: 0.2652355647377259, Median: 0.25522234854751114, Num: 111
Epoch: 226, Loss: 68.54634039086032, Learning Rate: 0.0005
Epoch: 227, Loss: 57.43907104630068, Learning Rate: 0.0005
Mean: 0.26296488970597537, Median: 0.2384473926327672, Num: 111
Epoch: 228, Loss: 66.88027735790574, Learning Rate: 0.0005
Epoch: 229, Loss: 53.25749959141375, Learning Rate: 0.0005
Mean: 0.2643468234013965, Median: 0.26062791134077123, Num: 111
Epoch: 230, Loss: 61.68186773736793, Learning Rate: 0.0005
Epoch: 231, Loss: 55.60423157014043, Learning Rate: 0.0005
Mean: 0.26920322256278023, Median: 0.25058020182551727, Num: 111
Epoch: 232, Loss: 66.62317627596568, Learning Rate: 0.0005
Epoch: 233, Loss: 58.74629887040839, Learning Rate: 0.0005
Mean: 0.2709595973801708, Median: 0.285431207517809, Num: 111
Epoch: 234, Loss: 60.30348350341062, Learning Rate: 0.0005
Epoch: 235, Loss: 60.97492247891714, Learning Rate: 0.0005
Mean: 0.270162640995647, Median: 0.25137962356954546, Num: 111
Epoch: 236, Loss: 51.28800617355898, Learning Rate: 0.0005
Epoch: 237, Loss: 58.2678612628615, Learning Rate: 0.0005
Mean: 0.2695495063047554, Median: 0.26649242909910353, Num: 111
Epoch: 238, Loss: 64.178125220609, Learning Rate: 0.0005
Epoch: 239, Loss: 57.95496024855648, Learning Rate: 0.0005
Mean: 0.2663578356130489, Median: 0.2573453083234591, Num: 111
Epoch: 240, Loss: 51.71413400948766, Learning Rate: 0.0005
Epoch: 241, Loss: 63.787304889724915, Learning Rate: 0.0005
Mean: 0.26709878105489987, Median: 0.25865650540703844, Num: 111
Epoch: 242, Loss: 62.092155427817836, Learning Rate: 0.0005
Epoch: 243, Loss: 55.06722930540521, Learning Rate: 0.0005
Mean: 0.2677939304051248, Median: 0.24813384544047942, Num: 111
Epoch: 244, Loss: 55.38811150516372, Learning Rate: 0.0005
Epoch: 245, Loss: 63.164263346109045, Learning Rate: 0.0005
Mean: 0.2708652407630507, Median: 0.2618418970426848, Num: 111
Epoch: 246, Loss: 57.83262252807617, Learning Rate: 0.0005
Epoch: 247, Loss: 65.64751059750476, Learning Rate: 0.0005
Mean: 0.2725431522994725, Median: 0.26332158506004194, Num: 111
Epoch: 248, Loss: 57.2550570476486, Learning Rate: 0.0005
Epoch: 249, Loss: 64.35081658880395, Learning Rate: 0.0005
Mean: 0.26638986178811025, Median: 0.2666457113621877, Num: 111
Epoch: 250, Loss: 59.34912935509739, Learning Rate: 0.0005
Epoch: 251, Loss: 60.070741193840306, Learning Rate: 0.0005
Mean: 0.26914117421796485, Median: 0.2619836949183451, Num: 111
Epoch: 252, Loss: 62.61642657130598, Learning Rate: 0.0005
Epoch: 253, Loss: 70.73758198841509, Learning Rate: 0.0005
Mean: 0.2688084935221387, Median: 0.2567853051636345, Num: 111
Epoch: 254, Loss: 63.99465184613883, Learning Rate: 0.0005
Epoch: 255, Loss: 60.02639005270349, Learning Rate: 0.0005
Mean: 0.26567029188179114, Median: 0.2742461231512691, Num: 111
Epoch: 256, Loss: 58.174015608178564, Learning Rate: 0.0005
Epoch: 257, Loss: 53.877763334527074, Learning Rate: 0.0005
Mean: 0.265580250768743, Median: 0.25780293202202076, Num: 111
Epoch: 258, Loss: 62.958836141839086, Learning Rate: 0.0005
Epoch: 259, Loss: 59.78542555383889, Learning Rate: 0.0005
Mean: 0.2659057297471735, Median: 0.2700801336947713, Num: 111
Epoch: 260, Loss: 56.96747423654579, Learning Rate: 0.0005
Epoch: 261, Loss: 63.318736915128774, Learning Rate: 0.0005
Mean: 0.2682398306217778, Median: 0.26753108697326483, Num: 111
Epoch: 262, Loss: 64.43471604082958, Learning Rate: 0.0005
Epoch: 263, Loss: 55.08759306712323, Learning Rate: 0.0005
Mean: 0.27011724803512055, Median: 0.2626862567973766, Num: 111
Epoch: 264, Loss: 63.299771642110436, Learning Rate: 0.0005
Epoch: 265, Loss: 57.64049509347203, Learning Rate: 0.0005
Mean: 0.267592538717166, Median: 0.2548781351328276, Num: 111
Epoch: 266, Loss: 56.23826330253877, Learning Rate: 0.0005
Epoch: 267, Loss: 65.13495752035854, Learning Rate: 0.0005
Mean: 0.2692879287276224, Median: 0.24453138413068168, Num: 111
Epoch: 268, Loss: 64.36648259679956, Learning Rate: 0.0005
Epoch: 269, Loss: 57.853034036705296, Learning Rate: 0.0005
Mean: 0.26941813477795146, Median: 0.2619455120735327, Num: 111
Epoch: 270, Loss: 57.17255040823695, Learning Rate: 0.0005
Epoch: 271, Loss: 58.194345313382435, Learning Rate: 0.0005
Mean: 0.2700995324999816, Median: 0.2596677074249225, Num: 111
Epoch: 272, Loss: 66.34245828835361, Learning Rate: 0.0005
Epoch: 273, Loss: 60.37777682959315, Learning Rate: 0.0005
Mean: 0.2687942441469478, Median: 0.25250841019731923, Num: 111
Epoch: 274, Loss: 64.04920731969627, Learning Rate: 0.0005
Epoch: 275, Loss: 57.94591162578169, Learning Rate: 0.0005
Mean: 0.26991089335175933, Median: 0.2621913755130797, Num: 111
Epoch: 276, Loss: 58.92383565098406, Learning Rate: 0.0005
Epoch: 277, Loss: 62.27089436082955, Learning Rate: 0.0005
Mean: 0.2722288138479603, Median: 0.2623973898471184, Num: 111
Epoch: 278, Loss: 64.31124490714934, Learning Rate: 0.0005
Epoch: 279, Loss: 56.8449975036713, Learning Rate: 0.0005
Mean: 0.2757040366984004, Median: 0.2592974680099427, Num: 111
Epoch: 280, Loss: 64.45759437744876, Learning Rate: 0.0005
Epoch: 281, Loss: 68.83988528079297, Learning Rate: 0.0005
Mean: 0.26924776935429334, Median: 0.2368838563097055, Num: 111
Epoch: 282, Loss: 61.482658684971824, Learning Rate: 0.0005
Epoch: 283, Loss: 54.854477824934996, Learning Rate: 0.0005
Mean: 0.27410365283022775, Median: 0.252257920818044, Num: 111
Epoch: 284, Loss: 61.919428072780015, Learning Rate: 0.0005
Epoch: 285, Loss: 62.826102360185374, Learning Rate: 0.0005
Mean: 0.27215791557016805, Median: 0.2594405015923845, Num: 111
Epoch: 286, Loss: 53.58614330981151, Learning Rate: 0.0005
Epoch: 287, Loss: 67.27774571797934, Learning Rate: 0.0005
Mean: 0.2698266072606149, Median: 0.2613950935251125, Num: 111
Epoch: 288, Loss: 64.20194968258042, Learning Rate: 0.0005
Epoch: 289, Loss: 60.960965817233166, Learning Rate: 0.0005
Mean: 0.26668538644869866, Median: 0.2586400774228604, Num: 111
Epoch: 290, Loss: 57.67196255419628, Learning Rate: 0.0005
Epoch: 291, Loss: 60.23606236009713, Learning Rate: 0.0005
Mean: 0.27910949148917635, Median: 0.2828635494169187, Num: 111
Epoch: 292, Loss: 57.257341672139, Learning Rate: 0.0005
Epoch: 293, Loss: 54.70946094788701, Learning Rate: 0.0005
Mean: 0.2722828645115658, Median: 0.2706317999718596, Num: 111
Epoch: 294, Loss: 58.04583570181605, Learning Rate: 0.0005
Epoch: 295, Loss: 61.73910460989159, Learning Rate: 0.0005
Mean: 0.2694660475838974, Median: 0.2481868474142645, Num: 111
Epoch: 296, Loss: 63.82860969635377, Learning Rate: 0.0005
Epoch: 297, Loss: 57.56799237124891, Learning Rate: 0.0005
Mean: 0.274903923183545, Median: 0.28871929783166816, Num: 111
Epoch: 298, Loss: 60.80549422804132, Learning Rate: 0.0005
Epoch: 299, Loss: 60.65059135620852, Learning Rate: 0.0005
Mean: 0.26982320965843276, Median: 0.23948938092876, Num: 111
Epoch: 300, Loss: 57.100617673023635, Learning Rate: 0.0005
Epoch: 301, Loss: 63.32820619445249, Learning Rate: 0.0005
Mean: 0.27646597930501915, Median: 0.2767457964825732, Num: 111
Epoch: 302, Loss: 57.31601267550365, Learning Rate: 0.0005
Epoch: 303, Loss: 59.31933187002159, Learning Rate: 0.0005
Mean: 0.270432722854346, Median: 0.23525661409896087, Num: 111
Epoch: 304, Loss: 62.5926461966641, Learning Rate: 0.0005
Epoch: 305, Loss: 59.71339168318783, Learning Rate: 0.0005
Mean: 0.2704442608913926, Median: 0.2726740958149729, Num: 111
Epoch: 306, Loss: 58.0073964222368, Learning Rate: 0.0005
Epoch: 307, Loss: 59.727563444390356, Learning Rate: 0.0005
Mean: 0.2688870339261454, Median: 0.26295830956954586, Num: 111
Epoch: 308, Loss: 58.50205485792045, Learning Rate: 0.0005
Epoch: 309, Loss: 55.186977937997106, Learning Rate: 0.0005
Mean: 0.2649980915096982, Median: 0.23465121658412708, Num: 111
Epoch: 310, Loss: 66.28657775327383, Learning Rate: 0.0005
Epoch: 311, Loss: 58.13538884542074, Learning Rate: 0.0005
Mean: 0.2752671990057029, Median: 0.2720930645643575, Num: 111
Epoch: 312, Loss: 58.49788475036621, Learning Rate: 0.0005
Epoch: 313, Loss: 58.12794811754342, Learning Rate: 0.0005
Mean: 0.2714913722004201, Median: 0.2538994944452264, Num: 111
Epoch: 314, Loss: 62.33441615966429, Learning Rate: 0.0005
Epoch: 315, Loss: 53.61594065700669, Learning Rate: 0.0005
Mean: 0.26957924450254056, Median: 0.27415562828340984, Num: 111
Epoch: 316, Loss: 55.47634562526841, Learning Rate: 0.0005
Epoch: 317, Loss: 61.85767727013094, Learning Rate: 0.0005
Mean: 0.2746443712509377, Median: 0.28195122016644536, Num: 111
Epoch: 318, Loss: 59.74190331654376, Learning Rate: 0.0005
Epoch: 319, Loss: 58.557976320565466, Learning Rate: 0.0005
Mean: 0.273565131885608, Median: 0.2469833959072802, Num: 111
Epoch: 320, Loss: 59.827557207590125, Learning Rate: 0.0005
Epoch: 321, Loss: 63.16219332131995, Learning Rate: 0.0005
Mean: 0.27196078678526603, Median: 0.2490203847487883, Num: 111
Epoch: 322, Loss: 60.61260333693171, Learning Rate: 0.0005
Epoch: 323, Loss: 60.51238917155438, Learning Rate: 0.0005
Mean: 0.2640494437311649, Median: 0.2396157987068645, Num: 111
Epoch: 324, Loss: 51.134129426565515, Learning Rate: 0.0005
Epoch: 325, Loss: 59.426161432840736, Learning Rate: 0.0005
Mean: 0.2705605246368873, Median: 0.2511772528368419, Num: 111
Epoch: 326, Loss: 55.85626687199236, Learning Rate: 0.0005
Epoch: 327, Loss: 56.36674099083407, Learning Rate: 0.0005
Mean: 0.276756967417171, Median: 0.2634164994178664, Num: 111
Epoch: 328, Loss: 64.36995490200549, Learning Rate: 0.0005
Epoch: 329, Loss: 58.95238774081311, Learning Rate: 0.0005
Mean: 0.26864778101032927, Median: 0.24547151688870647, Num: 111
Epoch: 330, Loss: 56.380335014986706, Learning Rate: 0.0005
Epoch: 331, Loss: 63.23810338399496, Learning Rate: 0.0005
Mean: 0.2713933733736446, Median: 0.2552862954463572, Num: 111
Epoch: 332, Loss: 59.55469948412424, Learning Rate: 0.0005
Epoch: 333, Loss: 58.84608015956649, Learning Rate: 0.0005
Mean: 0.267874830269075, Median: 0.2549621381922131, Num: 111
Epoch: 334, Loss: 62.02343866049525, Learning Rate: 0.0005
Epoch: 335, Loss: 56.60359475698816, Learning Rate: 0.0005
Mean: 0.2690050940012638, Median: 0.2459825592007047, Num: 111
Epoch: 336, Loss: 52.45216351244823, Learning Rate: 0.0005
Epoch: 337, Loss: 59.22908098151885, Learning Rate: 0.0005
Mean: 0.27599592667990547, Median: 0.27357184936632617, Num: 111
Epoch: 338, Loss: 60.919522526752516, Learning Rate: 0.0005
Epoch: 339, Loss: 52.451300701463076, Learning Rate: 0.0005
Mean: 0.2729882815180733, Median: 0.26802410807838484, Num: 111
Epoch: 340, Loss: 62.1775194650673, Learning Rate: 0.0005
Epoch: 341, Loss: 56.60891105468015, Learning Rate: 0.0005
Mean: 0.2700790574187056, Median: 0.2675614557032665, Num: 111
Epoch: 342, Loss: 56.03928397075239, Learning Rate: 0.0005
Epoch: 343, Loss: 57.636726413864686, Learning Rate: 0.0005
Mean: 0.27759783987986425, Median: 0.2681947840879123, Num: 111
Epoch: 344, Loss: 55.21888496215085, Learning Rate: 0.0005
Epoch: 345, Loss: 58.52497452425669, Learning Rate: 0.0005
Mean: 0.2736743669283439, Median: 0.2445573291328243, Num: 111
Epoch: 346, Loss: 62.83520087276597, Learning Rate: 0.0005
Epoch: 347, Loss: 55.51132073459855, Learning Rate: 0.0005
Mean: 0.2697467043234508, Median: 0.24890551835696936, Num: 111
Epoch: 348, Loss: 61.710338477628774, Learning Rate: 0.0005
Epoch: 349, Loss: 64.07410054034497, Learning Rate: 0.0005
Mean: 0.2720701818766563, Median: 0.2530147570729509, Num: 111
Epoch: 350, Loss: 59.07271750576525, Learning Rate: 0.0005
Epoch: 351, Loss: 54.55797660781676, Learning Rate: 0.0005
Mean: 0.2702281205235846, Median: 0.25662416642160657, Num: 111
Epoch: 352, Loss: 57.82862891921078, Learning Rate: 0.0005
Epoch: 353, Loss: 56.703051636017946, Learning Rate: 0.0005
Mean: 0.2779705485073477, Median: 0.26842744268596097, Num: 111
Epoch: 354, Loss: 53.97668004323201, Learning Rate: 0.0005
Epoch: 355, Loss: 59.133971869227395, Learning Rate: 0.0005
Mean: 0.27439403532709616, Median: 0.28472568906888784, Num: 111
Epoch: 356, Loss: 57.96491902133069, Learning Rate: 0.0005
Epoch: 357, Loss: 63.21188408495432, Learning Rate: 0.0005
Mean: 0.2741018859788422, Median: 0.25697351480798203, Num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
Epoch: 358, Loss: 58.57879252606128, Learning Rate: 0.0005
Epoch: 359, Loss: 55.62487368411328, Learning Rate: 0.0005
Mean: 0.2704914752673662, Median: 0.2443264577332926, Num: 111
Epoch: 360, Loss: 54.16528727060341, Learning Rate: 0.0005
Epoch: 361, Loss: 57.73665444247694, Learning Rate: 0.0005
Mean: 0.26911015334180494, Median: 0.2717475222832553, Num: 111
Epoch: 362, Loss: 58.34161992819912, Learning Rate: 0.0005
Epoch: 363, Loss: 58.656024002167115, Learning Rate: 0.0005
Mean: 0.2703380532702447, Median: 0.26572115712638256, Num: 111
Epoch: 364, Loss: 64.05689557776394, Learning Rate: 0.0005
Epoch: 365, Loss: 67.2014125915895, Learning Rate: 0.0005
Mean: 0.27756321746279206, Median: 0.2936869192671336, Num: 111
Epoch: 366, Loss: 57.077223835221254, Learning Rate: 0.0005
Epoch: 367, Loss: 59.63576412200928, Learning Rate: 0.0005
Mean: 0.27688535938376657, Median: 0.2564166602018369, Num: 111
Epoch: 368, Loss: 61.055936353752415, Learning Rate: 0.0005
Epoch: 369, Loss: 57.03650958854032, Learning Rate: 0.0005
Mean: 0.27782244407406037, Median: 0.26187972240066326, Num: 111
Epoch: 370, Loss: 54.78389767566359, Learning Rate: 0.0005
Epoch: 371, Loss: 61.36502390022738, Learning Rate: 0.0005
Mean: 0.2791345081653275, Median: 0.2754992309990478, Num: 111
Epoch: 372, Loss: 57.47201749502894, Learning Rate: 0.0005
Epoch: 373, Loss: 58.927200432283335, Learning Rate: 0.0005
Mean: 0.2828974086331801, Median: 0.2815371884733239, Num: 111
Epoch: 374, Loss: 48.03893778697554, Learning Rate: 0.0005
Epoch: 375, Loss: 60.74641146142799, Learning Rate: 0.0005
Mean: 0.27199452003117375, Median: 0.2487225488773031, Num: 111
Epoch: 376, Loss: 53.13197805795325, Learning Rate: 0.0005
Epoch: 377, Loss: 62.4655205830034, Learning Rate: 0.0005
Mean: 0.2696015692191616, Median: 0.2588539051711305, Num: 111
Epoch: 378, Loss: 63.47905955257186, Learning Rate: 0.0005
Epoch: 379, Loss: 64.19345262826207, Learning Rate: 0.0005
Mean: 0.2764457958282816, Median: 0.25138896554877266, Num: 111
Epoch: 380, Loss: 55.105849668204065, Learning Rate: 0.0005
Epoch: 381, Loss: 56.826370089887135, Learning Rate: 0.0005
Mean: 0.27324831586535575, Median: 0.25084296757194885, Num: 111
Epoch: 382, Loss: 60.64283465190106, Learning Rate: 0.0005
Epoch: 383, Loss: 62.83181390417627, Learning Rate: 0.0005
Mean: 0.2767165919019039, Median: 0.24938717847644917, Num: 111
Epoch: 384, Loss: 58.2618580324104, Learning Rate: 0.0005
Epoch: 385, Loss: 61.595094290124365, Learning Rate: 0.0005
Mean: 0.27376443861047733, Median: 0.2571262698781016, Num: 111
Epoch: 386, Loss: 65.80338011592268, Learning Rate: 0.0005
Epoch: 387, Loss: 61.07731281418398, Learning Rate: 0.0005
Mean: 0.2724507369915214, Median: 0.26312235434711323, Num: 111
Epoch: 388, Loss: 67.60456737840032, Learning Rate: 0.0005
Epoch: 389, Loss: 57.48448814828712, Learning Rate: 0.0005
Mean: 0.2709708559335311, Median: 0.2450350956657362, Num: 111
Epoch: 390, Loss: 58.29098876125841, Learning Rate: 0.0005
Epoch: 391, Loss: 62.53885841369629, Learning Rate: 0.0005
Mean: 0.2727963164877115, Median: 0.2648419123579515, Num: 111
Epoch: 392, Loss: 59.14650197775967, Learning Rate: 0.0005
Epoch: 393, Loss: 54.19759794028408, Learning Rate: 0.0005
Mean: 0.2745094115397776, Median: 0.25431297678436804, Num: 111
Epoch: 394, Loss: 60.96116222243711, Learning Rate: 0.0005
Epoch: 395, Loss: 54.887125899992796, Learning Rate: 0.0005
Mean: 0.27448671811069586, Median: 0.2726116231766556, Num: 111
Epoch: 396, Loss: 67.61992068463061, Learning Rate: 0.0005
Epoch: 397, Loss: 56.469041548579575, Learning Rate: 0.0005
Mean: 0.2771672786479466, Median: 0.2634256319172042, Num: 111
Epoch: 398, Loss: 50.95551400586783, Learning Rate: 0.0005
Epoch: 399, Loss: 56.34661883618458, Learning Rate: 0.0005
Mean: 0.2755519209354583, Median: 0.2626043305034083, Num: 111
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
10
Epoch: 0, Loss: 238.35426836128696, Learning Rate: 0.0005
Epoch: 1, Loss: 190.26906516753047, Learning Rate: 0.0005
Mean: 0.06342591088364648, Median: 0.01839783466382658, Num: 111
Epoch: 2, Loss: 172.85725871626153, Learning Rate: 0.0005
Epoch: 3, Loss: 170.36890255112246, Learning Rate: 0.0005
Mean: 0.09369003652669788, Median: 0.04721579199673713, Num: 111
Epoch: 4, Loss: 155.66476955184018, Learning Rate: 0.0005
Epoch: 5, Loss: 156.14871179052147, Learning Rate: 0.0005
Mean: 0.10972949003633815, Median: 0.06696569174453858, Num: 111
Epoch: 6, Loss: 152.55661350847726, Learning Rate: 0.0005
Epoch: 7, Loss: 148.1788715224668, Learning Rate: 0.0005
Mean: 0.12637552813083613, Median: 0.0964738217819651, Num: 111
Epoch: 8, Loss: 131.6495377184397, Learning Rate: 0.0005
Epoch: 9, Loss: 139.85913021593208, Learning Rate: 0.0005
Mean: 0.14087175965155266, Median: 0.10667258226667704, Num: 111
Epoch: 10, Loss: 132.18136909209102, Learning Rate: 0.0005
Epoch: 11, Loss: 135.2797340944589, Learning Rate: 0.0005
Mean: 0.16403698346854914, Median: 0.1202522219618858, Num: 111
Epoch: 12, Loss: 132.55005962877388, Learning Rate: 0.0005
Epoch: 13, Loss: 126.46679848360728, Learning Rate: 0.0005
Mean: 0.16819492689995805, Median: 0.13304205581862644, Num: 111
Epoch: 14, Loss: 129.7993875986122, Learning Rate: 0.0005
Epoch: 15, Loss: 121.43986649111093, Learning Rate: 0.0005
Mean: 0.1744419178332635, Median: 0.1317800121237812, Num: 111
Epoch: 16, Loss: 123.11504269795246, Learning Rate: 0.0005
Epoch: 17, Loss: 119.85447233269014, Learning Rate: 0.0005
Mean: 0.18539066722215586, Median: 0.16504557917314763, Num: 111
Epoch: 18, Loss: 124.95914073162768, Learning Rate: 0.0005
Epoch: 19, Loss: 110.31275898577219, Learning Rate: 0.0005
Mean: 0.18370572460347678, Median: 0.15216328339908358, Num: 111
Epoch: 20, Loss: 112.36853608740381, Learning Rate: 0.0005
Epoch: 21, Loss: 107.13910468227893, Learning Rate: 0.0005
Mean: 0.18967575510387932, Median: 0.15163031595128443, Num: 111
Epoch: 22, Loss: 113.24705684615905, Learning Rate: 0.0005
Epoch: 23, Loss: 108.87660883708172, Learning Rate: 0.0005
Mean: 0.19051201883922259, Median: 0.1721504064748675, Num: 111
Epoch: 24, Loss: 107.9102491355804, Learning Rate: 0.0005
Epoch: 25, Loss: 105.51623833897602, Learning Rate: 0.0005
Mean: 0.19622111427551367, Median: 0.17115070050262007, Num: 111
Epoch: 26, Loss: 102.87311248319695, Learning Rate: 0.0005
Epoch: 27, Loss: 102.58605593945606, Learning Rate: 0.0005
Mean: 0.19641657890757985, Median: 0.18082054047096652, Num: 111
Epoch: 28, Loss: 104.14666858351374, Learning Rate: 0.0005
Epoch: 29, Loss: 108.8018523526479, Learning Rate: 0.0005
Mean: 0.19622663162468923, Median: 0.17146653955441207, Num: 111
Epoch: 30, Loss: 99.45456559974026, Learning Rate: 0.0005
Epoch: 31, Loss: 100.41526070560317, Learning Rate: 0.0005
Mean: 0.19921910526254552, Median: 0.1800536040580256, Num: 111
Epoch: 32, Loss: 96.82160729098032, Learning Rate: 0.0005
Epoch: 33, Loss: 98.26465087339102, Learning Rate: 0.0005
Mean: 0.20941849945250407, Median: 0.18609095143811064, Num: 111
Epoch: 34, Loss: 95.42283715397478, Learning Rate: 0.0005
Epoch: 35, Loss: 91.58429338845862, Learning Rate: 0.0005
Mean: 0.20260918183419144, Median: 0.18202183272394146, Num: 111
Epoch: 36, Loss: 103.29579300478281, Learning Rate: 0.0005
Epoch: 37, Loss: 93.35923950930676, Learning Rate: 0.0005
Mean: 0.20116969502950394, Median: 0.16394929866643687, Num: 111
Epoch: 38, Loss: 89.52034819269755, Learning Rate: 0.0005
Epoch: 39, Loss: 91.98125209578548, Learning Rate: 0.0005
Mean: 0.21853610388836492, Median: 0.21628065075421724, Num: 111
Epoch: 40, Loss: 100.21959787391755, Learning Rate: 0.0005
Epoch: 41, Loss: 97.6034513956093, Learning Rate: 0.0005
Mean: 0.21563486029898488, Median: 0.21282173513556296, Num: 111
Epoch: 42, Loss: 90.26609675855522, Learning Rate: 0.0005
Epoch: 43, Loss: 88.30610472897449, Learning Rate: 0.0005
Mean: 0.2239568845715634, Median: 0.20722876692326087, Num: 111
Epoch: 44, Loss: 92.01367102473615, Learning Rate: 0.0005
Epoch: 45, Loss: 85.53149928816829, Learning Rate: 0.0005
Mean: 0.21639463185089722, Median: 0.19813052661368347, Num: 111
Epoch: 46, Loss: 88.09848454486892, Learning Rate: 0.0005
Epoch: 47, Loss: 87.14521904451301, Learning Rate: 0.0005
Mean: 0.2276728863193469, Median: 0.2113919465619367, Num: 111
Epoch: 48, Loss: 90.18250148267632, Learning Rate: 0.0005
Epoch: 49, Loss: 89.89356833768178, Learning Rate: 0.0005
Mean: 0.2224470045525343, Median: 0.21775238992434062, Num: 111
Epoch: 50, Loss: 90.4799529156053, Learning Rate: 0.0005
Epoch: 51, Loss: 87.8671465149845, Learning Rate: 0.0005
Mean: 0.22491148476282727, Median: 0.23252366464019886, Num: 111
Epoch: 52, Loss: 91.22555519012083, Learning Rate: 0.0005
Epoch: 53, Loss: 81.26913854300258, Learning Rate: 0.0005
Mean: 0.22371657886610782, Median: 0.20741607042079052, Num: 111
Epoch: 54, Loss: 84.36727450267378, Learning Rate: 0.0005
Epoch: 55, Loss: 88.64436652861446, Learning Rate: 0.0005
Mean: 0.22671729103361976, Median: 0.21659417664034067, Num: 111
Epoch: 56, Loss: 81.7662204834352, Learning Rate: 0.0005
Epoch: 57, Loss: 85.36173599886607, Learning Rate: 0.0005
Mean: 0.22970034352574664, Median: 0.22417484612769825, Num: 111
Epoch: 58, Loss: 85.37202237600303, Learning Rate: 0.0005
Epoch: 59, Loss: 89.42508476900767, Learning Rate: 0.0005
Mean: 0.22744776516597154, Median: 0.20895781247300788, Num: 111
Epoch: 60, Loss: 83.84136457328337, Learning Rate: 0.0005
Epoch: 61, Loss: 82.72299943487329, Learning Rate: 0.0005
Mean: 0.2371095806462781, Median: 0.23289065676635679, Num: 111
Epoch: 62, Loss: 86.87213176129812, Learning Rate: 0.0005
Epoch: 63, Loss: 74.17345376761563, Learning Rate: 0.0005
Mean: 0.24049179836685086, Median: 0.2179734734060155, Num: 111
Epoch: 64, Loss: 82.17757075665945, Learning Rate: 0.0005
Epoch: 65, Loss: 78.95150262763701, Learning Rate: 0.0005
Mean: 0.23640797791820828, Median: 0.22222488711668043, Num: 111
Epoch: 66, Loss: 75.14469431681805, Learning Rate: 0.0005
Epoch: 67, Loss: 78.73568895638707, Learning Rate: 0.0005
Mean: 0.2326368336488987, Median: 0.21680944590377366, Num: 111
Epoch: 68, Loss: 74.9400049692177, Learning Rate: 0.0005
Epoch: 69, Loss: 85.68431452096226, Learning Rate: 0.0005
Mean: 0.2459099258106022, Median: 0.24307442716103478, Num: 111
Epoch: 70, Loss: 78.5807252217488, Learning Rate: 0.0005
Epoch: 71, Loss: 81.16194674480393, Learning Rate: 0.0005
Mean: 0.24066029941676167, Median: 0.22730323287299367, Num: 111
Epoch: 72, Loss: 79.53866388711585, Learning Rate: 0.0005
Epoch: 73, Loss: 77.466892541173, Learning Rate: 0.0005
Mean: 0.23980082068853056, Median: 0.2438143520472037, Num: 111
Epoch: 74, Loss: 83.91433545767543, Learning Rate: 0.0005
Epoch: 75, Loss: 82.09508987794439, Learning Rate: 0.0005
Mean: 0.24365075298292782, Median: 0.242593905844168, Num: 111
Epoch: 76, Loss: 77.82032107157879, Learning Rate: 0.0005
Epoch: 77, Loss: 81.01248145965208, Learning Rate: 0.0005
Mean: 0.2372849893604525, Median: 0.24178217427429796, Num: 111
Epoch: 78, Loss: 77.35363146770432, Learning Rate: 0.0005
Epoch: 79, Loss: 75.21801493541304, Learning Rate: 0.0005
Mean: 0.24198712736353678, Median: 0.23183409862196325, Num: 111
Epoch: 80, Loss: 72.7753928770502, Learning Rate: 0.0005
Epoch: 81, Loss: 72.10581041818642, Learning Rate: 0.0005
Mean: 0.24130364185035366, Median: 0.23288250126966858, Num: 111
Epoch: 82, Loss: 73.93274923117764, Learning Rate: 0.0005
Epoch: 83, Loss: 74.10462871229792, Learning Rate: 0.0005
Mean: 0.24084009847087462, Median: 0.23854251390536188, Num: 111
Epoch: 84, Loss: 71.2764176058482, Learning Rate: 0.0005
Epoch: 85, Loss: 67.02827798314841, Learning Rate: 0.0005
Mean: 0.24042222254264464, Median: 0.24796964276073283, Num: 111
Epoch: 86, Loss: 77.91559870846301, Learning Rate: 0.0005
Epoch: 87, Loss: 91.62748706771667, Learning Rate: 0.0005
Mean: 0.2505687323110803, Median: 0.24537494576732916, Num: 111
Epoch: 88, Loss: 68.2977103612509, Learning Rate: 0.0005
Epoch: 89, Loss: 74.55776697181794, Learning Rate: 0.0005
Mean: 0.23990520447383112, Median: 0.2352430395431742, Num: 111
Epoch: 90, Loss: 68.5044163163886, Learning Rate: 0.0005
Epoch: 91, Loss: 62.291565952530824, Learning Rate: 0.0005
Mean: 0.24804365329953, Median: 0.23657511384370805, Num: 111
Epoch: 92, Loss: 68.1427185564156, Learning Rate: 0.0005
Epoch: 93, Loss: 75.88954852000776, Learning Rate: 0.0005
Mean: 0.2523227379416991, Median: 0.24215780413124277, Num: 111
Epoch: 94, Loss: 67.26930625180164, Learning Rate: 0.0005
Epoch: 95, Loss: 64.57761571493494, Learning Rate: 0.0005
Mean: 0.24631768060010156, Median: 0.22686490802847728, Num: 111
Epoch: 96, Loss: 70.59580198540745, Learning Rate: 0.0005
Epoch: 97, Loss: 74.53239974056382, Learning Rate: 0.0005
Mean: 0.24020214871497234, Median: 0.23924424655149704, Num: 111
Epoch: 98, Loss: 71.42565813409277, Learning Rate: 0.0005
Epoch: 99, Loss: 72.79966345178076, Learning Rate: 0.0005
Mean: 0.25258212482482684, Median: 0.2606864773151702, Num: 111
Epoch: 100, Loss: 74.17987194980483, Learning Rate: 0.0005
Epoch: 101, Loss: 63.36594020889466, Learning Rate: 0.0005
Mean: 0.24581444890738327, Median: 0.2275310993537935, Num: 111
Epoch: 102, Loss: 76.11799460721303, Learning Rate: 0.0005
Epoch: 103, Loss: 71.91512737503972, Learning Rate: 0.0005
Mean: 0.25785564121747623, Median: 0.25719958490146305, Num: 111
Epoch: 104, Loss: 68.68916982627776, Learning Rate: 0.0005
Epoch: 105, Loss: 64.23265675464309, Learning Rate: 0.0005
Mean: 0.2481669307205583, Median: 0.22808421083147637, Num: 111
Epoch: 106, Loss: 71.69758994320789, Learning Rate: 0.0005
Epoch: 107, Loss: 66.60060434456331, Learning Rate: 0.0005
Mean: 0.24614311361167943, Median: 0.23726747318942806, Num: 111
Epoch: 108, Loss: 69.21706576519702, Learning Rate: 0.0005
Epoch: 109, Loss: 70.19541765695595, Learning Rate: 0.0005
Mean: 0.2538187754329758, Median: 0.22499701498900357, Num: 111
Epoch: 110, Loss: 59.814379025654624, Learning Rate: 0.0005
Epoch: 111, Loss: 64.72672706052481, Learning Rate: 0.0005
Mean: 0.2522693949989, Median: 0.2436934492395407, Num: 111
Epoch: 112, Loss: 65.54953216644655, Learning Rate: 0.0005
Epoch: 113, Loss: 67.84378881339568, Learning Rate: 0.0005
Mean: 0.24871055164936767, Median: 0.22606205676138985, Num: 111
Epoch: 114, Loss: 65.20954012583537, Learning Rate: 0.0005
Epoch: 115, Loss: 73.65949812280127, Learning Rate: 0.0005
Mean: 0.2547352332749008, Median: 0.2398692031100936, Num: 111
Epoch: 116, Loss: 65.44482683848186, Learning Rate: 0.0005
Epoch: 117, Loss: 64.9247335870582, Learning Rate: 0.0005
Mean: 0.2536011292527121, Median: 0.2574399019296803, Num: 111
Epoch: 118, Loss: 73.65361919173274, Learning Rate: 0.0005
Epoch: 119, Loss: 73.86600719589785, Learning Rate: 0.0005
Mean: 0.2517626720833351, Median: 0.24623131862325207, Num: 111
Epoch: 120, Loss: 61.25760153115514, Learning Rate: 0.0005
Epoch: 121, Loss: 78.72985577870564, Learning Rate: 0.0005
Mean: 0.24663606062921534, Median: 0.2292694555888059, Num: 111
Epoch: 122, Loss: 71.87673897341074, Learning Rate: 0.0005
Epoch: 123, Loss: 63.58559118983257, Learning Rate: 0.0005
Mean: 0.25020891178496807, Median: 0.22759267102236655, Num: 111
Epoch: 124, Loss: 71.2695564821542, Learning Rate: 0.0005
Epoch: 125, Loss: 61.658825380256374, Learning Rate: 0.0005
Mean: 0.2554695981561525, Median: 0.24805132828131046, Num: 111
Epoch: 126, Loss: 68.13117787924158, Learning Rate: 0.0005
Epoch: 127, Loss: 66.48601816935711, Learning Rate: 0.0005
Mean: 0.2508056667225203, Median: 0.2588165005354927, Num: 111
Epoch: 128, Loss: 67.16602118618518, Learning Rate: 0.0005
Epoch: 129, Loss: 69.9461079563003, Learning Rate: 0.0005
Mean: 0.2528487361178645, Median: 0.2446868371171496, Num: 111
Epoch: 130, Loss: 68.56771871268032, Learning Rate: 0.0005
Epoch: 131, Loss: 60.202387798263366, Learning Rate: 0.0005
Mean: 0.2584910228024848, Median: 0.2568262860982816, Num: 111
Epoch: 132, Loss: 69.17586397837444, Learning Rate: 0.0005
Epoch: 133, Loss: 60.067176071994275, Learning Rate: 0.0005
Mean: 0.26079986451427234, Median: 0.23950801547354478, Num: 111
Epoch: 134, Loss: 63.765679922448584, Learning Rate: 0.0005
Epoch: 135, Loss: 64.20070544782891, Learning Rate: 0.0005
Mean: 0.2523585017388354, Median: 0.24990112330366127, Num: 111
Epoch: 136, Loss: 63.553384757903686, Learning Rate: 0.0005
Epoch: 137, Loss: 75.72349821803081, Learning Rate: 0.0005
Mean: 0.2567322891393581, Median: 0.25899338538187333, Num: 111
Epoch: 138, Loss: 62.653664278696816, Learning Rate: 0.0005
Epoch: 139, Loss: 65.17084248094673, Learning Rate: 0.0005
Mean: 0.25490226671085464, Median: 0.2467540431343714, Num: 111
Epoch: 140, Loss: 63.12176209185497, Learning Rate: 0.0005
Epoch: 141, Loss: 63.866456525871556, Learning Rate: 0.0005
Mean: 0.26359726663579996, Median: 0.2441220188123705, Num: 111
Epoch: 142, Loss: 68.7022336994309, Learning Rate: 0.0005
Epoch: 143, Loss: 61.74887026361672, Learning Rate: 0.0005
Mean: 0.2547516489444972, Median: 0.22497989387152045, Num: 111
Epoch: 144, Loss: 68.40025897198413, Learning Rate: 0.0005
Epoch: 145, Loss: 63.84179556513407, Learning Rate: 0.0005
Mean: 0.2581560009823239, Median: 0.2543515526041767, Num: 111
Epoch: 146, Loss: 69.94776368428425, Learning Rate: 0.0005
Epoch: 147, Loss: 67.87999176117312, Learning Rate: 0.0005
Mean: 0.2536057789138992, Median: 0.2363619262135941, Num: 111
Epoch: 148, Loss: 70.48908061291797, Learning Rate: 0.0005
Epoch: 149, Loss: 64.29522197217827, Learning Rate: 0.0005
Mean: 0.25595187021935367, Median: 0.24204502283214493, Num: 111
Epoch: 150, Loss: 59.4068609490452, Learning Rate: 0.0005
Epoch: 151, Loss: 70.78768253326416, Learning Rate: 0.0005
Mean: 0.2589703316310867, Median: 0.23396649468136577, Num: 111
Epoch: 152, Loss: 68.93004084207925, Learning Rate: 0.0005
Epoch: 153, Loss: 61.01724877989436, Learning Rate: 0.0005
Mean: 0.2604106135358939, Median: 0.25084393326224824, Num: 111
Epoch: 154, Loss: 62.16560137415507, Learning Rate: 0.0005
Epoch: 155, Loss: 60.306890533631105, Learning Rate: 0.0005
Mean: 0.2516695253397848, Median: 0.24014083201309627, Num: 111
Epoch: 156, Loss: 66.05634205599866, Learning Rate: 0.0005
Epoch: 157, Loss: 76.73004794982542, Learning Rate: 0.0005
Mean: 0.25604984422276167, Median: 0.2631195240963112, Num: 111
Epoch: 158, Loss: 62.48894845433982, Learning Rate: 0.0005
Epoch: 159, Loss: 61.5989887283509, Learning Rate: 0.0005
Mean: 0.25931121874978047, Median: 0.2500854595363781, Num: 111
Epoch: 160, Loss: 65.77633522217532, Learning Rate: 0.0005
Epoch: 161, Loss: 63.77419547575066, Learning Rate: 0.0005
Mean: 0.26768173352073055, Median: 0.2661475209314321, Num: 111
Epoch: 162, Loss: 63.71511326065983, Learning Rate: 0.0005
Epoch: 163, Loss: 62.04576981785786, Learning Rate: 0.0005
Mean: 0.25223540393524196, Median: 0.24830886199425597, Num: 111
Epoch: 164, Loss: 67.8367455723774, Learning Rate: 0.0005
Epoch: 165, Loss: 62.695991699954114, Learning Rate: 0.0005
Mean: 0.26239371227268576, Median: 0.2658704875903891, Num: 111
Epoch: 166, Loss: 62.728383673242774, Learning Rate: 0.0005
Epoch: 167, Loss: 64.4021588865533, Learning Rate: 0.0005
Mean: 0.26916598387518753, Median: 0.2584543260140412, Num: 111
Epoch: 168, Loss: 58.934242972408434, Learning Rate: 0.0005
Epoch: 169, Loss: 61.587730740926354, Learning Rate: 0.0005
Mean: 0.2619467419333993, Median: 0.26539767046681395, Num: 111
Epoch: 170, Loss: 64.3360388882189, Learning Rate: 0.0005
Epoch: 171, Loss: 63.59484805256487, Learning Rate: 0.0005
Mean: 0.259994228087361, Median: 0.2280157217808259, Num: 111
Epoch: 172, Loss: 62.181819674480394, Learning Rate: 0.0005
Epoch: 173, Loss: 65.28619569755462, Learning Rate: 0.0005
Mean: 0.26137110123955687, Median: 0.25361675331323874, Num: 111
Epoch: 174, Loss: 58.52827851169081, Learning Rate: 0.0005
Epoch: 175, Loss: 65.3324358606913, Learning Rate: 0.0005
Mean: 0.2579809664685349, Median: 0.2497656641092013, Num: 111
Epoch: 176, Loss: 64.35245093380112, Learning Rate: 0.0005
Epoch: 177, Loss: 60.77317720436188, Learning Rate: 0.0005
Mean: 0.26616700202513405, Median: 0.27180375932123135, Num: 111
Epoch: 178, Loss: 61.596066750675796, Learning Rate: 0.0005
Epoch: 179, Loss: 67.67277811234256, Learning Rate: 0.0005
Mean: 0.26539205171373953, Median: 0.2548280407874817, Num: 111
Epoch: 180, Loss: 62.7670118952372, Learning Rate: 0.0005
Epoch: 181, Loss: 70.42622776491096, Learning Rate: 0.0005
Mean: 0.26129181682687597, Median: 0.265819262614586, Num: 111
Epoch: 182, Loss: 68.27016731629888, Learning Rate: 0.0005
Epoch: 183, Loss: 65.07488740208638, Learning Rate: 0.0005
Mean: 0.2608654812015256, Median: 0.2565917706156408, Num: 111
Epoch: 184, Loss: 64.5894563514066, Learning Rate: 0.0005
Epoch: 185, Loss: 59.719499668443056, Learning Rate: 0.0005
Mean: 0.26414400965581347, Median: 0.2631414781596489, Num: 111
Epoch: 186, Loss: 56.26546875827284, Learning Rate: 0.0005
Epoch: 187, Loss: 53.49992515380124, Learning Rate: 0.0005
Mean: 0.2639233676940953, Median: 0.2630745782856596, Num: 111
Epoch: 188, Loss: 68.57066085539668, Learning Rate: 0.0005
Epoch: 189, Loss: 63.83337620654738, Learning Rate: 0.0005
Mean: 0.26907782011681697, Median: 0.2507727430819571, Num: 111
Epoch: 190, Loss: 66.68044363734234, Learning Rate: 0.0005
Epoch: 191, Loss: 60.36349722850753, Learning Rate: 0.0005
Mean: 0.2611537710855916, Median: 0.2494956917847527, Num: 111
Epoch: 192, Loss: 63.70703278966697, Learning Rate: 0.0005
Epoch: 193, Loss: 65.26077841563396, Learning Rate: 0.0005
Mean: 0.2579628823729333, Median: 0.2496179975762275, Num: 111
Epoch: 194, Loss: 61.43888696417751, Learning Rate: 0.0005
Epoch: 195, Loss: 64.94001938923296, Learning Rate: 0.0005
Mean: 0.26337623965139045, Median: 0.24346912216290117, Num: 111
Epoch: 196, Loss: 61.09725724645408, Learning Rate: 0.0005
Epoch: 197, Loss: 63.77985808361007, Learning Rate: 0.0005
Mean: 0.2686268172464593, Median: 0.27800891173090225, Num: 111
Epoch: 198, Loss: 59.55099712509707, Learning Rate: 0.0005
Epoch: 199, Loss: 73.74880055347121, Learning Rate: 0.0005
Mean: 0.2584861857501318, Median: 0.26823144987868813, Num: 111
Epoch: 200, Loss: 60.8872551745679, Learning Rate: 0.0005
Epoch: 201, Loss: 68.77020307908575, Learning Rate: 0.0005
Mean: 0.2626403288190129, Median: 0.2491478467037832, Num: 111
Epoch: 202, Loss: 59.00475908187499, Learning Rate: 0.0005
Epoch: 203, Loss: 62.08125245427511, Learning Rate: 0.0005
Mean: 0.26330459352582725, Median: 0.24908786976025812, Num: 111
Epoch: 204, Loss: 57.806362370410596, Learning Rate: 0.0005
Epoch: 205, Loss: 59.40007579757506, Learning Rate: 0.0005
Mean: 0.26746604671011004, Median: 0.2625780185104593, Num: 111
Epoch: 206, Loss: 60.662937531988305, Learning Rate: 0.0005
Epoch: 207, Loss: 62.902643893138475, Learning Rate: 0.0005
Mean: 0.27303647625673116, Median: 0.2913073025053297, Num: 111
Epoch: 208, Loss: 65.84999406194112, Learning Rate: 0.0005
Epoch: 209, Loss: 66.01643924253533, Learning Rate: 0.0005
Mean: 0.26224818999138405, Median: 0.2533538948875483, Num: 111
Epoch: 210, Loss: 54.79988572683679, Learning Rate: 0.0005
Epoch: 211, Loss: 60.960614652518764, Learning Rate: 0.0005
Mean: 0.26402119670117147, Median: 0.2699053846562883, Num: 111
Epoch: 212, Loss: 67.77284309088466, Learning Rate: 0.0005
Epoch: 213, Loss: 53.215408991618325, Learning Rate: 0.0005
Mean: 0.26460120742598325, Median: 0.2651097880530689, Num: 111
Epoch: 214, Loss: 60.63051998184388, Learning Rate: 0.0005
Epoch: 215, Loss: 59.69551497769643, Learning Rate: 0.0005
Mean: 0.26223145480565757, Median: 0.25742343035675624, Num: 111
Epoch: 216, Loss: 64.52293427019234, Learning Rate: 0.0005
Epoch: 217, Loss: 61.039100141410366, Learning Rate: 0.0005
Mean: 0.26963806735539203, Median: 0.2760781788236013, Num: 111
Epoch: 218, Loss: 56.17054739343115, Learning Rate: 0.0005
Epoch: 219, Loss: 60.82302838061229, Learning Rate: 0.0005
Mean: 0.2648357409891214, Median: 0.2598840062421656, Num: 111
Epoch: 220, Loss: 62.328217632799266, Learning Rate: 0.0005
Epoch: 221, Loss: 63.53848170085126, Learning Rate: 0.0005
Mean: 0.2682290007933008, Median: 0.25973090874513244, Num: 111
Epoch: 222, Loss: 63.021277795355005, Learning Rate: 0.0005
Epoch: 223, Loss: 63.516954387526916, Learning Rate: 0.0005
Mean: 0.2657978228831774, Median: 0.2614464974780001, Num: 111
Epoch: 224, Loss: 65.38646542882344, Learning Rate: 0.0005
Epoch: 225, Loss: 62.18358311595687, Learning Rate: 0.0005
Mean: 0.2663359174669967, Median: 0.26325653382101993, Num: 111
Epoch: 226, Loss: 68.54432023289692, Learning Rate: 0.0005
Epoch: 227, Loss: 57.42357308031565, Learning Rate: 0.0005
Mean: 0.2635052944691124, Median: 0.24965252233640223, Num: 111
Epoch: 228, Loss: 66.86128848432058, Learning Rate: 0.0005
Epoch: 229, Loss: 53.25203093563218, Learning Rate: 0.0005
Mean: 0.2640848147744834, Median: 0.2688063682385956, Num: 111
Epoch: 230, Loss: 61.67589365717876, Learning Rate: 0.0005
Epoch: 231, Loss: 55.58513657443495, Learning Rate: 0.0005
Mean: 0.2678679156894163, Median: 0.25281753657051165, Num: 111
Epoch: 232, Loss: 66.60307863534214, Learning Rate: 0.0005
Epoch: 233, Loss: 58.73994264257959, Learning Rate: 0.0005
Mean: 0.27215010868844675, Median: 0.285877103812084, Num: 111
Epoch: 234, Loss: 60.299833883722144, Learning Rate: 0.0005
Epoch: 235, Loss: 60.972024710781604, Learning Rate: 0.0005
Mean: 0.2697772217322998, Median: 0.27259409772588883, Num: 111
Epoch: 236, Loss: 51.27475058314312, Learning Rate: 0.0005
Epoch: 237, Loss: 58.26236262953425, Learning Rate: 0.0005
Mean: 0.26919143181533206, Median: 0.2861275834862109, Num: 111
Epoch: 238, Loss: 64.1742963675993, Learning Rate: 0.0005
Epoch: 239, Loss: 57.95752201310123, Learning Rate: 0.0005
Mean: 0.26802016164383474, Median: 0.25245749456652783, Num: 111
Epoch: 240, Loss: 51.71189007127141, Learning Rate: 0.0005
Epoch: 241, Loss: 63.767972900206786, Learning Rate: 0.0005
Mean: 0.2684139671788202, Median: 0.27211380488190573, Num: 111
Epoch: 242, Loss: 62.08803001082087, Learning Rate: 0.0005
Epoch: 243, Loss: 55.06682800384889, Learning Rate: 0.0005
Mean: 0.2620538865628307, Median: 0.25714402740367337, Num: 111
Epoch: 244, Loss: 55.39948009582887, Learning Rate: 0.0005
Epoch: 245, Loss: 63.14863978236555, Learning Rate: 0.0005
Mean: 0.2680144001400469, Median: 0.26426247369651357, Num: 111
Epoch: 246, Loss: 57.87690592386637, Learning Rate: 0.0005
Epoch: 247, Loss: 65.64950335169414, Learning Rate: 0.0005
Mean: 0.27628093469863485, Median: 0.2780792388245615, Num: 111
Epoch: 248, Loss: 57.24097801116576, Learning Rate: 0.0005
Epoch: 249, Loss: 64.32598061159433, Learning Rate: 0.0005
Mean: 0.2671165163576504, Median: 0.2601843974969112, Num: 111
Epoch: 250, Loss: 59.33282994649496, Learning Rate: 0.0005
Epoch: 251, Loss: 60.04812936897738, Learning Rate: 0.0005
Mean: 0.26988704246601236, Median: 0.2752758966601989, Num: 111
Epoch: 252, Loss: 62.61059871351863, Learning Rate: 0.0005
Epoch: 253, Loss: 70.73567036548293, Learning Rate: 0.0005
Mean: 0.26354362622671523, Median: 0.2545106576854343, Num: 111
Epoch: 254, Loss: 63.99537883896426, Learning Rate: 0.0005
Epoch: 255, Loss: 60.02431134143507, Learning Rate: 0.0005
Mean: 0.26407899386921213, Median: 0.27979498590802143, Num: 111
Epoch: 256, Loss: 58.18389199727989, Learning Rate: 0.0005
Epoch: 257, Loss: 53.85278343292604, Learning Rate: 0.0005
Mean: 0.2680213614552832, Median: 0.28045870822228475, Num: 111
Epoch: 258, Loss: 62.95609214506953, Learning Rate: 0.0005
Epoch: 259, Loss: 59.778131910117274, Learning Rate: 0.0005
Mean: 0.27297116620118306, Median: 0.2557146528197619, Num: 111
Epoch: 260, Loss: 56.965788576976365, Learning Rate: 0.0005
Epoch: 261, Loss: 63.310629419533605, Learning Rate: 0.0005
Mean: 0.27301816310928634, Median: 0.2686133062435222, Num: 111
Epoch: 262, Loss: 64.43347958484328, Learning Rate: 0.0005
Epoch: 263, Loss: 55.093220394777966, Learning Rate: 0.0005
Mean: 0.2706734157826433, Median: 0.30067436676495646, Num: 111
Epoch: 264, Loss: 63.28001052213003, Learning Rate: 0.0005
Epoch: 265, Loss: 57.64511386457696, Learning Rate: 0.0005
Mean: 0.2663446127650159, Median: 0.25953239569293673, Num: 111
Epoch: 266, Loss: 56.23323916239911, Learning Rate: 0.0005
Epoch: 267, Loss: 65.13298550571304, Learning Rate: 0.0005
Mean: 0.2699629042300874, Median: 0.25876193882432363, Num: 111
Epoch: 268, Loss: 64.35802799822336, Learning Rate: 0.0005
Epoch: 269, Loss: 57.83716654490276, Learning Rate: 0.0005
Mean: 0.27053560167911334, Median: 0.26719324267779193, Num: 111
Epoch: 270, Loss: 57.174966145710776, Learning Rate: 0.0005
Epoch: 271, Loss: 58.192854582545266, Learning Rate: 0.0005
Mean: 0.27238463408660646, Median: 0.27716362496684127, Num: 111
Epoch: 272, Loss: 66.34208215001118, Learning Rate: 0.0005
Epoch: 273, Loss: 60.36504645519946, Learning Rate: 0.0005
Mean: 0.2682032951582436, Median: 0.2804533272201268, Num: 111
Epoch: 274, Loss: 64.04337059158877, Learning Rate: 0.0005
Epoch: 275, Loss: 57.92803929800011, Learning Rate: 0.0005
Mean: 0.27735894298506386, Median: 0.2719343679650882, Num: 111
Epoch: 276, Loss: 58.91318245393684, Learning Rate: 0.0005
Epoch: 277, Loss: 62.268841456217935, Learning Rate: 0.0005
Mean: 0.27611670321910997, Median: 0.27225069209309943, Num: 111
Epoch: 278, Loss: 64.28839066516922, Learning Rate: 0.0005
Epoch: 279, Loss: 56.835292276129664, Learning Rate: 0.0005
Mean: 0.2738711734152272, Median: 0.27191982455493324, Num: 111
Epoch: 280, Loss: 64.43856301365129, Learning Rate: 0.0005
Epoch: 281, Loss: 68.83443810566362, Learning Rate: 0.0005
Mean: 0.2694692030469724, Median: 0.26413945273447453, Num: 111
Epoch: 282, Loss: 61.48729446135371, Learning Rate: 0.0005
Epoch: 283, Loss: 54.85340771043157, Learning Rate: 0.0005
Mean: 0.2730618166814523, Median: 0.2749603658877995, Num: 111
Epoch: 284, Loss: 61.91708211439202, Learning Rate: 0.0005
Epoch: 285, Loss: 62.829700975532994, Learning Rate: 0.0005
Mean: 0.27208668911991657, Median: 0.26255689462611526, Num: 111
Epoch: 286, Loss: 53.57987848534641, Learning Rate: 0.0005
Epoch: 287, Loss: 67.2711055939456, Learning Rate: 0.0005
Mean: 0.27459800465421147, Median: 0.2831085283582272, Num: 111
Epoch: 288, Loss: 64.20124039592513, Learning Rate: 0.0005
Epoch: 289, Loss: 60.951721386737134, Learning Rate: 0.0005
Mean: 0.27065953558228245, Median: 0.2722695694930003, Num: 111
Epoch: 290, Loss: 57.66597245687462, Learning Rate: 0.0005
Epoch: 291, Loss: 60.231194415724424, Learning Rate: 0.0005
Mean: 0.276215318605974, Median: 0.26598643657825105, Num: 111
Epoch: 292, Loss: 57.25760384065559, Learning Rate: 0.0005
Epoch: 293, Loss: 54.70506599150508, Learning Rate: 0.0005
Mean: 0.271176472969739, Median: 0.28111951775047855, Num: 111
Epoch: 294, Loss: 58.04703370059829, Learning Rate: 0.0005
Epoch: 295, Loss: 61.73217460908086, Learning Rate: 0.0005
Mean: 0.2678953784271448, Median: 0.2614418876309138, Num: 111
Epoch: 296, Loss: 63.82496861377394, Learning Rate: 0.0005
Epoch: 297, Loss: 57.56508132061327, Learning Rate: 0.0005
Mean: 0.268504705671437, Median: 0.24770513762517085, Num: 111
Epoch: 298, Loss: 60.79449112444039, Learning Rate: 0.0005
Epoch: 299, Loss: 60.6388873594353, Learning Rate: 0.0005
Mean: 0.27183868391982463, Median: 0.26507901798087175, Num: 111
Epoch: 300, Loss: 57.089663953666225, Learning Rate: 0.0005
Epoch: 301, Loss: 63.33304466684181, Learning Rate: 0.0005
Mean: 0.2720639946527605, Median: 0.2646139519657994, Num: 111
Epoch: 302, Loss: 57.312130388007105, Learning Rate: 0.0005
Epoch: 303, Loss: 59.319265181759754, Learning Rate: 0.0005
Mean: 0.27245572847007243, Median: 0.2707610429198935, Num: 111
Epoch: 304, Loss: 62.581666383398584, Learning Rate: 0.0005
Epoch: 305, Loss: 59.71825033210846, Learning Rate: 0.0005
Mean: 0.2707923916788994, Median: 0.27009289513830176, Num: 111
Epoch: 306, Loss: 58.02170719008848, Learning Rate: 0.0005
Epoch: 307, Loss: 59.72972067867417, Learning Rate: 0.0005
Mean: 0.27263949042339125, Median: 0.27722574383966253, Num: 111
Epoch: 308, Loss: 58.49964672686106, Learning Rate: 0.0005
Epoch: 309, Loss: 55.183143931699085, Learning Rate: 0.0005
Mean: 0.26292336083153756, Median: 0.2542871040141221, Num: 111
Epoch: 310, Loss: 66.27726368731763, Learning Rate: 0.0005
Epoch: 311, Loss: 58.128069418022434, Learning Rate: 0.0005
Mean: 0.2753696312302405, Median: 0.28719159304489267, Num: 111
Epoch: 312, Loss: 58.48869826994746, Learning Rate: 0.0005
Epoch: 313, Loss: 58.122415841343894, Learning Rate: 0.0005
Mean: 0.27034165008011823, Median: 0.27376506441628007, Num: 111
Epoch: 314, Loss: 62.329928329192015, Learning Rate: 0.0005
Epoch: 315, Loss: 53.60749534239252, Learning Rate: 0.0005
Mean: 0.2681522790430596, Median: 0.2718804056384923, Num: 111
Epoch: 316, Loss: 55.47795486450195, Learning Rate: 0.0005
Epoch: 317, Loss: 61.85669702506927, Learning Rate: 0.0005
Mean: 0.2726580755861084, Median: 0.278422747476667, Num: 111
Epoch: 318, Loss: 59.740369658872304, Learning Rate: 0.0005
Epoch: 319, Loss: 58.553519191512144, Learning Rate: 0.0005
Mean: 0.2715220633367771, Median: 0.25874476780158756, Num: 111
Epoch: 320, Loss: 59.81599778439625, Learning Rate: 0.0005
Epoch: 321, Loss: 63.156767730253286, Learning Rate: 0.0005
Mean: 0.27525193434225703, Median: 0.25757925979251856, Num: 111
Epoch: 322, Loss: 60.61476820061006, Learning Rate: 0.0005
Epoch: 323, Loss: 60.52607208849436, Learning Rate: 0.0005
Mean: 0.26508403743015607, Median: 0.2734009237678977, Num: 111
Epoch: 324, Loss: 51.14148588065641, Learning Rate: 0.0005
Epoch: 325, Loss: 59.401576306446486, Learning Rate: 0.0005
Mean: 0.2710249601152979, Median: 0.2571790166083177, Num: 111
Epoch: 326, Loss: 55.8561198223068, Learning Rate: 0.0005
Epoch: 327, Loss: 56.36117575542036, Learning Rate: 0.0005
Mean: 0.27297149963047984, Median: 0.2776225616538719, Num: 111
Epoch: 328, Loss: 64.3628526021199, Learning Rate: 0.0005
Epoch: 329, Loss: 58.93532387607069, Learning Rate: 0.0005
Mean: 0.2710784161440214, Median: 0.27668672665528615, Num: 111
Epoch: 330, Loss: 56.360620073525304, Learning Rate: 0.0005
Epoch: 331, Loss: 63.23719049936317, Learning Rate: 0.0005
Mean: 0.2749243620397211, Median: 0.2753208791643252, Num: 111
Epoch: 332, Loss: 59.569200963859096, Learning Rate: 0.0005
Epoch: 333, Loss: 58.84091848350433, Learning Rate: 0.0005
Mean: 0.2728768658158062, Median: 0.272634672501236, Num: 111
Epoch: 334, Loss: 62.02012159738196, Learning Rate: 0.0005
Epoch: 335, Loss: 56.59100464740431, Learning Rate: 0.0005
Mean: 0.27139308308852134, Median: 0.26874559673320736, Num: 111
Epoch: 336, Loss: 52.44471092683723, Learning Rate: 0.0005
Epoch: 337, Loss: 59.2267499027482, Learning Rate: 0.0005
Mean: 0.2786295681298789, Median: 0.2818794609263398, Num: 111
Epoch: 338, Loss: 60.91385351893413, Learning Rate: 0.0005
Epoch: 339, Loss: 52.44508892656809, Learning Rate: 0.0005
Mean: 0.26935299747219005, Median: 0.2652808763157569, Num: 111
Epoch: 340, Loss: 62.16928892250521, Learning Rate: 0.0005
Epoch: 341, Loss: 56.607720363570984, Learning Rate: 0.0005
Mean: 0.27308910742123477, Median: 0.2678218343818176, Num: 111
Epoch: 342, Loss: 56.04485074008804, Learning Rate: 0.0005
Epoch: 343, Loss: 57.642550238643786, Learning Rate: 0.0005
Mean: 0.2749701728016174, Median: 0.27185911730137785, Num: 111
Epoch: 344, Loss: 55.21160560056388, Learning Rate: 0.0005
Epoch: 345, Loss: 58.52282296031355, Learning Rate: 0.0005
Mean: 0.2734999293255353, Median: 0.2813658262105454, Num: 111
Epoch: 346, Loss: 62.83321658674493, Learning Rate: 0.0005
Epoch: 347, Loss: 55.51064923297928, Learning Rate: 0.0005
Mean: 0.2683032567374554, Median: 0.26097863459583986, Num: 111
Epoch: 348, Loss: 61.71267105010619, Learning Rate: 0.0005
Epoch: 349, Loss: 64.06988326612725, Learning Rate: 0.0005
Mean: 0.27074932070869484, Median: 0.25739760293593145, Num: 111
Epoch: 350, Loss: 59.06088856616652, Learning Rate: 0.0005
Epoch: 351, Loss: 54.55709571148976, Learning Rate: 0.0005
Mean: 0.27190817484383056, Median: 0.27316087469282896, Num: 111
Epoch: 352, Loss: 57.82058540597019, Learning Rate: 0.0005
Epoch: 353, Loss: 56.70331980234169, Learning Rate: 0.0005
Mean: 0.27277937543088343, Median: 0.2704635844117208, Num: 111
Epoch: 354, Loss: 53.97216780788927, Learning Rate: 0.0005
Epoch: 355, Loss: 59.13307640351445, Learning Rate: 0.0005
Mean: 0.27522628235258756, Median: 0.27418693890777, Num: 111
Epoch: 356, Loss: 57.95997004911124, Learning Rate: 0.0005
Epoch: 357, Loss: 63.21106351140034, Learning Rate: 0.0005
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
Mean: 0.27448932932890635, Median: 0.2617258296871896, Num: 111
Epoch: 358, Loss: 58.57026966508612, Learning Rate: 0.0005
Epoch: 359, Loss: 55.60858211172632, Learning Rate: 0.0005
Mean: 0.26717063839699773, Median: 0.25495844812051416, Num: 111
Epoch: 360, Loss: 54.15848714759551, Learning Rate: 0.0005
Epoch: 361, Loss: 57.72293110353401, Learning Rate: 0.0005
Mean: 0.2694357078587181, Median: 0.27734749169794515, Num: 111
Epoch: 362, Loss: 58.33955036301211, Learning Rate: 0.0005
Epoch: 363, Loss: 58.666583578270604, Learning Rate: 0.0005
Mean: 0.2723524255753258, Median: 0.2516814251306979, Num: 111
Epoch: 364, Loss: 64.05307716921152, Learning Rate: 0.0005
Epoch: 365, Loss: 67.20249057677854, Learning Rate: 0.0005
Mean: 0.2736949594921722, Median: 0.2985108108556635, Num: 111
Epoch: 366, Loss: 57.07730564439153, Learning Rate: 0.0005
Epoch: 367, Loss: 59.632174658488076, Learning Rate: 0.0005
Mean: 0.2744267347994198, Median: 0.2624502827976963, Num: 111
Epoch: 368, Loss: 61.052516500633885, Learning Rate: 0.0005
Epoch: 369, Loss: 57.03562370553074, Learning Rate: 0.0005
Mean: 0.2766250839102814, Median: 0.2674572242057419, Num: 111
Epoch: 370, Loss: 54.7797513065568, Learning Rate: 0.0005
Epoch: 371, Loss: 61.360778762633544, Learning Rate: 0.0005
Mean: 0.27792471997587415, Median: 0.2703083179701041, Num: 111
Epoch: 372, Loss: 57.47424293426146, Learning Rate: 0.0005
Epoch: 373, Loss: 58.930299529110094, Learning Rate: 0.0005
Mean: 0.27814980851316823, Median: 0.27546012063382286, Num: 111
Epoch: 374, Loss: 48.030492897493296, Learning Rate: 0.0005
Epoch: 375, Loss: 60.73681339585637, Learning Rate: 0.0005
Mean: 0.27238498219290314, Median: 0.2541448867473426, Num: 111
Epoch: 376, Loss: 53.12296177967485, Learning Rate: 0.0005
Epoch: 377, Loss: 62.454867466386546, Learning Rate: 0.0005
Mean: 0.2725493162799626, Median: 0.27251868575450955, Num: 111
Epoch: 378, Loss: 63.47532411942999, Learning Rate: 0.0005
Epoch: 379, Loss: 64.20296384053059, Learning Rate: 0.0005
Mean: 0.274685252437446, Median: 0.2851085874142557, Num: 111
Epoch: 380, Loss: 55.10401957868093, Learning Rate: 0.0005
Epoch: 381, Loss: 56.82778665818364, Learning Rate: 0.0005
Mean: 0.2715351285983662, Median: 0.2547113377363744, Num: 111
Epoch: 382, Loss: 60.64957233796637, Learning Rate: 0.0005
Epoch: 383, Loss: 62.83068362776056, Learning Rate: 0.0005
Mean: 0.27605406386864845, Median: 0.27414768732568084, Num: 111
Epoch: 384, Loss: 58.26659779376294, Learning Rate: 0.0005
Epoch: 385, Loss: 61.58749552807176, Learning Rate: 0.0005
Mean: 0.2744588196663674, Median: 0.2618427734369567, Num: 111
Epoch: 386, Loss: 65.79684344831719, Learning Rate: 0.0005
Epoch: 387, Loss: 61.08186076060835, Learning Rate: 0.0005
Mean: 0.27025523130835527, Median: 0.2567807750269239, Num: 111
Epoch: 388, Loss: 67.60176637948277, Learning Rate: 0.0005
Epoch: 389, Loss: 57.47916593321835, Learning Rate: 0.0005
Mean: 0.27319093102751374, Median: 0.2791525161930166, Num: 111
Epoch: 390, Loss: 58.29179648893425, Learning Rate: 0.0005
Epoch: 391, Loss: 62.53732846731163, Learning Rate: 0.0005
Mean: 0.2731780426598297, Median: 0.2806191008865523, Num: 111
Epoch: 392, Loss: 59.1504296222365, Learning Rate: 0.0005
Epoch: 393, Loss: 54.19805791004595, Learning Rate: 0.0005
Mean: 0.26936242703651647, Median: 0.2780274771583212, Num: 111
Epoch: 394, Loss: 60.967213366405076, Learning Rate: 0.0005
Epoch: 395, Loss: 54.88628894736968, Learning Rate: 0.0005
Mean: 0.27837864268731005, Median: 0.27120176512159194, Num: 111
Epoch: 396, Loss: 67.60884388383613, Learning Rate: 0.0005
Epoch: 397, Loss: 56.457069787634424, Learning Rate: 0.0005
Mean: 0.27971239261770986, Median: 0.2787235954792094, Num: 111
Epoch: 398, Loss: 50.946384791868276, Learning Rate: 0.0005
Epoch: 399, Loss: 56.34092467664236, Learning Rate: 0.0005
Mean: 0.2774789389649885, Median: 0.27830310940209835, Num: 111
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
10
Epoch: 0, Loss: 237.7852478946548, Learning Rate: 0.0005
Epoch: 1, Loss: 188.79373187329395, Learning Rate: 0.0005
Mean: 0.06204670371032558, Median: 0.017123699409738322, Num: 111
Epoch: 2, Loss: 172.71820693418204, Learning Rate: 0.0005
Epoch: 3, Loss: 170.13523533832597, Learning Rate: 0.0005
Mean: 0.07797028673589833, Median: 0.03307859564430433, Num: 111
Epoch: 4, Loss: 156.08162312335278, Learning Rate: 0.0005
Epoch: 5, Loss: 156.50388235069184, Learning Rate: 0.0005
Mean: 0.1169721884236163, Median: 0.07193628253519123, Num: 111
Epoch: 6, Loss: 152.25463802843208, Learning Rate: 0.0005
Epoch: 7, Loss: 148.3303795780044, Learning Rate: 0.0005
Mean: 0.1258818484053201, Median: 0.09426089204572845, Num: 111
Epoch: 8, Loss: 131.51728965575435, Learning Rate: 0.0005
Epoch: 9, Loss: 139.9363239701972, Learning Rate: 0.0005
Mean: 0.14750718208093663, Median: 0.10895158390607265, Num: 111
Epoch: 10, Loss: 132.66107104198042, Learning Rate: 0.0005
Epoch: 11, Loss: 135.1823987845915, Learning Rate: 0.0005
Mean: 0.16702684078263802, Median: 0.12372608713897396, Num: 111
Epoch: 12, Loss: 132.90031166536264, Learning Rate: 0.0005
Epoch: 13, Loss: 126.8004946421428, Learning Rate: 0.0005
Mean: 0.15822182509742358, Median: 0.11855654582379309, Num: 111
Epoch: 14, Loss: 129.77210548125117, Learning Rate: 0.0005
Epoch: 15, Loss: 121.5500030057976, Learning Rate: 0.0005
Mean: 0.17747055306506354, Median: 0.15114170475973995, Num: 111
Epoch: 16, Loss: 123.29650024046381, Learning Rate: 0.0005
Epoch: 17, Loss: 119.90967794211514, Learning Rate: 0.0005
Mean: 0.18949177538626974, Median: 0.16421263364849942, Num: 111
Epoch: 18, Loss: 124.55686481889472, Learning Rate: 0.0005
Epoch: 19, Loss: 110.35963711106633, Learning Rate: 0.0005
Mean: 0.18765061732642063, Median: 0.1517051033142924, Num: 111
Epoch: 20, Loss: 112.56247561810964, Learning Rate: 0.0005
Epoch: 21, Loss: 107.48387904339526, Learning Rate: 0.0005
Mean: 0.1888258213444637, Median: 0.15506776661761512, Num: 111
Epoch: 22, Loss: 113.11459470082478, Learning Rate: 0.0005
Epoch: 23, Loss: 109.28750182921628, Learning Rate: 0.0005
Mean: 0.19208194523266456, Median: 0.1708560157416357, Num: 111
Epoch: 24, Loss: 108.44307136535645, Learning Rate: 0.0005
Epoch: 25, Loss: 105.4483665558229, Learning Rate: 0.0005
Mean: 0.19737008965071653, Median: 0.16822314279968298, Num: 111
Epoch: 26, Loss: 103.07324981689453, Learning Rate: 0.0005
Epoch: 27, Loss: 102.84756722507707, Learning Rate: 0.0005
Mean: 0.19017213199049682, Median: 0.16068216085613782, Num: 111
Epoch: 28, Loss: 104.32875780312412, Learning Rate: 0.0005
Epoch: 29, Loss: 108.79199556557529, Learning Rate: 0.0005
Mean: 0.20027358538483306, Median: 0.1882660091644182, Num: 111
Epoch: 30, Loss: 99.79992305801575, Learning Rate: 0.0005
Epoch: 31, Loss: 100.65632583434324, Learning Rate: 0.0005
Mean: 0.19417007411262382, Median: 0.17233473598884116, Num: 111
Epoch: 32, Loss: 97.22057009317788, Learning Rate: 0.0005
Epoch: 33, Loss: 98.5525670568627, Learning Rate: 0.0005
Mean: 0.2024795787810107, Median: 0.17760616053300424, Num: 111
Epoch: 34, Loss: 95.82342940640737, Learning Rate: 0.0005
Epoch: 35, Loss: 91.75034465559993, Learning Rate: 0.0005
Mean: 0.20000732308917074, Median: 0.17972090081313818, Num: 111
Epoch: 36, Loss: 103.59021019073855, Learning Rate: 0.0005
Epoch: 37, Loss: 93.63693485489812, Learning Rate: 0.0005
Mean: 0.20604177763195575, Median: 0.1880610985777783, Num: 111
Epoch: 38, Loss: 89.72106308534921, Learning Rate: 0.0005
Epoch: 39, Loss: 92.24267394284168, Learning Rate: 0.0005
Mean: 0.20894719449027555, Median: 0.20060657809900387, Num: 111
Epoch: 40, Loss: 100.45019216422575, Learning Rate: 0.0005
Epoch: 41, Loss: 97.76524511590061, Learning Rate: 0.0005
Mean: 0.21083894796759037, Median: 0.21489662283259245, Num: 111
Epoch: 42, Loss: 90.47036182449524, Learning Rate: 0.0005
Epoch: 43, Loss: 88.65365517857563, Learning Rate: 0.0005
Mean: 0.21312659377783305, Median: 0.20050422273133844, Num: 111
Epoch: 44, Loss: 92.19912090071712, Learning Rate: 0.0005
Epoch: 45, Loss: 85.78003026203938, Learning Rate: 0.0005
Mean: 0.21830014326520744, Median: 0.20716985530539245, Num: 111
Epoch: 46, Loss: 88.37025486130312, Learning Rate: 0.0005
Epoch: 47, Loss: 87.28888610472163, Learning Rate: 0.0005
Mean: 0.2209533050031331, Median: 0.19997120207839905, Num: 111
Epoch: 48, Loss: 90.43760729410562, Learning Rate: 0.0005
Epoch: 49, Loss: 90.136377173734, Learning Rate: 0.0005
Mean: 0.22345856117473725, Median: 0.21722279286331966, Num: 111
Epoch: 50, Loss: 90.6383623789592, Learning Rate: 0.0005
Epoch: 51, Loss: 88.01894066132695, Learning Rate: 0.0005
Mean: 0.21298687471049813, Median: 0.19547609579260855, Num: 111
Epoch: 52, Loss: 91.42397793229804, Learning Rate: 0.0005
Epoch: 53, Loss: 81.54504697868623, Learning Rate: 0.0005
Mean: 0.21111391031003995, Median: 0.20131305995469997, Num: 111
Epoch: 54, Loss: 84.76263280661709, Learning Rate: 0.0005
Epoch: 55, Loss: 88.97580682225974, Learning Rate: 0.0005
Mean: 0.22736681833634453, Median: 0.21497941068541762, Num: 111
Epoch: 56, Loss: 81.93860460763955, Learning Rate: 0.0005
Epoch: 57, Loss: 85.56394503490034, Learning Rate: 0.0005
Mean: 0.22456741087419743, Median: 0.2078623417214385, Num: 111
Epoch: 58, Loss: 85.60139260809106, Learning Rate: 0.0005
Epoch: 59, Loss: 89.58931942445686, Learning Rate: 0.0005
Mean: 0.21920929802877503, Median: 0.2093899823532861, Num: 111
Epoch: 60, Loss: 83.99333956155432, Learning Rate: 0.0005
Epoch: 61, Loss: 82.79317359464714, Learning Rate: 0.0005
Mean: 0.2267583165931017, Median: 0.20780986974776416, Num: 111
Epoch: 62, Loss: 87.00652887735022, Learning Rate: 0.0005
Epoch: 63, Loss: 74.20951485921101, Learning Rate: 0.0005
Mean: 0.23100549856460306, Median: 0.2233184238563206, Num: 111
Epoch: 64, Loss: 82.48822598284985, Learning Rate: 0.0005
Epoch: 65, Loss: 79.14036371621741, Learning Rate: 0.0005
Mean: 0.22578840277333018, Median: 0.2064623038902669, Num: 111
Epoch: 66, Loss: 75.3748748388635, Learning Rate: 0.0005
Epoch: 67, Loss: 78.92690150709038, Learning Rate: 0.0005
Mean: 0.23055271070792982, Median: 0.22136924322915047, Num: 111
Epoch: 68, Loss: 75.16955327413169, Learning Rate: 0.0005
Epoch: 69, Loss: 85.94333436115679, Learning Rate: 0.0005
Mean: 0.23581399325190072, Median: 0.24434846711167582, Num: 111
Epoch: 70, Loss: 78.71492790314089, Learning Rate: 0.0005
Epoch: 71, Loss: 81.31113847479763, Learning Rate: 0.0005
Mean: 0.2383255105506737, Median: 0.22434129112575424, Num: 111
Epoch: 72, Loss: 79.7267252450966, Learning Rate: 0.0005
Epoch: 73, Loss: 77.62897769514336, Learning Rate: 0.0005
Mean: 0.23261091554936164, Median: 0.22144419597114787, Num: 111
Epoch: 74, Loss: 84.09753569637436, Learning Rate: 0.0005
Epoch: 75, Loss: 82.22331348097468, Learning Rate: 0.0005
Mean: 0.23378190352814934, Median: 0.23827804577009054, Num: 111
Epoch: 76, Loss: 77.9672296133386, Learning Rate: 0.0005
Epoch: 77, Loss: 81.24851298044963, Learning Rate: 0.0005
Mean: 0.23491943906023435, Median: 0.22670722954176545, Num: 111
Epoch: 78, Loss: 77.47338622449392, Learning Rate: 0.0005
Epoch: 79, Loss: 75.33890331222351, Learning Rate: 0.0005
Mean: 0.2372204547807032, Median: 0.23118922622352814, Num: 111
Epoch: 80, Loss: 72.94470908842891, Learning Rate: 0.0005
Epoch: 81, Loss: 72.19225210166839, Learning Rate: 0.0005
Mean: 0.23745214785661728, Median: 0.23790643707484901, Num: 111
Epoch: 82, Loss: 74.13639344364763, Learning Rate: 0.0005
Epoch: 83, Loss: 74.13332415201577, Learning Rate: 0.0005
Mean: 0.23886574988944906, Median: 0.2226930379436224, Num: 111
Epoch: 84, Loss: 71.34974214255092, Learning Rate: 0.0005
Epoch: 85, Loss: 67.23976089293699, Learning Rate: 0.0005
Mean: 0.22833735916003153, Median: 0.22046930085484207, Num: 111
Epoch: 86, Loss: 78.21604015166501, Learning Rate: 0.0005
Epoch: 87, Loss: 91.76144875675799, Learning Rate: 0.0005
Mean: 0.2457005054597053, Median: 0.26191755897861396, Num: 111
Epoch: 88, Loss: 68.42142333754573, Learning Rate: 0.0005
Epoch: 89, Loss: 74.7180903561144, Learning Rate: 0.0005
Mean: 0.24133706731070673, Median: 0.221048867910263, Num: 111
Epoch: 90, Loss: 68.62607186099133, Learning Rate: 0.0005
Epoch: 91, Loss: 62.37846016022096, Learning Rate: 0.0005
Mean: 0.24905062951661727, Median: 0.25404657898659877, Num: 111
Epoch: 92, Loss: 68.27383533156062, Learning Rate: 0.0005
Epoch: 93, Loss: 76.0414820751512, Learning Rate: 0.0005
Mean: 0.24700211958002366, Median: 0.24755117461302895, Num: 111
Epoch: 94, Loss: 67.3836050378271, Learning Rate: 0.0005
Epoch: 95, Loss: 64.6812652909612, Learning Rate: 0.0005
Mean: 0.24337808012833198, Median: 0.22208383071129154, Num: 111
Epoch: 96, Loss: 70.64205307558359, Learning Rate: 0.0005
Epoch: 97, Loss: 74.66269417268684, Learning Rate: 0.0005
Mean: 0.24449823790791228, Median: 0.2529399622836473, Num: 111
Epoch: 98, Loss: 71.5989313929914, Learning Rate: 0.0005
Epoch: 99, Loss: 72.90025290523667, Learning Rate: 0.0005
Mean: 0.24732326661390178, Median: 0.23860856987302193, Num: 111
Epoch: 100, Loss: 74.30765399013657, Learning Rate: 0.0005
Epoch: 101, Loss: 63.50576067545328, Learning Rate: 0.0005
Mean: 0.2425022050623791, Median: 0.23878821587599017, Num: 111
Epoch: 102, Loss: 76.25470043090452, Learning Rate: 0.0005
Epoch: 103, Loss: 72.01329860917058, Learning Rate: 0.0005
Mean: 0.24694439133138063, Median: 0.2721993026390045, Num: 111
Epoch: 104, Loss: 68.8275880698698, Learning Rate: 0.0005
Epoch: 105, Loss: 64.3609569848302, Learning Rate: 0.0005
Mean: 0.24836426937999553, Median: 0.22727762548525626, Num: 111
Epoch: 106, Loss: 71.79298773156592, Learning Rate: 0.0005
Epoch: 107, Loss: 66.70714674800276, Learning Rate: 0.0005
Mean: 0.24870281741877148, Median: 0.24409817703937065, Num: 111
Epoch: 108, Loss: 69.25876635815723, Learning Rate: 0.0005
Epoch: 109, Loss: 70.24052193653152, Learning Rate: 0.0005
Mean: 0.24363945001353504, Median: 0.23116002399094585, Num: 111
Epoch: 110, Loss: 59.88445732392461, Learning Rate: 0.0005
Epoch: 111, Loss: 64.80085336156638, Learning Rate: 0.0005
Mean: 0.23956878734021445, Median: 0.2267555669847943, Num: 111
Epoch: 112, Loss: 65.64137782820735, Learning Rate: 0.0005
Epoch: 113, Loss: 67.95926074522087, Learning Rate: 0.0005
Mean: 0.24239541470271495, Median: 0.23085544947277864, Num: 111
Epoch: 114, Loss: 65.23410923509712, Learning Rate: 0.0005
Epoch: 115, Loss: 73.72173145018428, Learning Rate: 0.0005
Mean: 0.2508767693114185, Median: 0.24552169365160062, Num: 111
Epoch: 116, Loss: 65.55857357346868, Learning Rate: 0.0005
Epoch: 117, Loss: 65.00354509468538, Learning Rate: 0.0005
Mean: 0.24914401439706846, Median: 0.23896698243030234, Num: 111
Epoch: 118, Loss: 73.73144551932094, Learning Rate: 0.0005
Epoch: 119, Loss: 73.93407111570059, Learning Rate: 0.0005
Mean: 0.24478541857292524, Median: 0.24212477778092067, Num: 111
Epoch: 120, Loss: 61.342917591692455, Learning Rate: 0.0005
Epoch: 121, Loss: 78.78514899977718, Learning Rate: 0.0005
Mean: 0.24246779053416292, Median: 0.22090021550532712, Num: 111
Epoch: 122, Loss: 71.95816956945212, Learning Rate: 0.0005
Epoch: 123, Loss: 63.64636156932417, Learning Rate: 0.0005
Mean: 0.24999033556823072, Median: 0.24591315743140843, Num: 111
Epoch: 124, Loss: 71.42139545118953, Learning Rate: 0.0005
Epoch: 125, Loss: 61.76443164319877, Learning Rate: 0.0005
Mean: 0.251564109896526, Median: 0.2454475262164789, Num: 111
Epoch: 126, Loss: 68.22296891729516, Learning Rate: 0.0005
Epoch: 127, Loss: 66.52445006083293, Learning Rate: 0.0005
Mean: 0.2466292180519304, Median: 0.25585017306229435, Num: 111
Epoch: 128, Loss: 67.23187196111104, Learning Rate: 0.0005
Epoch: 129, Loss: 70.04543357297598, Learning Rate: 0.0005
Mean: 0.24752127452690517, Median: 0.2414623688612121, Num: 111
Epoch: 130, Loss: 68.65421536457107, Learning Rate: 0.0005
Epoch: 131, Loss: 60.2419129566974, Learning Rate: 0.0005
Mean: 0.2528683698323706, Median: 0.22172247680573073, Num: 111
Epoch: 132, Loss: 69.25975335362446, Learning Rate: 0.0005
Epoch: 133, Loss: 60.16375607180308, Learning Rate: 0.0005
Mean: 0.2565935091200574, Median: 0.23509809267506782, Num: 111
Epoch: 134, Loss: 63.830707193857215, Learning Rate: 0.0005
Epoch: 135, Loss: 64.24992730244097, Learning Rate: 0.0005
Mean: 0.25231164654270805, Median: 0.2449811595615717, Num: 111
Epoch: 136, Loss: 63.594483708760826, Learning Rate: 0.0005
Epoch: 137, Loss: 75.77257837732154, Learning Rate: 0.0005
Mean: 0.2492036443731814, Median: 0.2570787154274469, Num: 111
Epoch: 138, Loss: 62.70341078057346, Learning Rate: 0.0005
Epoch: 139, Loss: 65.23360684406326, Learning Rate: 0.0005
Mean: 0.2537170559000007, Median: 0.24909492324439272, Num: 111
Epoch: 140, Loss: 63.17515033124441, Learning Rate: 0.0005
Epoch: 141, Loss: 63.906843828867714, Learning Rate: 0.0005
Mean: 0.25123496806100515, Median: 0.23728664128975255, Num: 111
Epoch: 142, Loss: 68.71887115110835, Learning Rate: 0.0005
Epoch: 143, Loss: 61.81375921777932, Learning Rate: 0.0005
Mean: 0.25203504078962374, Median: 0.22683137404095804, Num: 111
Epoch: 144, Loss: 68.45506024647908, Learning Rate: 0.0005
Epoch: 145, Loss: 63.88976455596556, Learning Rate: 0.0005
Mean: 0.2561738618483246, Median: 0.2473023905911022, Num: 111
Epoch: 146, Loss: 69.99956641139754, Learning Rate: 0.0005
Epoch: 147, Loss: 67.91162057669766, Learning Rate: 0.0005
Mean: 0.25168990462854995, Median: 0.23622354800986792, Num: 111
Epoch: 148, Loss: 70.57069948495152, Learning Rate: 0.0005
Epoch: 149, Loss: 64.37358079473657, Learning Rate: 0.0005
Mean: 0.2504648132457771, Median: 0.23393148193579014, Num: 111
Epoch: 150, Loss: 59.462753916361244, Learning Rate: 0.0005
Epoch: 151, Loss: 70.8046330601336, Learning Rate: 0.0005
Mean: 0.25729946582358515, Median: 0.2477218776403567, Num: 111
Epoch: 152, Loss: 68.98569348346756, Learning Rate: 0.0005
Epoch: 153, Loss: 61.082691870540025, Learning Rate: 0.0005
Mean: 0.25577658749040444, Median: 0.2402352129592419, Num: 111
Epoch: 154, Loss: 62.24398994445801, Learning Rate: 0.0005
Epoch: 155, Loss: 60.346733943525564, Learning Rate: 0.0005
Mean: 0.25055326962656876, Median: 0.2374747265133857, Num: 111
Epoch: 156, Loss: 66.11639994885547, Learning Rate: 0.0005
Epoch: 157, Loss: 76.79606109067618, Learning Rate: 0.0005
Mean: 0.25172589780990984, Median: 0.2546951599610796, Num: 111
Epoch: 158, Loss: 62.54467898679067, Learning Rate: 0.0005
Epoch: 159, Loss: 61.62587016461843, Learning Rate: 0.0005
Mean: 0.2604124596144654, Median: 0.24469157450674625, Num: 111
Epoch: 160, Loss: 65.82227847087815, Learning Rate: 0.0005
Epoch: 161, Loss: 63.81132248798048, Learning Rate: 0.0005
Mean: 0.2613637228548307, Median: 0.25664265213607973, Num: 111
Epoch: 162, Loss: 63.774723857282154, Learning Rate: 0.0005
Epoch: 163, Loss: 62.09342116620167, Learning Rate: 0.0005
Mean: 0.2480700456305631, Median: 0.25187728096498746, Num: 111
Epoch: 164, Loss: 67.8760476169816, Learning Rate: 0.0005
Epoch: 165, Loss: 62.729954397822, Learning Rate: 0.0005
Mean: 0.25906313949246007, Median: 0.24670875957663832, Num: 111
Epoch: 166, Loss: 62.76143772630807, Learning Rate: 0.0005
Epoch: 167, Loss: 64.42104293639402, Learning Rate: 0.0005
Mean: 0.25777278101525947, Median: 0.2617845990544553, Num: 111
Epoch: 168, Loss: 59.003729406609594, Learning Rate: 0.0005
Epoch: 169, Loss: 61.62951648666198, Learning Rate: 0.0005
Mean: 0.2614111066279896, Median: 0.24506781291645444, Num: 111
Epoch: 170, Loss: 64.40074893078172, Learning Rate: 0.0005
Epoch: 171, Loss: 63.643003934837246, Learning Rate: 0.0005
Mean: 0.25157220242541706, Median: 0.23154006976967612, Num: 111
Epoch: 172, Loss: 62.23595465234963, Learning Rate: 0.0005
Epoch: 173, Loss: 65.32320533890322, Learning Rate: 0.0005
Mean: 0.2570838998743895, Median: 0.24002851243854714, Num: 111
Epoch: 174, Loss: 58.540702153401206, Learning Rate: 0.0005
Epoch: 175, Loss: 65.38360635918308, Learning Rate: 0.0005
Mean: 0.2557524565768762, Median: 0.24547617182401998, Num: 111
Epoch: 176, Loss: 64.39807894143713, Learning Rate: 0.0005
Epoch: 177, Loss: 60.81795439662704, Learning Rate: 0.0005
Mean: 0.25674979879461163, Median: 0.25228912457851754, Num: 111
Epoch: 178, Loss: 61.64315521286195, Learning Rate: 0.0005
Epoch: 179, Loss: 67.70832754617714, Learning Rate: 0.0005
Mean: 0.2605562872015544, Median: 0.2595983536501799, Num: 111
Epoch: 180, Loss: 62.801424175859935, Learning Rate: 0.0005
Epoch: 181, Loss: 70.48010327442583, Learning Rate: 0.0005
Mean: 0.26058703645169273, Median: 0.25126540049642804, Num: 111
Epoch: 182, Loss: 68.24542740741408, Learning Rate: 0.0005
Epoch: 183, Loss: 65.09363565100244, Learning Rate: 0.0005
Mean: 0.2609079081376201, Median: 0.26112202435038495, Num: 111
Epoch: 184, Loss: 64.61019561951419, Learning Rate: 0.0005
Epoch: 185, Loss: 59.73682010030172, Learning Rate: 0.0005
Mean: 0.26082086157119133, Median: 0.24937341708051378, Num: 111
Epoch: 186, Loss: 56.29146976930549, Learning Rate: 0.0005
Epoch: 187, Loss: 53.543586248374844, Learning Rate: 0.0005
Mean: 0.259320075141213, Median: 0.24449100153989745, Num: 111
Epoch: 188, Loss: 68.58333025782942, Learning Rate: 0.0005
Epoch: 189, Loss: 63.877265780805104, Learning Rate: 0.0005
Mean: 0.25938108402820775, Median: 0.24922585421171592, Num: 111
Epoch: 190, Loss: 66.71093422533518, Learning Rate: 0.0005
Epoch: 191, Loss: 60.385041271347596, Learning Rate: 0.0005
Mean: 0.26414504072374345, Median: 0.25631876017726185, Num: 111
Epoch: 192, Loss: 63.73749305541257, Learning Rate: 0.0005
Epoch: 193, Loss: 65.3000422443252, Learning Rate: 0.0005
Mean: 0.2568080216743125, Median: 0.2402682694832486, Num: 111
Epoch: 194, Loss: 61.45141167238534, Learning Rate: 0.0005
Epoch: 195, Loss: 64.96730873383672, Learning Rate: 0.0005
Mean: 0.2584624853169639, Median: 0.246937188958071, Num: 111
Epoch: 196, Loss: 61.11830281636801, Learning Rate: 0.0005
Epoch: 197, Loss: 63.81242255704949, Learning Rate: 0.0005
Mean: 0.265895034675619, Median: 0.25610209457431, Num: 111
Epoch: 198, Loss: 59.557271980377564, Learning Rate: 0.0005
Epoch: 199, Loss: 73.79326197612717, Learning Rate: 0.0005
Mean: 0.2555905895358937, Median: 0.2513368445367925, Num: 111
Epoch: 200, Loss: 60.922564460570555, Learning Rate: 0.0005
Epoch: 201, Loss: 68.80492813615913, Learning Rate: 0.0005
Mean: 0.261721935810338, Median: 0.24204329887860146, Num: 111
Epoch: 202, Loss: 59.04107100705066, Learning Rate: 0.0005
Epoch: 203, Loss: 62.10911628998906, Learning Rate: 0.0005
Mean: 0.2579694856609664, Median: 0.23966368434276525, Num: 111
Epoch: 204, Loss: 57.84530075486884, Learning Rate: 0.0005
Epoch: 205, Loss: 59.453135754688674, Learning Rate: 0.0005
Mean: 0.26492459925519785, Median: 0.25549253536815564, Num: 111
Epoch: 206, Loss: 60.7043210684535, Learning Rate: 0.0005
Epoch: 207, Loss: 62.92836522481527, Learning Rate: 0.0005
Mean: 0.2661901782335544, Median: 0.25633243373307174, Num: 111
Epoch: 208, Loss: 65.85156980767307, Learning Rate: 0.0005
Epoch: 209, Loss: 66.04981021421501, Learning Rate: 0.0005
Mean: 0.2571438539068745, Median: 0.24896059735375614, Num: 111
Epoch: 210, Loss: 54.83622571646449, Learning Rate: 0.0005
Epoch: 211, Loss: 60.991622166461255, Learning Rate: 0.0005
Mean: 0.2609722986024245, Median: 0.2515396115972357, Num: 111
Epoch: 212, Loss: 67.80230029232531, Learning Rate: 0.0005
Epoch: 213, Loss: 53.240285689572254, Learning Rate: 0.0005
Mean: 0.26218983961784537, Median: 0.24591415331261057, Num: 111
Epoch: 214, Loss: 60.63829401314977, Learning Rate: 0.0005
Epoch: 215, Loss: 59.7271080936294, Learning Rate: 0.0005
Mean: 0.25930359733784053, Median: 0.2373197722112756, Num: 111
Epoch: 216, Loss: 64.56119265039283, Learning Rate: 0.0005
Epoch: 217, Loss: 61.062646682003894, Learning Rate: 0.0005
Mean: 0.2580297422776685, Median: 0.23739951196293307, Num: 111
Epoch: 218, Loss: 56.1887350427099, Learning Rate: 0.0005
Epoch: 219, Loss: 60.82341028696083, Learning Rate: 0.0005
Mean: 0.26242357881211237, Median: 0.23372553398839582, Num: 111
Epoch: 220, Loss: 62.360696195119836, Learning Rate: 0.0005
Epoch: 221, Loss: 63.55890581406743, Learning Rate: 0.0005
Mean: 0.26044443878294815, Median: 0.24019615265484281, Num: 111
Epoch: 222, Loss: 63.03368034132992, Learning Rate: 0.0005
Epoch: 223, Loss: 63.55521911598114, Learning Rate: 0.0005
Mean: 0.265268065420129, Median: 0.24456371083818215, Num: 111
Epoch: 224, Loss: 65.40849301901208, Learning Rate: 0.0005
Epoch: 225, Loss: 62.1996842292418, Learning Rate: 0.0005
Mean: 0.26439126669443186, Median: 0.26037909340063387, Num: 111
Epoch: 226, Loss: 68.55974008950842, Learning Rate: 0.0005
Epoch: 227, Loss: 57.44610478504595, Learning Rate: 0.0005
Mean: 0.2590250712653755, Median: 0.23968175717389947, Num: 111
Epoch: 228, Loss: 66.86322598284985, Learning Rate: 0.0005
Epoch: 229, Loss: 53.27781967944409, Learning Rate: 0.0005
Mean: 0.2645254653139022, Median: 0.2470551889942952, Num: 111
Epoch: 230, Loss: 61.69419995273452, Learning Rate: 0.0005
Epoch: 231, Loss: 55.61607245939324, Learning Rate: 0.0005
Mean: 0.26519821100912266, Median: 0.26054292624593345, Num: 111
Epoch: 232, Loss: 66.62770254066191, Learning Rate: 0.0005
Epoch: 233, Loss: 58.75533308465797, Learning Rate: 0.0005
Mean: 0.2646270568931454, Median: 0.2658255101984464, Num: 111
Epoch: 234, Loss: 60.31668806650553, Learning Rate: 0.0005
Epoch: 235, Loss: 60.98771306692836, Learning Rate: 0.0005
Mean: 0.2638702226190788, Median: 0.25317293445192374, Num: 111
Epoch: 236, Loss: 51.29506563853069, Learning Rate: 0.0005
Epoch: 237, Loss: 58.28414553906544, Learning Rate: 0.0005
Mean: 0.2674162827895943, Median: 0.24924946529702816, Num: 111
Epoch: 238, Loss: 64.18978789915522, Learning Rate: 0.0005
Epoch: 239, Loss: 57.98480061450636, Learning Rate: 0.0005
Mean: 0.264087524682716, Median: 0.25721927555800966, Num: 111
Epoch: 240, Loss: 51.74102153548275, Learning Rate: 0.0005
Epoch: 241, Loss: 63.8033447495426, Learning Rate: 0.0005
Mean: 0.2619227811780163, Median: 0.24763610590885635, Num: 111
Epoch: 242, Loss: 62.11816395334451, Learning Rate: 0.0005
Epoch: 243, Loss: 55.078977745699596, Learning Rate: 0.0005
Mean: 0.25664300675256424, Median: 0.2320033803149758, Num: 111
Epoch: 244, Loss: 55.41199485365167, Learning Rate: 0.0005
Epoch: 245, Loss: 63.16545057871256, Learning Rate: 0.0005
Mean: 0.2602128605088511, Median: 0.255796537992306, Num: 111
Epoch: 246, Loss: 57.84447692963014, Learning Rate: 0.0005
Epoch: 247, Loss: 65.66085362721638, Learning Rate: 0.0005
Mean: 0.2667402521303314, Median: 0.252814054531039, Num: 111
Epoch: 248, Loss: 57.2666474307876, Learning Rate: 0.0005
Epoch: 249, Loss: 64.34885829328054, Learning Rate: 0.0005
Mean: 0.2574429594437556, Median: 0.24171652696665127, Num: 111
Epoch: 250, Loss: 59.34658932973103, Learning Rate: 0.0005
Epoch: 251, Loss: 60.087012830987035, Learning Rate: 0.0005
Mean: 0.2617864797060289, Median: 0.2791451451704542, Num: 111
Epoch: 252, Loss: 62.63700019882386, Learning Rate: 0.0005
Epoch: 253, Loss: 70.77786117002188, Learning Rate: 0.0005
Mean: 0.25793391375590274, Median: 0.2484588145736277, Num: 111
Epoch: 254, Loss: 64.03072346836687, Learning Rate: 0.0005
Epoch: 255, Loss: 60.037713866635976, Learning Rate: 0.0005
Mean: 0.2609062733692042, Median: 0.24357141301701937, Num: 111
Epoch: 256, Loss: 58.17101765253458, Learning Rate: 0.0005
Epoch: 257, Loss: 53.87437900864934, Learning Rate: 0.0005
Mean: 0.2660463687564749, Median: 0.2598489297870174, Num: 111
Epoch: 258, Loss: 62.95693459568253, Learning Rate: 0.0005
Epoch: 259, Loss: 59.79194751417781, Learning Rate: 0.0005
Mean: 0.2593824634398606, Median: 0.23296172048481448, Num: 111
Epoch: 260, Loss: 56.98755158573748, Learning Rate: 0.0005
Epoch: 261, Loss: 63.332399058054726, Learning Rate: 0.0005
Mean: 0.2629139218865776, Median: 0.24128532051883714, Num: 111
Epoch: 262, Loss: 64.44286472826118, Learning Rate: 0.0005
Epoch: 263, Loss: 55.11098192973309, Learning Rate: 0.0005
Mean: 0.2662142987296164, Median: 0.25411970544953416, Num: 111
Epoch: 264, Loss: 63.30154423541333, Learning Rate: 0.0005
Epoch: 265, Loss: 57.67377380003412, Learning Rate: 0.0005
Mean: 0.25847135438643126, Median: 0.2338892631725863, Num: 111
Epoch: 266, Loss: 56.244195455528164, Learning Rate: 0.0005
Epoch: 267, Loss: 65.14575983530068, Learning Rate: 0.0005
Mean: 0.26627913512918144, Median: 0.23903637803337707, Num: 111
Epoch: 268, Loss: 64.37981680215123, Learning Rate: 0.0005
Epoch: 269, Loss: 57.86064952827362, Learning Rate: 0.0005
Mean: 0.26273904911419904, Median: 0.2411267383756042, Num: 111
Epoch: 270, Loss: 57.173443277198146, Learning Rate: 0.0005
Epoch: 271, Loss: 58.19813928259424, Learning Rate: 0.0005
Mean: 0.2672024585088817, Median: 0.26187322193582896, Num: 111
Epoch: 272, Loss: 66.35750503999641, Learning Rate: 0.0005
Epoch: 273, Loss: 60.37806235164045, Learning Rate: 0.0005
Mean: 0.2651335118558734, Median: 0.24201804683698327, Num: 111
Epoch: 274, Loss: 64.04401799856899, Learning Rate: 0.0005
Epoch: 275, Loss: 57.93260077970574, Learning Rate: 0.0005
Mean: 0.26652750140092796, Median: 0.25398119699739297, Num: 111
Epoch: 276, Loss: 58.93492152317461, Learning Rate: 0.0005
Epoch: 277, Loss: 62.29612527410668, Learning Rate: 0.0005
Mean: 0.26718781675977415, Median: 0.2565228869318317, Num: 111
Epoch: 278, Loss: 64.31308153451207, Learning Rate: 0.0005
Epoch: 279, Loss: 56.842839338693274, Learning Rate: 0.0005
Mean: 0.26677782464511063, Median: 0.2348624143125805, Num: 111
Epoch: 280, Loss: 64.44743774597903, Learning Rate: 0.0005
Epoch: 281, Loss: 68.85008086928401, Learning Rate: 0.0005
Mean: 0.26459381872150706, Median: 0.26342481673908197, Num: 111
Epoch: 282, Loss: 61.508144355682006, Learning Rate: 0.0005
Epoch: 283, Loss: 54.88204859538251, Learning Rate: 0.0005
Mean: 0.2652964349382268, Median: 0.24524966943505552, Num: 111
Epoch: 284, Loss: 61.93785526091794, Learning Rate: 0.0005
Epoch: 285, Loss: 62.850761321653806, Learning Rate: 0.0005
Mean: 0.26716887916865606, Median: 0.26806956673278703, Num: 111
Epoch: 286, Loss: 53.59606817544225, Learning Rate: 0.0005
Epoch: 287, Loss: 67.27979836980981, Learning Rate: 0.0005
Mean: 0.2692312060471414, Median: 0.24840697268029005, Num: 111
Epoch: 288, Loss: 64.20678728172578, Learning Rate: 0.0005
Epoch: 289, Loss: 60.96595441289695, Learning Rate: 0.0005
Mean: 0.265483494175914, Median: 0.24565465144768484, Num: 111
Epoch: 290, Loss: 57.682080693991786, Learning Rate: 0.0005
Epoch: 291, Loss: 60.24758564133242, Learning Rate: 0.0005
Mean: 0.27069522612934904, Median: 0.25821668176045554, Num: 111
Epoch: 292, Loss: 57.26396989247885, Learning Rate: 0.0005
Epoch: 293, Loss: 54.715333467506504, Learning Rate: 0.0005
Mean: 0.26914770409264027, Median: 0.25736405248897676, Num: 111
Epoch: 294, Loss: 58.04634214883827, Learning Rate: 0.0005
Epoch: 295, Loss: 61.74439452067915, Learning Rate: 0.0005
Mean: 0.26177327018683966, Median: 0.24360255493278316, Num: 111
Epoch: 296, Loss: 63.835056994334764, Learning Rate: 0.0005
Epoch: 297, Loss: 57.583558800708815, Learning Rate: 0.0005
Mean: 0.26666597868116654, Median: 0.2542312761109813, Num: 111
Epoch: 298, Loss: 60.81471293805593, Learning Rate: 0.0005
Epoch: 299, Loss: 60.65125242486057, Learning Rate: 0.0005
Mean: 0.2659512129870826, Median: 0.24372319464486292, Num: 111
Epoch: 300, Loss: 57.10867304974292, Learning Rate: 0.0005
Epoch: 301, Loss: 63.33943780646267, Learning Rate: 0.0005
Mean: 0.27117614303386955, Median: 0.26632036715393753, Num: 111
Epoch: 302, Loss: 57.32721809019525, Learning Rate: 0.0005
Epoch: 303, Loss: 59.32238946478051, Learning Rate: 0.0005
Mean: 0.26834294989088664, Median: 0.2628252387977498, Num: 111
Epoch: 304, Loss: 62.60531497863402, Learning Rate: 0.0005
Epoch: 305, Loss: 59.73251404819718, Learning Rate: 0.0005
Mean: 0.26217606270400934, Median: 0.2520849132987006, Num: 111
Epoch: 306, Loss: 58.003497721200965, Learning Rate: 0.0005
Epoch: 307, Loss: 59.730139123388085, Learning Rate: 0.0005
Mean: 0.2683367455922293, Median: 0.2618812934005108, Num: 111
Epoch: 308, Loss: 58.516613489173984, Learning Rate: 0.0005
Epoch: 309, Loss: 55.18957680391978, Learning Rate: 0.0005
Mean: 0.26353371857440017, Median: 0.24232228624510974, Num: 111
Epoch: 310, Loss: 66.30392832928393, Learning Rate: 0.0005
Epoch: 311, Loss: 58.14420200830482, Learning Rate: 0.0005
Mean: 0.2699299032805086, Median: 0.2621676785179067, Num: 111
Epoch: 312, Loss: 58.50061307470482, Learning Rate: 0.0005
Epoch: 313, Loss: 58.13817588392511, Learning Rate: 0.0005
Mean: 0.27024165003036005, Median: 0.2591301680543447, Num: 111
Epoch: 314, Loss: 62.337703773774294, Learning Rate: 0.0005
Epoch: 315, Loss: 53.62725230297411, Learning Rate: 0.0005
Mean: 0.2635183762784718, Median: 0.2542578483144377, Num: 111
Epoch: 316, Loss: 55.48405164695648, Learning Rate: 0.0005
Epoch: 317, Loss: 61.8756736732391, Learning Rate: 0.0005
Mean: 0.2677299008062113, Median: 0.2429783204895238, Num: 111
Epoch: 318, Loss: 59.756587752376696, Learning Rate: 0.0005
Epoch: 319, Loss: 58.572149575474754, Learning Rate: 0.0005
Mean: 0.26873471586185144, Median: 0.251803982299235, Num: 111
Epoch: 320, Loss: 59.824085660727626, Learning Rate: 0.0005
Epoch: 321, Loss: 63.16364249263901, Learning Rate: 0.0005
Mean: 0.26423473894666116, Median: 0.25213136500027716, Num: 111
Epoch: 322, Loss: 60.6173557028713, Learning Rate: 0.0005
Epoch: 323, Loss: 60.52395395485752, Learning Rate: 0.0005
Mean: 0.2637052144280456, Median: 0.23116867060850282, Num: 111
Epoch: 324, Loss: 51.13885830109378, Learning Rate: 0.0005
Epoch: 325, Loss: 59.41923781475389, Learning Rate: 0.0005
Mean: 0.26416371590683885, Median: 0.245846779225643, Num: 111
Epoch: 326, Loss: 55.85921974641731, Learning Rate: 0.0005
Epoch: 327, Loss: 56.37325875155897, Learning Rate: 0.0005
Mean: 0.2705155904280138, Median: 0.23347347181770106, Num: 111
Epoch: 328, Loss: 64.37404673932546, Learning Rate: 0.0005
Epoch: 329, Loss: 58.95270924395825, Learning Rate: 0.0005
Mean: 0.264301456821954, Median: 0.24807637360103915, Num: 111
Epoch: 330, Loss: 56.375395820801515, Learning Rate: 0.0005
Epoch: 331, Loss: 63.243954945759604, Learning Rate: 0.0005
Mean: 0.2679331668748111, Median: 0.2814974029767965, Num: 111
Epoch: 332, Loss: 59.56734252837767, Learning Rate: 0.0005
Epoch: 333, Loss: 58.85165726994894, Learning Rate: 0.0005
Mean: 0.26186432032342594, Median: 0.24667673243738736, Num: 111
Epoch: 334, Loss: 62.029838855008045, Learning Rate: 0.0005
Epoch: 335, Loss: 56.597847007843384, Learning Rate: 0.0005
Mean: 0.2705515402492302, Median: 0.24250077075436396, Num: 111
Epoch: 336, Loss: 52.45982363137854, Learning Rate: 0.0005
Epoch: 337, Loss: 59.2342683952975, Learning Rate: 0.0005
Mean: 0.2694360731370376, Median: 0.264100023324757, Num: 111
Epoch: 338, Loss: 60.91875124552164, Learning Rate: 0.0005
Epoch: 339, Loss: 52.45879711013242, Learning Rate: 0.0005
Mean: 0.2658674349105325, Median: 0.24352138981340174, Num: 111
Epoch: 340, Loss: 62.18651703754103, Learning Rate: 0.0005
Epoch: 341, Loss: 56.61224158413439, Learning Rate: 0.0005
Mean: 0.26526479506780676, Median: 0.25302769159587324, Num: 111
Epoch: 342, Loss: 56.04987391506333, Learning Rate: 0.0005
Epoch: 343, Loss: 57.64258287039148, Learning Rate: 0.0005
Mean: 0.2716958060419446, Median: 0.2650653838233583, Num: 111
Epoch: 344, Loss: 55.216024904366, Learning Rate: 0.0005
Epoch: 345, Loss: 58.5357279375375, Learning Rate: 0.0005
Mean: 0.2693243672741936, Median: 0.24136545139700713, Num: 111
Epoch: 346, Loss: 62.84906499931611, Learning Rate: 0.0005
Epoch: 347, Loss: 55.523953179279005, Learning Rate: 0.0005
Mean: 0.26696747108874813, Median: 0.25324443700182414, Num: 111
Epoch: 348, Loss: 61.71632125578731, Learning Rate: 0.0005
Epoch: 349, Loss: 64.07878262163645, Learning Rate: 0.0005
Mean: 0.2664942210729183, Median: 0.2382551062834013, Num: 111
Epoch: 350, Loss: 59.07078455729657, Learning Rate: 0.0005
Epoch: 351, Loss: 54.56659052170903, Learning Rate: 0.0005
Mean: 0.2669316257117549, Median: 0.2606903308907747, Num: 111
Epoch: 352, Loss: 57.8346079171422, Learning Rate: 0.0005
Epoch: 353, Loss: 56.70834673456399, Learning Rate: 0.0005
Mean: 0.27251504517564273, Median: 0.285757505323743, Num: 111
Epoch: 354, Loss: 53.98097679413945, Learning Rate: 0.0005
Epoch: 355, Loss: 59.1381881897708, Learning Rate: 0.0005
Mean: 0.2708220105018684, Median: 0.26381392829182887, Num: 111
Epoch: 356, Loss: 57.97362570015781, Learning Rate: 0.0005
Epoch: 357, Loss: 63.21692321961184, Learning Rate: 0.0005
Mean: 0.26806485918547734, Median: 0.25606971404650053, Num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
Epoch: 358, Loss: 58.584073072456455, Learning Rate: 0.0005
Epoch: 359, Loss: 55.61347968319812, Learning Rate: 0.0005
Mean: 0.2615279355600037, Median: 0.24903321254132574, Num: 111
Epoch: 360, Loss: 54.1673595704228, Learning Rate: 0.0005
Epoch: 361, Loss: 57.736120160803736, Learning Rate: 0.0005
Mean: 0.2630949222525855, Median: 0.25287492368778025, Num: 111
Epoch: 362, Loss: 58.354908403143824, Learning Rate: 0.0005
Epoch: 363, Loss: 58.673022189772276, Learning Rate: 0.0005
Mean: 0.26658984618135473, Median: 0.24788413887675423, Num: 111
Epoch: 364, Loss: 64.06321710563567, Learning Rate: 0.0005
Epoch: 365, Loss: 67.2105793091188, Learning Rate: 0.0005
Mean: 0.27203836979051277, Median: 0.26319240642174685, Num: 111
Epoch: 366, Loss: 57.08067609028644, Learning Rate: 0.0005
Epoch: 367, Loss: 59.646432141223585, Learning Rate: 0.0005
Mean: 0.26913497686371274, Median: 0.2421877697504616, Num: 111
Epoch: 368, Loss: 61.05823547294341, Learning Rate: 0.0005
Epoch: 369, Loss: 57.04763214846692, Learning Rate: 0.0005
Mean: 0.26996578263187926, Median: 0.2613526527205658, Num: 111
Epoch: 370, Loss: 54.802383687122756, Learning Rate: 0.0005
Epoch: 371, Loss: 61.365263651652505, Learning Rate: 0.0005
Mean: 0.27465930881220907, Median: 0.2695451742974916, Num: 111
Epoch: 372, Loss: 57.47757321093456, Learning Rate: 0.0005
Epoch: 373, Loss: 58.934498005602734, Learning Rate: 0.0005
Mean: 0.27385551496417754, Median: 0.2828522078464817, Num: 111
Epoch: 374, Loss: 48.041978962450145, Learning Rate: 0.0005
Epoch: 375, Loss: 60.74471990746188, Learning Rate: 0.0005
Mean: 0.2685359675472968, Median: 0.2716516960042668, Num: 111
Epoch: 376, Loss: 53.13189492742699, Learning Rate: 0.0005
Epoch: 377, Loss: 62.46628434974027, Learning Rate: 0.0005
Mean: 0.26784864933963215, Median: 0.241220700581201, Num: 111
Epoch: 378, Loss: 63.48286419604198, Learning Rate: 0.0005
Epoch: 379, Loss: 64.20163046595562, Learning Rate: 0.0005
Mean: 0.2673147959375834, Median: 0.24791733350859543, Num: 111
Epoch: 380, Loss: 55.11236634311906, Learning Rate: 0.0005
Epoch: 381, Loss: 56.839137261172375, Learning Rate: 0.0005
Mean: 0.2615943768812751, Median: 0.22309379048694275, Num: 111
Epoch: 382, Loss: 60.651014040751626, Learning Rate: 0.0005
Epoch: 383, Loss: 62.834290837667076, Learning Rate: 0.0005
Mean: 0.2696406026367548, Median: 0.25152940924534395, Num: 111
Epoch: 384, Loss: 58.268498684986525, Learning Rate: 0.0005
Epoch: 385, Loss: 61.5982666015625, Learning Rate: 0.0005
Mean: 0.27283065046367616, Median: 0.26337959691280666, Num: 111
Epoch: 386, Loss: 65.80570515092597, Learning Rate: 0.0005
Epoch: 387, Loss: 61.0776527818427, Learning Rate: 0.0005
Mean: 0.26623262715487384, Median: 0.23375472571517542, Num: 111
Epoch: 388, Loss: 67.60070957045957, Learning Rate: 0.0005
Epoch: 389, Loss: 57.488192167626806, Learning Rate: 0.0005
Mean: 0.2657341640996031, Median: 0.23324539866576746, Num: 111
Epoch: 390, Loss: 58.296310137553384, Learning Rate: 0.0005
Epoch: 391, Loss: 62.54998305906732, Learning Rate: 0.0005
Mean: 0.271540390677796, Median: 0.2688102168623268, Num: 111
Epoch: 392, Loss: 59.15117846339582, Learning Rate: 0.0005
Epoch: 393, Loss: 54.20053553293987, Learning Rate: 0.0005
Mean: 0.2675304366723313, Median: 0.26753892216957487, Num: 111
Epoch: 394, Loss: 60.95786460049181, Learning Rate: 0.0005
Epoch: 395, Loss: 54.889660599720045, Learning Rate: 0.0005
Mean: 0.2701949470978831, Median: 0.26007060945852895, Num: 111
Epoch: 396, Loss: 67.63049288830125, Learning Rate: 0.0005
Epoch: 397, Loss: 56.47444139043969, Learning Rate: 0.0005
Mean: 0.2727044300618654, Median: 0.2731776088819497, Num: 111
Epoch: 398, Loss: 50.95378138071083, Learning Rate: 0.0005
Epoch: 399, Loss: 56.34917320113584, Learning Rate: 0.0005
Mean: 0.2722949768302793, Median: 0.2723625780476654, Num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
10
Epoch: 0, Loss: 239.21185229198042, Learning Rate: 0.0005
Epoch: 1, Loss: 192.28765005088715, Learning Rate: 0.0005
Mean: 0.06222965277034433, Median: 0.01568279908015279, Num: 111
Epoch: 2, Loss: 173.5369043005518, Learning Rate: 0.0005
Epoch: 3, Loss: 171.62970375152955, Learning Rate: 0.0005
Mean: 0.08456321914995546, Median: 0.032580225219917366, Num: 111
Epoch: 4, Loss: 155.66131826193936, Learning Rate: 0.0005
Epoch: 5, Loss: 156.55442938747177, Learning Rate: 0.0005
Mean: 0.11300171174713754, Median: 0.060104842037297164, Num: 111
Epoch: 6, Loss: 152.6429535739393, Learning Rate: 0.0005
Epoch: 7, Loss: 148.5977467456496, Learning Rate: 0.0005
Mean: 0.1386356456090492, Median: 0.09622852423232128, Num: 111
Epoch: 8, Loss: 131.5107324209558, Learning Rate: 0.0005
Epoch: 9, Loss: 139.65432086622857, Learning Rate: 0.0005
Mean: 0.15260526840903044, Median: 0.09930372414063011, Num: 111
Epoch: 10, Loss: 132.72411502700254, Learning Rate: 0.0005
Epoch: 11, Loss: 135.08338974182865, Learning Rate: 0.0005
Mean: 0.1701372808826732, Median: 0.13284792750940033, Num: 111
Epoch: 12, Loss: 132.67982135910586, Learning Rate: 0.0005
Epoch: 13, Loss: 126.92951248352786, Learning Rate: 0.0005
Mean: 0.16231893491295038, Median: 0.11931389378093413, Num: 111
Epoch: 14, Loss: 129.89868717883007, Learning Rate: 0.0005
Epoch: 15, Loss: 121.63328193756472, Learning Rate: 0.0005
Mean: 0.17302034447486542, Median: 0.13776421052549223, Num: 111
Epoch: 16, Loss: 123.34065747548298, Learning Rate: 0.0005
Epoch: 17, Loss: 119.93898111366364, Learning Rate: 0.0005
Mean: 0.18474073941623995, Median: 0.15055520162098837, Num: 111
Epoch: 18, Loss: 124.73811009418533, Learning Rate: 0.0005
Epoch: 19, Loss: 110.37880522946277, Learning Rate: 0.0005
Mean: 0.18579530013108136, Median: 0.15823527218234007, Num: 111
Epoch: 20, Loss: 112.29352839596301, Learning Rate: 0.0005
Epoch: 21, Loss: 107.70431343905896, Learning Rate: 0.0005
Mean: 0.16998250640431126, Median: 0.13177438593144247, Num: 111
Epoch: 22, Loss: 113.93961398572807, Learning Rate: 0.0005
Epoch: 23, Loss: 109.09601583825537, Learning Rate: 0.0005
Mean: 0.19059794168272157, Median: 0.160028301552655, Num: 111
Epoch: 24, Loss: 108.48816113299634, Learning Rate: 0.0005
Epoch: 25, Loss: 105.43643245926823, Learning Rate: 0.0005
Mean: 0.19235011621390974, Median: 0.16103112950521456, Num: 111
Epoch: 26, Loss: 103.2429537026279, Learning Rate: 0.0005
Epoch: 27, Loss: 102.69003144229751, Learning Rate: 0.0005
Mean: 0.1894590125499077, Median: 0.16456959044873692, Num: 111
Epoch: 28, Loss: 104.12575154132153, Learning Rate: 0.0005
Epoch: 29, Loss: 108.82604076201658, Learning Rate: 0.0005
Mean: 0.20067469080004063, Median: 0.17509918827447488, Num: 111
Epoch: 30, Loss: 99.77705394790833, Learning Rate: 0.0005
Epoch: 31, Loss: 100.68643652674663, Learning Rate: 0.0005
Mean: 0.19586433757629196, Median: 0.16893617407547246, Num: 111
Epoch: 32, Loss: 97.0709171065365, Learning Rate: 0.0005
Epoch: 33, Loss: 98.43453432565713, Learning Rate: 0.0005
Mean: 0.2055842865051401, Median: 0.175713274303391, Num: 111
Epoch: 34, Loss: 95.63963055897908, Learning Rate: 0.0005
Epoch: 35, Loss: 91.71809368822949, Learning Rate: 0.0005
Mean: 0.20597191597136552, Median: 0.18901806033122392, Num: 111
Epoch: 36, Loss: 103.36621845199402, Learning Rate: 0.0005
Epoch: 37, Loss: 93.77232517104551, Learning Rate: 0.0005
Mean: 0.20938354986721977, Median: 0.19071493496109182, Num: 111
Epoch: 38, Loss: 89.77476611769343, Learning Rate: 0.0005
Epoch: 39, Loss: 92.11336186420486, Learning Rate: 0.0005
Mean: 0.21282670729074193, Median: 0.19730769713031038, Num: 111
Epoch: 40, Loss: 100.40089485444219, Learning Rate: 0.0005
Epoch: 41, Loss: 97.67490405346973, Learning Rate: 0.0005
Mean: 0.21311958861198715, Median: 0.1855279690789424, Num: 111
Epoch: 42, Loss: 90.35843998552805, Learning Rate: 0.0005
Epoch: 43, Loss: 88.59447837737669, Learning Rate: 0.0005
Mean: 0.21731212070117556, Median: 0.19751708477070937, Num: 111
Epoch: 44, Loss: 92.0554589420916, Learning Rate: 0.0005
Epoch: 45, Loss: 85.67964613581279, Learning Rate: 0.0005
Mean: 0.21357552693583312, Median: 0.19175573653838232, Num: 111
Epoch: 46, Loss: 88.18991201469697, Learning Rate: 0.0005
Epoch: 47, Loss: 87.12999040534697, Learning Rate: 0.0005
Mean: 0.21977722183734816, Median: 0.20490155308124897, Num: 111
Epoch: 48, Loss: 90.36898038473474, Learning Rate: 0.0005
Epoch: 49, Loss: 90.09652144650379, Learning Rate: 0.0005
Mean: 0.22428821757143472, Median: 0.19907941516932393, Num: 111
Epoch: 50, Loss: 90.53772287483675, Learning Rate: 0.0005
Epoch: 51, Loss: 87.97814088844392, Learning Rate: 0.0005
Mean: 0.21891800237707507, Median: 0.20571487603098731, Num: 111
Epoch: 52, Loss: 91.24178169434329, Learning Rate: 0.0005
Epoch: 53, Loss: 81.47004518164209, Learning Rate: 0.0005
Mean: 0.21955374095347002, Median: 0.1963691661574588, Num: 111
Epoch: 54, Loss: 84.62473131662392, Learning Rate: 0.0005
Epoch: 55, Loss: 88.80620754195984, Learning Rate: 0.0005
Mean: 0.22547235243233302, Median: 0.21316037307865301, Num: 111
Epoch: 56, Loss: 81.91141836327243, Learning Rate: 0.0005
Epoch: 57, Loss: 85.48150223421763, Learning Rate: 0.0005
Mean: 0.22945648212512215, Median: 0.2086543197162654, Num: 111
Epoch: 58, Loss: 85.52807982571154, Learning Rate: 0.0005
Epoch: 59, Loss: 89.5911937506802, Learning Rate: 0.0005
Mean: 0.22066548061316255, Median: 0.21263974721204146, Num: 111
Epoch: 60, Loss: 84.00140888719673, Learning Rate: 0.0005
Epoch: 61, Loss: 82.86325362791499, Learning Rate: 0.0005
Mean: 0.2272378745977852, Median: 0.2096587231810128, Num: 111
Epoch: 62, Loss: 87.01055990931499, Learning Rate: 0.0005
Epoch: 63, Loss: 74.2221805848271, Learning Rate: 0.0005
Mean: 0.23363512807722756, Median: 0.20868835621380777, Num: 111
Epoch: 64, Loss: 82.42372742618423, Learning Rate: 0.0005
Epoch: 65, Loss: 79.0635696663914, Learning Rate: 0.0005
Mean: 0.23081928326355253, Median: 0.22260435596310038, Num: 111
Epoch: 66, Loss: 75.31755494496909, Learning Rate: 0.0005
Epoch: 67, Loss: 78.9203757090741, Learning Rate: 0.0005
Mean: 0.23469830999803676, Median: 0.21649058447028394, Num: 111
Epoch: 68, Loss: 75.16261592543269, Learning Rate: 0.0005
Epoch: 69, Loss: 85.85069173789886, Learning Rate: 0.0005
Mean: 0.24035635679340525, Median: 0.24284096262005395, Num: 111
Epoch: 70, Loss: 78.71751854218633, Learning Rate: 0.0005
Epoch: 71, Loss: 81.27201477878064, Learning Rate: 0.0005
Mean: 0.23832622177316684, Median: 0.21397759009838874, Num: 111
Epoch: 72, Loss: 79.79147734124976, Learning Rate: 0.0005
Epoch: 73, Loss: 77.60594453007342, Learning Rate: 0.0005
Mean: 0.23678781341995245, Median: 0.23348861571284305, Num: 111
Epoch: 74, Loss: 83.98889982843974, Learning Rate: 0.0005
Epoch: 75, Loss: 82.22343987154673, Learning Rate: 0.0005
Mean: 0.2382755157294335, Median: 0.22261715553777578, Num: 111
Epoch: 76, Loss: 77.943681510098, Learning Rate: 0.0005
Epoch: 77, Loss: 81.25596209606492, Learning Rate: 0.0005
Mean: 0.2323853423260411, Median: 0.22073111362520156, Num: 111
Epoch: 78, Loss: 77.4354849206396, Learning Rate: 0.0005
Epoch: 79, Loss: 75.29839134216309, Learning Rate: 0.0005
Mean: 0.2381220812600584, Median: 0.23608258715745983, Num: 111
Epoch: 80, Loss: 72.8943873485887, Learning Rate: 0.0005
Epoch: 81, Loss: 72.21200543139355, Learning Rate: 0.0005
Mean: 0.24310913837344328, Median: 0.23245089807806227, Num: 111
Epoch: 82, Loss: 74.00562750575054, Learning Rate: 0.0005
Epoch: 83, Loss: 74.17476194450654, Learning Rate: 0.0005
Mean: 0.23726986194309446, Median: 0.2056758670347572, Num: 111
Epoch: 84, Loss: 71.33208711463284, Learning Rate: 0.0005
Epoch: 85, Loss: 67.16674613952637, Learning Rate: 0.0005
Mean: 0.23508771773700385, Median: 0.2120871103895346, Num: 111
Epoch: 86, Loss: 78.02256075158176, Learning Rate: 0.0005
Epoch: 87, Loss: 91.70199376990996, Learning Rate: 0.0005
Mean: 0.24374557883869255, Median: 0.2337794105790451, Num: 111
Epoch: 88, Loss: 68.40012227483543, Learning Rate: 0.0005
Epoch: 89, Loss: 74.68248600557627, Learning Rate: 0.0005
Mean: 0.24086484774236744, Median: 0.2324203692200809, Num: 111
Epoch: 90, Loss: 68.58507310338767, Learning Rate: 0.0005
Epoch: 91, Loss: 62.32991188692759, Learning Rate: 0.0005
Mean: 0.24333835412419783, Median: 0.22516494198843262, Num: 111
Epoch: 92, Loss: 68.21482568763825, Learning Rate: 0.0005
Epoch: 93, Loss: 76.03934952149908, Learning Rate: 0.0005
Mean: 0.24651801713915247, Median: 0.21683950206740232, Num: 111
Epoch: 94, Loss: 67.37659757683076, Learning Rate: 0.0005
Epoch: 95, Loss: 64.59680322853916, Learning Rate: 0.0005
Mean: 0.24095683791901534, Median: 0.2085281042172786, Num: 111
Epoch: 96, Loss: 70.62266337153423, Learning Rate: 0.0005
Epoch: 97, Loss: 74.63086769379765, Learning Rate: 0.0005
Mean: 0.23950128111924354, Median: 0.23228599576246467, Num: 111
Epoch: 98, Loss: 71.53713275725583, Learning Rate: 0.0005
Epoch: 99, Loss: 72.8452750286424, Learning Rate: 0.0005
Mean: 0.2506388988595155, Median: 0.23766389411421843, Num: 111
Epoch: 100, Loss: 74.26406833924443, Learning Rate: 0.0005
Epoch: 101, Loss: 63.426784216639504, Learning Rate: 0.0005
Mean: 0.2414244219073697, Median: 0.23458105554998185, Num: 111
Epoch: 102, Loss: 76.21077057252447, Learning Rate: 0.0005
Epoch: 103, Loss: 72.0206295266209, Learning Rate: 0.0005
Mean: 0.25441548080700827, Median: 0.2493250440676894, Num: 111
Epoch: 104, Loss: 68.73075627706137, Learning Rate: 0.0005
Epoch: 105, Loss: 64.31066292452525, Learning Rate: 0.0005
Mean: 0.24736635407906035, Median: 0.22620066846903478, Num: 111
Epoch: 106, Loss: 71.77405777896743, Learning Rate: 0.0005
Epoch: 107, Loss: 66.64665187697813, Learning Rate: 0.0005
Mean: 0.23904393769004945, Median: 0.206178779076118, Num: 111
Epoch: 108, Loss: 69.23074315542198, Learning Rate: 0.0005
Epoch: 109, Loss: 70.20971174125212, Learning Rate: 0.0005
Mean: 0.24056463194034602, Median: 0.2078114933282461, Num: 111
Epoch: 110, Loss: 59.88416232832943, Learning Rate: 0.0005
Epoch: 111, Loss: 64.7996692082968, Learning Rate: 0.0005
Mean: 0.2478686878692635, Median: 0.21241508634528186, Num: 111
Epoch: 112, Loss: 65.57641932475998, Learning Rate: 0.0005
Epoch: 113, Loss: 67.93636677064092, Learning Rate: 0.0005
Mean: 0.2420205182975036, Median: 0.21918884098512578, Num: 111
Epoch: 114, Loss: 65.26410413075642, Learning Rate: 0.0005
Epoch: 115, Loss: 73.73209844152612, Learning Rate: 0.0005
Mean: 0.25714858820491737, Median: 0.2627552264401476, Num: 111
Epoch: 116, Loss: 65.50086818832949, Learning Rate: 0.0005
Epoch: 117, Loss: 64.98367164795657, Learning Rate: 0.0005
Mean: 0.24585664315105704, Median: 0.22512198578133327, Num: 111
Epoch: 118, Loss: 73.66272733297693, Learning Rate: 0.0005
Epoch: 119, Loss: 73.94082586449314, Learning Rate: 0.0005
Mean: 0.2549480407895317, Median: 0.2610925397418985, Num: 111
Epoch: 120, Loss: 61.327664363815124, Learning Rate: 0.0005
Epoch: 121, Loss: 78.78677245220506, Learning Rate: 0.0005
Mean: 0.24538627137732136, Median: 0.22310694617345486, Num: 111
Epoch: 122, Loss: 71.95498326313064, Learning Rate: 0.0005
Epoch: 123, Loss: 63.617169437638246, Learning Rate: 0.0005
Mean: 0.24485564237277485, Median: 0.23632606448217508, Num: 111
Epoch: 124, Loss: 71.36149038751441, Learning Rate: 0.0005
Epoch: 125, Loss: 61.744022357894714, Learning Rate: 0.0005
Mean: 0.2506810733215026, Median: 0.2517482114310916, Num: 111
Epoch: 126, Loss: 68.16096687316895, Learning Rate: 0.0005
Epoch: 127, Loss: 66.49436244045395, Learning Rate: 0.0005
Mean: 0.252439377039595, Median: 0.24208073810636813, Num: 111
Epoch: 128, Loss: 67.21408861229219, Learning Rate: 0.0005
Epoch: 129, Loss: 70.00110391823642, Learning Rate: 0.0005
Mean: 0.2538933391209591, Median: 0.25304472903628167, Num: 111
Epoch: 130, Loss: 68.60604227594582, Learning Rate: 0.0005
Epoch: 131, Loss: 60.23377859736063, Learning Rate: 0.0005
Mean: 0.25404641680166307, Median: 0.25137122003655726, Num: 111
Epoch: 132, Loss: 69.24267058774649, Learning Rate: 0.0005
Epoch: 133, Loss: 60.129303966660096, Learning Rate: 0.0005
Mean: 0.25611213262830596, Median: 0.25984769845748923, Num: 111
Epoch: 134, Loss: 63.80016368268484, Learning Rate: 0.0005
Epoch: 135, Loss: 64.26698197514177, Learning Rate: 0.0005
Mean: 0.24594445928524583, Median: 0.21889378437551682, Num: 111
Epoch: 136, Loss: 63.59875155069742, Learning Rate: 0.0005
Epoch: 137, Loss: 75.73346417208752, Learning Rate: 0.0005
Mean: 0.2535831777620513, Median: 0.25506798576609, Num: 111
Epoch: 138, Loss: 62.67135413296251, Learning Rate: 0.0005
Epoch: 139, Loss: 65.21012306213379, Learning Rate: 0.0005
Mean: 0.2545225587953328, Median: 0.26533993500277403, Num: 111
Epoch: 140, Loss: 63.16446343387466, Learning Rate: 0.0005
Epoch: 141, Loss: 63.897645973297486, Learning Rate: 0.0005
Mean: 0.2555898307404594, Median: 0.2369552672818765, Num: 111
Epoch: 142, Loss: 68.72778499557312, Learning Rate: 0.0005
Epoch: 143, Loss: 61.81738349041307, Learning Rate: 0.0005
Mean: 0.25193549701125395, Median: 0.2278965676154817, Num: 111
Epoch: 144, Loss: 68.44320497168115, Learning Rate: 0.0005
Epoch: 145, Loss: 63.883302941379775, Learning Rate: 0.0005
Mean: 0.25697059932418437, Median: 0.2571223558302637, Num: 111
Epoch: 146, Loss: 69.96685478486211, Learning Rate: 0.0005
Epoch: 147, Loss: 67.90236675308411, Learning Rate: 0.0005
Mean: 0.2473666424078997, Median: 0.2165577925453208, Num: 111
Epoch: 148, Loss: 70.53910418590867, Learning Rate: 0.0005
Epoch: 149, Loss: 64.3328157447907, Learning Rate: 0.0005
Mean: 0.2511552375925532, Median: 0.25766045690792605, Num: 111
Epoch: 150, Loss: 59.44650216849453, Learning Rate: 0.0005
Epoch: 151, Loss: 70.81138657949057, Learning Rate: 0.0005
Mean: 0.26080906446426116, Median: 0.24690892347939244, Num: 111
Epoch: 152, Loss: 68.96070873306458, Learning Rate: 0.0005
Epoch: 153, Loss: 61.07997538095497, Learning Rate: 0.0005
Mean: 0.2564842565269505, Median: 0.2438704184882021, Num: 111
Epoch: 154, Loss: 62.21376150200166, Learning Rate: 0.0005
Epoch: 155, Loss: 60.32589126494994, Learning Rate: 0.0005
Mean: 0.2529041068639039, Median: 0.2203737821745609, Num: 111
Epoch: 156, Loss: 66.09527429327908, Learning Rate: 0.0005
Epoch: 157, Loss: 76.73408263562673, Learning Rate: 0.0005
Mean: 0.25543806796987073, Median: 0.2611938408718092, Num: 111
Epoch: 158, Loss: 62.528254221720864, Learning Rate: 0.0005
Epoch: 159, Loss: 61.6384650081037, Learning Rate: 0.0005
Mean: 0.2586935439452947, Median: 0.2614314365585947, Num: 111
Epoch: 160, Loss: 65.79984490268201, Learning Rate: 0.0005
Epoch: 161, Loss: 63.807894385004616, Learning Rate: 0.0005
Mean: 0.2613665377906587, Median: 0.25878151855802356, Num: 111
Epoch: 162, Loss: 63.762327676796055, Learning Rate: 0.0005
Epoch: 163, Loss: 62.055410373641784, Learning Rate: 0.0005
Mean: 0.254599744429177, Median: 0.24381323290330226, Num: 111
Epoch: 164, Loss: 67.8599509273667, Learning Rate: 0.0005
Epoch: 165, Loss: 62.71082219732813, Learning Rate: 0.0005
Mean: 0.25993037432357824, Median: 0.25091602553485215, Num: 111
Epoch: 166, Loss: 62.74950346889266, Learning Rate: 0.0005
Epoch: 167, Loss: 64.41049815947751, Learning Rate: 0.0005
Mean: 0.26175559767068374, Median: 0.25104876145419125, Num: 111
Epoch: 168, Loss: 58.96578892167792, Learning Rate: 0.0005
Epoch: 169, Loss: 61.61216403777341, Learning Rate: 0.0005
Mean: 0.2663500095352074, Median: 0.2658574387615136, Num: 111
Epoch: 170, Loss: 64.3844813611134, Learning Rate: 0.0005
Epoch: 171, Loss: 63.61969009652195, Learning Rate: 0.0005
Mean: 0.26302270495346375, Median: 0.2571256721377185, Num: 111
Epoch: 172, Loss: 62.202387717833005, Learning Rate: 0.0005
Epoch: 173, Loss: 65.3238425082471, Learning Rate: 0.0005
Mean: 0.257041007747435, Median: 0.2309989804858548, Num: 111
Epoch: 174, Loss: 58.53738532008895, Learning Rate: 0.0005
Epoch: 175, Loss: 65.38015100180385, Learning Rate: 0.0005
Mean: 0.2561932586852837, Median: 0.2450754791402977, Num: 111
Epoch: 176, Loss: 64.38503214824631, Learning Rate: 0.0005
Epoch: 177, Loss: 60.79438179659556, Learning Rate: 0.0005
Mean: 0.2603784419843923, Median: 0.248674085126055, Num: 111
Epoch: 178, Loss: 61.6155083438, Learning Rate: 0.0005
Epoch: 179, Loss: 67.68779444407268, Learning Rate: 0.0005
Mean: 0.26260935912456423, Median: 0.259888232570524, Num: 111
Epoch: 180, Loss: 62.79511437933129, Learning Rate: 0.0005
Epoch: 181, Loss: 70.46356194277844, Learning Rate: 0.0005
Mean: 0.26388910034680063, Median: 0.2601748295677202, Num: 111
Epoch: 182, Loss: 68.25887151511319, Learning Rate: 0.0005
Epoch: 183, Loss: 65.08152938750852, Learning Rate: 0.0005
Mean: 0.25936480846994364, Median: 0.23602916530475848, Num: 111
Epoch: 184, Loss: 64.60143577621643, Learning Rate: 0.0005
Epoch: 185, Loss: 59.732783047549695, Learning Rate: 0.0005
Mean: 0.26156330821983337, Median: 0.2589005003028134, Num: 111
Epoch: 186, Loss: 56.28162604642202, Learning Rate: 0.0005
Epoch: 187, Loss: 53.53242621364364, Learning Rate: 0.0005
Mean: 0.26203002594434066, Median: 0.2620009733907709, Num: 111
Epoch: 188, Loss: 68.58263922312173, Learning Rate: 0.0005
Epoch: 189, Loss: 63.850284576416016, Learning Rate: 0.0005
Mean: 0.2621438163387805, Median: 0.2474543144970753, Num: 111
Epoch: 190, Loss: 66.71002584480378, Learning Rate: 0.0005
Epoch: 191, Loss: 60.38057522601392, Learning Rate: 0.0005
Mean: 0.26483352889672424, Median: 0.26436198071280853, Num: 111
Epoch: 192, Loss: 63.712078117462525, Learning Rate: 0.0005
Epoch: 193, Loss: 65.28456170874905, Learning Rate: 0.0005
Mean: 0.26241375990827315, Median: 0.270414959327035, Num: 111
Epoch: 194, Loss: 61.440500673041285, Learning Rate: 0.0005
Epoch: 195, Loss: 64.96154862139599, Learning Rate: 0.0005
Mean: 0.2595430339599059, Median: 0.25455041884469365, Num: 111
Epoch: 196, Loss: 61.129624608051344, Learning Rate: 0.0005
Epoch: 197, Loss: 63.8171283538083, Learning Rate: 0.0005
Mean: 0.26753064021231293, Median: 0.26350992791767225, Num: 111
Epoch: 198, Loss: 59.56418572850974, Learning Rate: 0.0005
Epoch: 199, Loss: 73.79161230340061, Learning Rate: 0.0005
Mean: 0.26146919033370364, Median: 0.2513888820462254, Num: 111
Epoch: 200, Loss: 60.922336325588, Learning Rate: 0.0005
Epoch: 201, Loss: 68.79361377853945, Learning Rate: 0.0005
Mean: 0.2651416505947081, Median: 0.2608573859110779, Num: 111
Epoch: 202, Loss: 59.03897769192615, Learning Rate: 0.0005
Epoch: 203, Loss: 62.096664084009376, Learning Rate: 0.0005
Mean: 0.2662511708269349, Median: 0.26172627203572657, Num: 111
Epoch: 204, Loss: 57.81445861724486, Learning Rate: 0.0005
Epoch: 205, Loss: 59.42800126592797, Learning Rate: 0.0005
Mean: 0.2630302846703832, Median: 0.2556680795778708, Num: 111
Epoch: 206, Loss: 60.685666049819396, Learning Rate: 0.0005
Epoch: 207, Loss: 62.91658941521702, Learning Rate: 0.0005
Mean: 0.27056969275409254, Median: 0.26171625276931193, Num: 111
Epoch: 208, Loss: 65.83562641833203, Learning Rate: 0.0005
Epoch: 209, Loss: 66.05070106667209, Learning Rate: 0.0005
Mean: 0.25957457340781487, Median: 0.2573208736411127, Num: 111
Epoch: 210, Loss: 54.813493567776966, Learning Rate: 0.0005
Epoch: 211, Loss: 60.989998231451196, Learning Rate: 0.0005
Mean: 0.26693653239273596, Median: 0.26847991871965926, Num: 111
Epoch: 212, Loss: 67.78828079154692, Learning Rate: 0.0005
Epoch: 213, Loss: 53.231525559023204, Learning Rate: 0.0005
Mean: 0.2670985623679685, Median: 0.26267986582212177, Num: 111
Epoch: 214, Loss: 60.65888255475515, Learning Rate: 0.0005
Epoch: 215, Loss: 59.71045409053205, Learning Rate: 0.0005
Mean: 0.26488896205909984, Median: 0.24178159693286963, Num: 111
Epoch: 216, Loss: 64.5415035960186, Learning Rate: 0.0005
Epoch: 217, Loss: 61.048568059162925, Learning Rate: 0.0005
Mean: 0.265946494951957, Median: 0.2580466744384602, Num: 111
Epoch: 218, Loss: 56.1768615446895, Learning Rate: 0.0005
Epoch: 219, Loss: 60.81408493777356, Learning Rate: 0.0005
Mean: 0.26555929106739995, Median: 0.24106605369037956, Num: 111
Epoch: 220, Loss: 62.35833438046007, Learning Rate: 0.0005
Epoch: 221, Loss: 63.554690573588914, Learning Rate: 0.0005
Mean: 0.2584243686367012, Median: 0.23519829681797208, Num: 111
Epoch: 222, Loss: 63.04555195498179, Learning Rate: 0.0005
Epoch: 223, Loss: 63.53892548687487, Learning Rate: 0.0005
Mean: 0.2688225426634814, Median: 0.2648411038376323, Num: 111
Epoch: 224, Loss: 65.40173568495784, Learning Rate: 0.0005
Epoch: 225, Loss: 62.19316448648292, Learning Rate: 0.0005
Mean: 0.2615706629654012, Median: 0.2540297442557819, Num: 111
Epoch: 226, Loss: 68.55589733353581, Learning Rate: 0.0005
Epoch: 227, Loss: 57.448983600340696, Learning Rate: 0.0005
Mean: 0.2599185438814507, Median: 0.2591524059934088, Num: 111
Epoch: 228, Loss: 66.87825745272349, Learning Rate: 0.0005
Epoch: 229, Loss: 53.264634074934996, Learning Rate: 0.0005
Mean: 0.26341505486123346, Median: 0.2664865170334809, Num: 111
Epoch: 230, Loss: 61.68236010907644, Learning Rate: 0.0005
Epoch: 231, Loss: 55.60399138209331, Learning Rate: 0.0005
Mean: 0.2691156149313957, Median: 0.2746012411218785, Num: 111
Epoch: 232, Loss: 66.62308605607734, Learning Rate: 0.0005
Epoch: 233, Loss: 58.7549384013716, Learning Rate: 0.0005
Mean: 0.2717531916050202, Median: 0.2777029050534053, Num: 111
Epoch: 234, Loss: 60.306458300854786, Learning Rate: 0.0005
Epoch: 235, Loss: 60.97627113526126, Learning Rate: 0.0005
Mean: 0.271223554947352, Median: 0.27083253235438576, Num: 111
Epoch: 236, Loss: 51.290726466351245, Learning Rate: 0.0005
Epoch: 237, Loss: 58.26786709980792, Learning Rate: 0.0005
Mean: 0.2694129095201136, Median: 0.24784822340961782, Num: 111
Epoch: 238, Loss: 64.17565446876618, Learning Rate: 0.0005
Epoch: 239, Loss: 57.97425029938479, Learning Rate: 0.0005
Mean: 0.2670024716752605, Median: 0.26758983027157396, Num: 111
Epoch: 240, Loss: 51.736668896962364, Learning Rate: 0.0005
Epoch: 241, Loss: 63.79827084598771, Learning Rate: 0.0005
Mean: 0.2644465243202768, Median: 0.2593531642938949, Num: 111
Epoch: 242, Loss: 62.10150835887495, Learning Rate: 0.0005
Epoch: 243, Loss: 55.06927278817418, Learning Rate: 0.0005
Mean: 0.2655218583872905, Median: 0.2614609025783143, Num: 111
Epoch: 244, Loss: 55.39633753213538, Learning Rate: 0.0005
Epoch: 245, Loss: 63.16850881691439, Learning Rate: 0.0005
Mean: 0.26438801865360945, Median: 0.2542661615278833, Num: 111
Epoch: 246, Loss: 57.834026129848986, Learning Rate: 0.0005
Epoch: 247, Loss: 65.65960890988269, Learning Rate: 0.0005
Mean: 0.27120882977317756, Median: 0.26363009426264855, Num: 111
Epoch: 248, Loss: 57.26354440436306, Learning Rate: 0.0005
Epoch: 249, Loss: 64.34947769900403, Learning Rate: 0.0005
Mean: 0.2602322365066878, Median: 0.26840506438447703, Num: 111
Epoch: 250, Loss: 59.35831003303988, Learning Rate: 0.0005
Epoch: 251, Loss: 60.069640975400624, Learning Rate: 0.0005
Mean: 0.26985239077634043, Median: 0.25786527456827235, Num: 111
Epoch: 252, Loss: 62.61223671235234, Learning Rate: 0.0005
Epoch: 253, Loss: 70.74285532480263, Learning Rate: 0.0005
Mean: 0.26285084834091454, Median: 0.2377593606301582, Num: 111
Epoch: 254, Loss: 64.00827233188123, Learning Rate: 0.0005
Epoch: 255, Loss: 60.02347856544586, Learning Rate: 0.0005
Mean: 0.2628828584034286, Median: 0.2611884596259278, Num: 111
Epoch: 256, Loss: 58.180190563201904, Learning Rate: 0.0005
Epoch: 257, Loss: 53.85646691379777, Learning Rate: 0.0005
Mean: 0.26716472691586435, Median: 0.24504793709810824, Num: 111
Epoch: 258, Loss: 62.95221996307373, Learning Rate: 0.0005
Epoch: 259, Loss: 59.78558109467288, Learning Rate: 0.0005
Mean: 0.2642787150832986, Median: 0.24003181875241328, Num: 111
Epoch: 260, Loss: 56.9910166637007, Learning Rate: 0.0005
Epoch: 261, Loss: 63.32754364932876, Learning Rate: 0.0005
Mean: 0.27150446019790825, Median: 0.27414141279799087, Num: 111
Epoch: 262, Loss: 64.44874128088894, Learning Rate: 0.0005
Epoch: 263, Loss: 55.09496441806655, Learning Rate: 0.0005
Mean: 0.2699232673867544, Median: 0.27627366725067964, Num: 111
Epoch: 264, Loss: 63.304946853453856, Learning Rate: 0.0005
Epoch: 265, Loss: 57.650936287569714, Learning Rate: 0.0005
Mean: 0.2641139409575484, Median: 0.24508015664899208, Num: 111
Epoch: 266, Loss: 56.23870240636619, Learning Rate: 0.0005
Epoch: 267, Loss: 65.1390093837876, Learning Rate: 0.0005
Mean: 0.26481037576840083, Median: 0.23936214800152966, Num: 111
Epoch: 268, Loss: 64.37465593039272, Learning Rate: 0.0005
Epoch: 269, Loss: 57.84589418158474, Learning Rate: 0.0005
Mean: 0.2677677346841645, Median: 0.26968396853439847, Num: 111
Epoch: 270, Loss: 57.17807032114052, Learning Rate: 0.0005
Epoch: 271, Loss: 58.18694773065039, Learning Rate: 0.0005
Mean: 0.2739543200136576, Median: 0.2813561079932849, Num: 111
Epoch: 272, Loss: 66.33788294964526, Learning Rate: 0.0005
Epoch: 273, Loss: 60.378888796611, Learning Rate: 0.0005
Mean: 0.26737481231689386, Median: 0.24929169654787756, Num: 111
Epoch: 274, Loss: 64.05474858111646, Learning Rate: 0.0005
Epoch: 275, Loss: 57.93331153134265, Learning Rate: 0.0005
Mean: 0.2714093420935144, Median: 0.2748591378825057, Num: 111
Epoch: 276, Loss: 58.92825625890709, Learning Rate: 0.0005
Epoch: 277, Loss: 62.27694435579231, Learning Rate: 0.0005
Mean: 0.27149126982241145, Median: 0.26684093523409835, Num: 111
Epoch: 278, Loss: 64.29226717891463, Learning Rate: 0.0005
Epoch: 279, Loss: 56.83848490772477, Learning Rate: 0.0005
Mean: 0.27170468834431555, Median: 0.25955818569798944, Num: 111
Epoch: 280, Loss: 64.45151604801775, Learning Rate: 0.0005
Epoch: 281, Loss: 68.84599010053887, Learning Rate: 0.0005
Mean: 0.2732464299262276, Median: 0.25603506379811103, Num: 111
Epoch: 282, Loss: 61.50484054059867, Learning Rate: 0.0005
Epoch: 283, Loss: 54.870818930936146, Learning Rate: 0.0005
Mean: 0.27046206712423915, Median: 0.25937535095745756, Num: 111
Epoch: 284, Loss: 61.92613630409701, Learning Rate: 0.0005
Epoch: 285, Loss: 62.83614404517484, Learning Rate: 0.0005
Mean: 0.26852236697389265, Median: 0.2533696464782258, Num: 111
Epoch: 286, Loss: 53.587785146322595, Learning Rate: 0.0005
Epoch: 287, Loss: 67.29121842441789, Learning Rate: 0.0005
Mean: 0.270460039396075, Median: 0.27140926354836065, Num: 111
Epoch: 288, Loss: 64.20837769450911, Learning Rate: 0.0005
Epoch: 289, Loss: 60.95943048775914, Learning Rate: 0.0005
Mean: 0.26926276747615174, Median: 0.26883736585459284, Num: 111
Epoch: 290, Loss: 57.67586937869888, Learning Rate: 0.0005
Epoch: 291, Loss: 60.23973814263401, Learning Rate: 0.0005
Mean: 0.2711168126270808, Median: 0.2540918472434416, Num: 111
Epoch: 292, Loss: 57.268653927079164, Learning Rate: 0.0005
Epoch: 293, Loss: 54.71667585028223, Learning Rate: 0.0005
Mean: 0.2723538981239756, Median: 0.28458637208633714, Num: 111
Epoch: 294, Loss: 58.039211548954604, Learning Rate: 0.0005
Epoch: 295, Loss: 61.738969228353845, Learning Rate: 0.0005
Mean: 0.2680443446916211, Median: 0.25910370583252185, Num: 111
Epoch: 296, Loss: 63.82538602438318, Learning Rate: 0.0005
Epoch: 297, Loss: 57.57227972329381, Learning Rate: 0.0005
Mean: 0.26929549545661874, Median: 0.23501288683842858, Num: 111
Epoch: 298, Loss: 60.813620228365245, Learning Rate: 0.0005
Epoch: 299, Loss: 60.636839533426674, Learning Rate: 0.0005
Mean: 0.2690923023904931, Median: 0.24003437558035928, Num: 111
Epoch: 300, Loss: 57.09853583646108, Learning Rate: 0.0005
Epoch: 301, Loss: 63.34043338499873, Learning Rate: 0.0005
Mean: 0.27217708929287354, Median: 0.27863301368952503, Num: 111
Epoch: 302, Loss: 57.31385548143502, Learning Rate: 0.0005
Epoch: 303, Loss: 59.31493487990046, Learning Rate: 0.0005
Mean: 0.2673683049648123, Median: 0.25167960962459546, Num: 111
Epoch: 304, Loss: 62.58569074538817, Learning Rate: 0.0005
Epoch: 305, Loss: 59.72545207839414, Learning Rate: 0.0005
Mean: 0.26741882891570085, Median: 0.27107172660996987, Num: 111
Epoch: 306, Loss: 58.007098634558986, Learning Rate: 0.0005
Epoch: 307, Loss: 59.72941380236522, Learning Rate: 0.0005
Mean: 0.27117286530198076, Median: 0.26637491115852074, Num: 111
Epoch: 308, Loss: 58.50962241299181, Learning Rate: 0.0005
Epoch: 309, Loss: 55.188087779355335, Learning Rate: 0.0005
Mean: 0.2677596135034787, Median: 0.25139777136774855, Num: 111
Epoch: 310, Loss: 66.28591944223427, Learning Rate: 0.0005
Epoch: 311, Loss: 58.135449104998486, Learning Rate: 0.0005
Mean: 0.2695916969196129, Median: 0.26497400032611174, Num: 111
Epoch: 312, Loss: 58.497091408235484, Learning Rate: 0.0005
Epoch: 313, Loss: 58.12288017732551, Learning Rate: 0.0005
Mean: 0.26532244817370715, Median: 0.24345214486468772, Num: 111
Epoch: 314, Loss: 62.33262415966356, Learning Rate: 0.0005
Epoch: 315, Loss: 53.62163143847362, Learning Rate: 0.0005
Mean: 0.26697894700964203, Median: 0.25395607857986424, Num: 111
Epoch: 316, Loss: 55.480651074145214, Learning Rate: 0.0005
Epoch: 317, Loss: 61.85926252388092, Learning Rate: 0.0005
Mean: 0.2715310645306205, Median: 0.2573284105776347, Num: 111
Epoch: 318, Loss: 59.74548732803529, Learning Rate: 0.0005
Epoch: 319, Loss: 58.55684705527432, Learning Rate: 0.0005
Mean: 0.2671498654290567, Median: 0.24065672855958947, Num: 111
Epoch: 320, Loss: 59.82720646226262, Learning Rate: 0.0005
Epoch: 321, Loss: 63.16054375200387, Learning Rate: 0.0005
Mean: 0.2705804784253174, Median: 0.2667211837598857, Num: 111
Epoch: 322, Loss: 60.607125592519004, Learning Rate: 0.0005
Epoch: 323, Loss: 60.52379011246095, Learning Rate: 0.0005
Mean: 0.27007738322807806, Median: 0.24602535022528205, Num: 111
Epoch: 324, Loss: 51.145611671080076, Learning Rate: 0.0005
Epoch: 325, Loss: 59.416357873434045, Learning Rate: 0.0005
Mean: 0.2698048959335481, Median: 0.24657108308075124, Num: 111
Epoch: 326, Loss: 55.85751848335726, Learning Rate: 0.0005
Epoch: 327, Loss: 56.36850760930992, Learning Rate: 0.0005
Mean: 0.27472107200668533, Median: 0.25949061449189936, Num: 111
Epoch: 328, Loss: 64.36909183823919, Learning Rate: 0.0005
Epoch: 329, Loss: 58.94770312022014, Learning Rate: 0.0005
Mean: 0.26842066866045655, Median: 0.2547960842550758, Num: 111
Epoch: 330, Loss: 56.37225703733513, Learning Rate: 0.0005
Epoch: 331, Loss: 63.236506105905555, Learning Rate: 0.0005
Mean: 0.2746299556828946, Median: 0.26443940180521613, Num: 111
Epoch: 332, Loss: 59.557212910020205, Learning Rate: 0.0005
Epoch: 333, Loss: 58.85315361942153, Learning Rate: 0.0005
Mean: 0.26772758041443845, Median: 0.25400279978863033, Num: 111
Epoch: 334, Loss: 62.031841404466746, Learning Rate: 0.0005
Epoch: 335, Loss: 56.60357077724962, Learning Rate: 0.0005
Mean: 0.2688862098510628, Median: 0.2472712288796696, Num: 111
Epoch: 336, Loss: 52.45646084934832, Learning Rate: 0.0005
Epoch: 337, Loss: 59.232419002487, Learning Rate: 0.0005
Mean: 0.27159383051180097, Median: 0.2560060596979842, Num: 111
Epoch: 338, Loss: 60.919962549784096, Learning Rate: 0.0005
Epoch: 339, Loss: 52.45960210317589, Learning Rate: 0.0005
Mean: 0.27380003371855827, Median: 0.2692765539358662, Num: 111
Epoch: 340, Loss: 62.177483087562656, Learning Rate: 0.0005
Epoch: 341, Loss: 56.60913699506277, Learning Rate: 0.0005
Mean: 0.27091083190573567, Median: 0.266283969427061, Num: 111
Epoch: 342, Loss: 56.0425841779594, Learning Rate: 0.0005
Epoch: 343, Loss: 57.65267890332693, Learning Rate: 0.0005
Mean: 0.2759076427962768, Median: 0.249249482298907, Num: 111
Epoch: 344, Loss: 55.21491377037692, Learning Rate: 0.0005
Epoch: 345, Loss: 58.529775521841394, Learning Rate: 0.0005
Mean: 0.2733458316362844, Median: 0.26262070709752033, Num: 111
Epoch: 346, Loss: 62.83678496027567, Learning Rate: 0.0005
Epoch: 347, Loss: 55.53000645465161, Learning Rate: 0.0005
Mean: 0.2698784805977499, Median: 0.2433243656526252, Num: 111
Epoch: 348, Loss: 61.72135118691318, Learning Rate: 0.0005
Epoch: 349, Loss: 64.07368076278503, Learning Rate: 0.0005
Mean: 0.2706758505422355, Median: 0.24990002822770566, Num: 111
Epoch: 350, Loss: 59.05751789047057, Learning Rate: 0.0005
Epoch: 351, Loss: 54.55932440240699, Learning Rate: 0.0005
Mean: 0.2689454005298852, Median: 0.2579233494038151, Num: 111
Epoch: 352, Loss: 57.82399432630424, Learning Rate: 0.0005
Epoch: 353, Loss: 56.70629241667598, Learning Rate: 0.0005
Mean: 0.2750811589878967, Median: 0.28482943999421895, Num: 111
Epoch: 354, Loss: 53.97083142292069, Learning Rate: 0.0005
Epoch: 355, Loss: 59.13120368589838, Learning Rate: 0.0005
Mean: 0.2723533225449395, Median: 0.25516299571347106, Num: 111
Epoch: 356, Loss: 57.96498638750559, Learning Rate: 0.0005
Epoch: 357, Loss: 63.217053505311526, Learning Rate: 0.0005
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
Mean: 0.27214206944092856, Median: 0.25779459228032336, Num: 111
Epoch: 358, Loss: 58.57260398405144, Learning Rate: 0.0005
Epoch: 359, Loss: 55.61150446282812, Learning Rate: 0.0005
Mean: 0.26758028721590355, Median: 0.23316233949329754, Num: 111
Epoch: 360, Loss: 54.16193050936044, Learning Rate: 0.0005
Epoch: 361, Loss: 57.73080487423633, Learning Rate: 0.0005
Mean: 0.27165985837759604, Median: 0.2638650600256802, Num: 111
Epoch: 362, Loss: 58.34520962726639, Learning Rate: 0.0005
Epoch: 363, Loss: 58.6570128245526, Learning Rate: 0.0005
Mean: 0.26936969139438155, Median: 0.2669615766528281, Num: 111
Epoch: 364, Loss: 64.05757323230605, Learning Rate: 0.0005
Epoch: 365, Loss: 67.20398403075804, Learning Rate: 0.0005
Mean: 0.2736366761825407, Median: 0.2617979171493367, Num: 111
Epoch: 366, Loss: 57.079723013452735, Learning Rate: 0.0005
Epoch: 367, Loss: 59.65535421256559, Learning Rate: 0.0005
Mean: 0.2700004227824264, Median: 0.2518550017713158, Num: 111
Epoch: 368, Loss: 61.064859775175535, Learning Rate: 0.0005
Epoch: 369, Loss: 57.0426476777318, Learning Rate: 0.0005
Mean: 0.2677716621745574, Median: 0.2613281229935236, Num: 111
Epoch: 370, Loss: 54.78985471610563, Learning Rate: 0.0005
Epoch: 371, Loss: 61.361143502844385, Learning Rate: 0.0005
Mean: 0.27561614051460775, Median: 0.2536067439052283, Num: 111
Epoch: 372, Loss: 57.47938666286239, Learning Rate: 0.0005
Epoch: 373, Loss: 58.931364001997984, Learning Rate: 0.0005
Mean: 0.2769152700597745, Median: 0.25571418033087534, Num: 111
Epoch: 374, Loss: 48.03739319077457, Learning Rate: 0.0005
Epoch: 375, Loss: 60.74385600492179, Learning Rate: 0.0005
Mean: 0.2723224740859675, Median: 0.2726684221730481, Num: 111
Epoch: 376, Loss: 53.128630787493236, Learning Rate: 0.0005
Epoch: 377, Loss: 62.45881119693618, Learning Rate: 0.0005
Mean: 0.2707855542543388, Median: 0.27843727303450344, Num: 111
Epoch: 378, Loss: 63.47308646052717, Learning Rate: 0.0005
Epoch: 379, Loss: 64.1974535333105, Learning Rate: 0.0005
Mean: 0.2725823012024508, Median: 0.255615926889094, Num: 111
Epoch: 380, Loss: 55.10975663058729, Learning Rate: 0.0005
Epoch: 381, Loss: 56.836243778826244, Learning Rate: 0.0005
Mean: 0.27073434434531984, Median: 0.2663507035504189, Num: 111
Epoch: 382, Loss: 60.64654711068395, Learning Rate: 0.0005
Epoch: 383, Loss: 62.83625416583325, Learning Rate: 0.0005
Mean: 0.2784298380866111, Median: 0.2558511688800254, Num: 111
Epoch: 384, Loss: 58.27545205081802, Learning Rate: 0.0005
Epoch: 385, Loss: 61.60281477778791, Learning Rate: 0.0005
Mean: 0.27733113861793335, Median: 0.27034034366871934, Num: 111
Epoch: 386, Loss: 65.80588565964297, Learning Rate: 0.0005
Epoch: 387, Loss: 61.080942659492955, Learning Rate: 0.0005
Mean: 0.2688469007310594, Median: 0.2571494040962882, Num: 111
Epoch: 388, Loss: 67.60817696100258, Learning Rate: 0.0005
Epoch: 389, Loss: 57.48311824683683, Learning Rate: 0.0005
Mean: 0.2696458309365177, Median: 0.24730263354402215, Num: 111
Epoch: 390, Loss: 58.288126290562644, Learning Rate: 0.0005
Epoch: 391, Loss: 62.54391173856804, Learning Rate: 0.0005
Mean: 0.27379020638077883, Median: 0.26156277729967264, Num: 111
Epoch: 392, Loss: 59.15602856371776, Learning Rate: 0.0005
Epoch: 393, Loss: 54.19943202834531, Learning Rate: 0.0005
Mean: 0.275945665003023, Median: 0.2748570723636136, Num: 111
Epoch: 394, Loss: 60.96822547912598, Learning Rate: 0.0005
Epoch: 395, Loss: 54.886099436196936, Learning Rate: 0.0005
Mean: 0.27522292247292807, Median: 0.2852925100099944, Num: 111
Epoch: 396, Loss: 67.61719708270337, Learning Rate: 0.0005
Epoch: 397, Loss: 56.46387659785259, Learning Rate: 0.0005
Mean: 0.2761295095539375, Median: 0.26278916179395634, Num: 111
Epoch: 398, Loss: 50.95301876297916, Learning Rate: 0.0005
Epoch: 399, Loss: 56.34189640183047, Learning Rate: 0.0005
Mean: 0.27524318287578947, Median: 0.2641910416997938, Num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
10
Epoch: 0, Loss: 237.94445001073632, Learning Rate: 0.0005
Epoch: 1, Loss: 190.66154741953653, Learning Rate: 0.0005
Mean: 0.05503524453120754, Median: 0.01743058077915825, Num: 111
Epoch: 2, Loss: 173.30725897363868, Learning Rate: 0.0005
Epoch: 3, Loss: 171.56018254843102, Learning Rate: 0.0005
Mean: 0.0859366256877413, Median: 0.03571128959616042, Num: 111
Epoch: 4, Loss: 156.0014333150473, Learning Rate: 0.0005
Epoch: 5, Loss: 156.963157745729, Learning Rate: 0.0005
Mean: 0.10566874539140683, Median: 0.05986480536727432, Num: 111
Epoch: 6, Loss: 152.6008971340685, Learning Rate: 0.0005
Epoch: 7, Loss: 148.41173259321465, Learning Rate: 0.0005
Mean: 0.13379011894980222, Median: 0.09397516545103235, Num: 111
Epoch: 8, Loss: 131.90650459657232, Learning Rate: 0.0005
Epoch: 9, Loss: 140.228223318077, Learning Rate: 0.0005
Mean: 0.1424412098350541, Median: 0.10444222149961242, Num: 111
Epoch: 10, Loss: 132.79483262027603, Learning Rate: 0.0005
Epoch: 11, Loss: 135.6145805450807, Learning Rate: 0.0005
Mean: 0.1623923325925559, Median: 0.13811861170325865, Num: 111
Epoch: 12, Loss: 133.1456504729857, Learning Rate: 0.0005
Epoch: 13, Loss: 126.77659721834114, Learning Rate: 0.0005
Mean: 0.1533774638762918, Median: 0.12462635939724678, Num: 111
Epoch: 14, Loss: 129.95713030573833, Learning Rate: 0.0005
Epoch: 15, Loss: 121.55647029646907, Learning Rate: 0.0005
Mean: 0.17486193148394913, Median: 0.14963454838171275, Num: 111
Epoch: 16, Loss: 123.0581827738199, Learning Rate: 0.0005
Epoch: 17, Loss: 119.777076629271, Learning Rate: 0.0005
Mean: 0.18157424571202005, Median: 0.16774365859326473, Num: 111
Epoch: 18, Loss: 124.66965813234629, Learning Rate: 0.0005
Epoch: 19, Loss: 110.59038814866399, Learning Rate: 0.0005
Mean: 0.18491797194835471, Median: 0.16417005355621694, Num: 111
Epoch: 20, Loss: 112.15242298539863, Learning Rate: 0.0005
Epoch: 21, Loss: 107.17772031117634, Learning Rate: 0.0005
Mean: 0.19549310989636615, Median: 0.18275773918736254, Num: 111
Epoch: 22, Loss: 112.90030097961426, Learning Rate: 0.0005
Epoch: 23, Loss: 108.88325973878423, Learning Rate: 0.0005
Mean: 0.19240682505383422, Median: 0.17146900807245585, Num: 111
Epoch: 24, Loss: 108.52127502625247, Learning Rate: 0.0005
Epoch: 25, Loss: 105.27391571596445, Learning Rate: 0.0005
Mean: 0.1971559974750346, Median: 0.19311746791000578, Num: 111
Epoch: 26, Loss: 103.28338641430958, Learning Rate: 0.0005
Epoch: 27, Loss: 103.06237949233457, Learning Rate: 0.0005
Mean: 0.1967751822277227, Median: 0.19417710526602402, Num: 111
Epoch: 28, Loss: 104.21470154911638, Learning Rate: 0.0005
Epoch: 29, Loss: 108.68508584815335, Learning Rate: 0.0005
Mean: 0.19948129208057222, Median: 0.18982173925920384, Num: 111
Epoch: 30, Loss: 99.70763445475015, Learning Rate: 0.0005
Epoch: 31, Loss: 100.5037440564259, Learning Rate: 0.0005
Mean: 0.19879372054946276, Median: 0.1809132004266621, Num: 111
Epoch: 32, Loss: 96.97196526125254, Learning Rate: 0.0005
Epoch: 33, Loss: 98.43402568403496, Learning Rate: 0.0005
Mean: 0.20577492225661637, Median: 0.18882271328615202, Num: 111
Epoch: 34, Loss: 95.65153216166668, Learning Rate: 0.0005
Epoch: 35, Loss: 91.6334870579731, Learning Rate: 0.0005
Mean: 0.2016747522166035, Median: 0.19205182159070094, Num: 111
Epoch: 36, Loss: 103.50090118775884, Learning Rate: 0.0005
Epoch: 37, Loss: 93.57888412475586, Learning Rate: 0.0005
Mean: 0.20525418975777465, Median: 0.19724402572604624, Num: 111
Epoch: 38, Loss: 89.65832305816282, Learning Rate: 0.0005
Epoch: 39, Loss: 92.07008251511907, Learning Rate: 0.0005
Mean: 0.2179358449969419, Median: 0.20877085635252002, Num: 111
Epoch: 40, Loss: 100.3727274170841, Learning Rate: 0.0005
Epoch: 41, Loss: 97.6423528970006, Learning Rate: 0.0005
Mean: 0.21405331060471244, Median: 0.20343097168027183, Num: 111
Epoch: 42, Loss: 90.38607146941035, Learning Rate: 0.0005
Epoch: 43, Loss: 88.40642014469009, Learning Rate: 0.0005
Mean: 0.21645318158225213, Median: 0.21704033141468682, Num: 111
Epoch: 44, Loss: 92.10962472479028, Learning Rate: 0.0005
Epoch: 45, Loss: 85.77302978699466, Learning Rate: 0.0005
Mean: 0.21617513393708768, Median: 0.20318553920211804, Num: 111
Epoch: 46, Loss: 88.18509784376765, Learning Rate: 0.0005
Epoch: 47, Loss: 87.0128752467144, Learning Rate: 0.0005
Mean: 0.22491670011463682, Median: 0.21014114356204666, Num: 111
Epoch: 48, Loss: 90.34584750899349, Learning Rate: 0.0005
Epoch: 49, Loss: 90.05261904934802, Learning Rate: 0.0005
Mean: 0.2227852610204777, Median: 0.21119224923801289, Num: 111
Epoch: 50, Loss: 90.48067607649837, Learning Rate: 0.0005
Epoch: 51, Loss: 87.8285720319633, Learning Rate: 0.0005
Mean: 0.22103829654355356, Median: 0.20640620968047232, Num: 111
Epoch: 52, Loss: 91.23480272867593, Learning Rate: 0.0005
Epoch: 53, Loss: 81.44566138393908, Learning Rate: 0.0005
Mean: 0.21881663887141292, Median: 0.20699009007476082, Num: 111
Epoch: 54, Loss: 84.53952917995223, Learning Rate: 0.0005
Epoch: 55, Loss: 88.80255471654685, Learning Rate: 0.0005
Mean: 0.22674365079629952, Median: 0.2213639655177522, Num: 111
Epoch: 56, Loss: 81.96781415824431, Learning Rate: 0.0005
Epoch: 57, Loss: 85.52383889347674, Learning Rate: 0.0005
Mean: 0.22897860199083736, Median: 0.22638978643851612, Num: 111
Epoch: 58, Loss: 85.50112607105669, Learning Rate: 0.0005
Epoch: 59, Loss: 89.55550990047225, Learning Rate: 0.0005
Mean: 0.21816921478227697, Median: 0.2069185979142726, Num: 111
Epoch: 60, Loss: 83.94428588683347, Learning Rate: 0.0005
Epoch: 61, Loss: 82.75492201655744, Learning Rate: 0.0005
Mean: 0.2295038168366623, Median: 0.21022700649713266, Num: 111
Epoch: 62, Loss: 86.90423579388354, Learning Rate: 0.0005
Epoch: 63, Loss: 74.2059246201113, Learning Rate: 0.0005
Mean: 0.23256733506078192, Median: 0.22237916929146126, Num: 111
Epoch: 64, Loss: 82.30326339997441, Learning Rate: 0.0005
Epoch: 65, Loss: 79.03022605252553, Learning Rate: 0.0005
Mean: 0.22721809129989365, Median: 0.20749469560401845, Num: 111
Epoch: 66, Loss: 75.34338586876191, Learning Rate: 0.0005
Epoch: 67, Loss: 78.87297584349851, Learning Rate: 0.0005
Mean: 0.2272070252028943, Median: 0.20831162985508997, Num: 111
Epoch: 68, Loss: 75.22551961691983, Learning Rate: 0.0005
Epoch: 69, Loss: 85.91248114712268, Learning Rate: 0.0005
Mean: 0.23790104325071224, Median: 0.2421788139749762, Num: 111
Epoch: 70, Loss: 78.67812108419028, Learning Rate: 0.0005
Epoch: 71, Loss: 81.25271561634109, Learning Rate: 0.0005
Mean: 0.23796598262697152, Median: 0.238826521682124, Num: 111
Epoch: 72, Loss: 79.75070103105293, Learning Rate: 0.0005
Epoch: 73, Loss: 77.59656713095056, Learning Rate: 0.0005
Mean: 0.2378855123742134, Median: 0.2406774322193904, Num: 111
Epoch: 74, Loss: 83.94163145501929, Learning Rate: 0.0005
Epoch: 75, Loss: 82.08304136345185, Learning Rate: 0.0005
Mean: 0.23476560686253756, Median: 0.23493843161365602, Num: 111
Epoch: 76, Loss: 77.89404933423882, Learning Rate: 0.0005
Epoch: 77, Loss: 81.21598011614329, Learning Rate: 0.0005
Mean: 0.2337799193673645, Median: 0.22537587389653113, Num: 111
Epoch: 78, Loss: 77.42085806145725, Learning Rate: 0.0005
Epoch: 79, Loss: 75.30466863333461, Learning Rate: 0.0005
Mean: 0.23832147619652294, Median: 0.226294630631455, Num: 111
Epoch: 80, Loss: 72.86620806498699, Learning Rate: 0.0005
Epoch: 81, Loss: 72.12385867015425, Learning Rate: 0.0005
Mean: 0.23919191146265417, Median: 0.22241144673841912, Num: 111
Epoch: 82, Loss: 73.98782435957207, Learning Rate: 0.0005
Epoch: 83, Loss: 74.09366203216184, Learning Rate: 0.0005
Mean: 0.2395744091087314, Median: 0.24183224933598751, Num: 111
Epoch: 84, Loss: 71.29243190030017, Learning Rate: 0.0005
Epoch: 85, Loss: 67.14887095072183, Learning Rate: 0.0005
Mean: 0.236617503040595, Median: 0.20793025679116195, Num: 111
Epoch: 86, Loss: 78.00861493076187, Learning Rate: 0.0005
Epoch: 87, Loss: 91.7106405740761, Learning Rate: 0.0005
Mean: 0.2443485566499221, Median: 0.22077460027337853, Num: 111
Epoch: 88, Loss: 68.40757831895208, Learning Rate: 0.0005
Epoch: 89, Loss: 74.59049114549016, Learning Rate: 0.0005
Mean: 0.24724711891612206, Median: 0.24301495565348177, Num: 111
Epoch: 90, Loss: 68.5903205871582, Learning Rate: 0.0005
Epoch: 91, Loss: 62.36262229551752, Learning Rate: 0.0005
Mean: 0.24830885940116315, Median: 0.24604530985196815, Num: 111
Epoch: 92, Loss: 68.23111586972891, Learning Rate: 0.0005
Epoch: 93, Loss: 75.99810161360774, Learning Rate: 0.0005
Mean: 0.24852462389621613, Median: 0.24308769947485623, Num: 111
Epoch: 94, Loss: 67.35495314540633, Learning Rate: 0.0005
Epoch: 95, Loss: 64.55682864821101, Learning Rate: 0.0005
Mean: 0.24572647270893858, Median: 0.257343077666842, Num: 111
Epoch: 96, Loss: 70.58569086603372, Learning Rate: 0.0005
Epoch: 97, Loss: 74.6010277989399, Learning Rate: 0.0005
Mean: 0.24155846435119868, Median: 0.22836854491949699, Num: 111
Epoch: 98, Loss: 71.48055962482131, Learning Rate: 0.0005
Epoch: 99, Loss: 72.81637574391193, Learning Rate: 0.0005
Mean: 0.24709023097942598, Median: 0.22729029309221183, Num: 111
Epoch: 100, Loss: 74.26920818420778, Learning Rate: 0.0005
Epoch: 101, Loss: 63.428922331476784, Learning Rate: 0.0005
Mean: 0.24242802466701402, Median: 0.218893014270794, Num: 111
Epoch: 102, Loss: 76.14692157147879, Learning Rate: 0.0005
Epoch: 103, Loss: 71.97765861649111, Learning Rate: 0.0005
Mean: 0.2522025708524652, Median: 0.25402652098458633, Num: 111
Epoch: 104, Loss: 68.73211658431823, Learning Rate: 0.0005
Epoch: 105, Loss: 64.31931701338435, Learning Rate: 0.0005
Mean: 0.24187997037174558, Median: 0.24009931110190322, Num: 111
Epoch: 106, Loss: 71.7699793620282, Learning Rate: 0.0005
Epoch: 107, Loss: 66.65045618723674, Learning Rate: 0.0005
Mean: 0.2482134314598751, Median: 0.2486872931844267, Num: 111
Epoch: 108, Loss: 69.2355120325663, Learning Rate: 0.0005
Epoch: 109, Loss: 70.20390325569245, Learning Rate: 0.0005
Mean: 0.24561847452683447, Median: 0.24367825904626386, Num: 111
Epoch: 110, Loss: 59.8021309174687, Learning Rate: 0.0005
Epoch: 111, Loss: 64.76781878413924, Learning Rate: 0.0005
Mean: 0.24332090416603686, Median: 0.2419471822329178, Num: 111
Epoch: 112, Loss: 65.55264798129897, Learning Rate: 0.0005
Epoch: 113, Loss: 67.88527797790896, Learning Rate: 0.0005
Mean: 0.2477377620715525, Median: 0.23121726452734595, Num: 111
Epoch: 114, Loss: 65.23133514588137, Learning Rate: 0.0005
Epoch: 115, Loss: 73.7039508589779, Learning Rate: 0.0005
Mean: 0.25320661721694854, Median: 0.24772594870275794, Num: 111
Epoch: 116, Loss: 65.47225938360376, Learning Rate: 0.0005
Epoch: 117, Loss: 64.93625990166721, Learning Rate: 0.0005
Mean: 0.2488097123775477, Median: 0.2368586469731876, Num: 111
Epoch: 118, Loss: 73.69889475351357, Learning Rate: 0.0005
Epoch: 119, Loss: 73.93794465352254, Learning Rate: 0.0005
Mean: 0.24735215319564946, Median: 0.22903377202955122, Num: 111
Epoch: 120, Loss: 61.2922042249197, Learning Rate: 0.0005
Epoch: 121, Loss: 78.73111705320427, Learning Rate: 0.0005
Mean: 0.24985790875366665, Median: 0.2419919156628614, Num: 111
Epoch: 122, Loss: 71.87306810861611, Learning Rate: 0.0005
Epoch: 123, Loss: 63.597576899700854, Learning Rate: 0.0005
Mean: 0.24589169305842656, Median: 0.22067437759816977, Num: 111
Epoch: 124, Loss: 71.31007456492229, Learning Rate: 0.0005
Epoch: 125, Loss: 61.70257175399596, Learning Rate: 0.0005
Mean: 0.2559656239678797, Median: 0.24901759434714044, Num: 111
Epoch: 126, Loss: 68.17983530802898, Learning Rate: 0.0005
Epoch: 127, Loss: 66.49686115908335, Learning Rate: 0.0005
Mean: 0.2510820736573253, Median: 0.24132357293417994, Num: 111
Epoch: 128, Loss: 67.19865727711873, Learning Rate: 0.0005
Epoch: 129, Loss: 69.98906491750694, Learning Rate: 0.0005
Mean: 0.25434836868326194, Median: 0.24988025977445835, Num: 111
Epoch: 130, Loss: 68.56371277498911, Learning Rate: 0.0005
Epoch: 131, Loss: 60.209670216204174, Learning Rate: 0.0005
Mean: 0.25213226038017306, Median: 0.2369041180285113, Num: 111
Epoch: 132, Loss: 69.23644968975022, Learning Rate: 0.0005
Epoch: 133, Loss: 60.12267548204905, Learning Rate: 0.0005
Mean: 0.2557626744636506, Median: 0.2559148056627645, Num: 111
Epoch: 134, Loss: 63.797344322664195, Learning Rate: 0.0005
Epoch: 135, Loss: 64.19320709734077, Learning Rate: 0.0005
Mean: 0.2492483921197986, Median: 0.23853614166187834, Num: 111
Epoch: 136, Loss: 63.563579076743984, Learning Rate: 0.0005
Epoch: 137, Loss: 75.72884744621184, Learning Rate: 0.0005
Mean: 0.25307168520850426, Median: 0.25434262574826916, Num: 111
Epoch: 138, Loss: 62.67393197209002, Learning Rate: 0.0005
Epoch: 139, Loss: 65.16802677476262, Learning Rate: 0.0005
Mean: 0.2574204804157395, Median: 0.25094227900376986, Num: 111
Epoch: 140, Loss: 63.12971036979951, Learning Rate: 0.0005
Epoch: 141, Loss: 63.89614119012672, Learning Rate: 0.0005
Mean: 0.25780980827442695, Median: 0.25467046914208247, Num: 111
Epoch: 142, Loss: 68.72065470017583, Learning Rate: 0.0005
Epoch: 143, Loss: 61.77852198589279, Learning Rate: 0.0005
Mean: 0.25274327239652183, Median: 0.2306225770576776, Num: 111
Epoch: 144, Loss: 68.41204702997783, Learning Rate: 0.0005
Epoch: 145, Loss: 63.85369514557252, Learning Rate: 0.0005
Mean: 0.2549098856185125, Median: 0.25127805822828875, Num: 111
Epoch: 146, Loss: 69.95599793813315, Learning Rate: 0.0005
Epoch: 147, Loss: 67.87872012264758, Learning Rate: 0.0005
Mean: 0.25458431482066907, Median: 0.2588691079490794, Num: 111
Epoch: 148, Loss: 70.50729319560959, Learning Rate: 0.0005
Epoch: 149, Loss: 64.32453769086355, Learning Rate: 0.0005
Mean: 0.2542873790761208, Median: 0.2566938714209236, Num: 111
Epoch: 150, Loss: 59.44133955024811, Learning Rate: 0.0005
Epoch: 151, Loss: 70.7928684648261, Learning Rate: 0.0005
Mean: 0.2554476506090882, Median: 0.23026895763136665, Num: 111
Epoch: 152, Loss: 68.95747474302729, Learning Rate: 0.0005
Epoch: 153, Loss: 61.041277667126025, Learning Rate: 0.0005
Mean: 0.2583266755674853, Median: 0.26586423763936823, Num: 111
Epoch: 154, Loss: 62.17369426589414, Learning Rate: 0.0005
Epoch: 155, Loss: 60.302189447793616, Learning Rate: 0.0005
Mean: 0.24943838050297085, Median: 0.22965471535674184, Num: 111
Epoch: 156, Loss: 66.06907837649426, Learning Rate: 0.0005
Epoch: 157, Loss: 76.7540765095906, Learning Rate: 0.0005
Mean: 0.25881607770194154, Median: 0.2586780037756739, Num: 111
Epoch: 158, Loss: 62.503342881260146, Learning Rate: 0.0005
Epoch: 159, Loss: 61.62022877888507, Learning Rate: 0.0005
Mean: 0.2598524077340339, Median: 0.24570994123755363, Num: 111
Epoch: 160, Loss: 65.76691310377007, Learning Rate: 0.0005
Epoch: 161, Loss: 63.79365201743252, Learning Rate: 0.0005
Mean: 0.25807341286461566, Median: 0.2441406627038066, Num: 111
Epoch: 162, Loss: 63.720971233873485, Learning Rate: 0.0005
Epoch: 163, Loss: 62.019344502184765, Learning Rate: 0.0005
Mean: 0.2519473387796113, Median: 0.2354877056980378, Num: 111
Epoch: 164, Loss: 67.83116660060652, Learning Rate: 0.0005
Epoch: 165, Loss: 62.69538500222815, Learning Rate: 0.0005
Mean: 0.2626875743112789, Median: 0.2642990214260919, Num: 111
Epoch: 166, Loss: 62.743334414011024, Learning Rate: 0.0005
Epoch: 167, Loss: 64.39896210130439, Learning Rate: 0.0005
Mean: 0.2600023088343149, Median: 0.25447415389675687, Num: 111
Epoch: 168, Loss: 58.9562885330384, Learning Rate: 0.0005
Epoch: 169, Loss: 61.589729401002444, Learning Rate: 0.0005
Mean: 0.2623662715243875, Median: 0.2493752359013951, Num: 111
Epoch: 170, Loss: 64.3549942338323, Learning Rate: 0.0005
Epoch: 171, Loss: 63.603707135441795, Learning Rate: 0.0005
Mean: 0.25867447974163726, Median: 0.23671643410111168, Num: 111
Epoch: 172, Loss: 62.21796559712973, Learning Rate: 0.0005
Epoch: 173, Loss: 65.31308754380927, Learning Rate: 0.0005
Mean: 0.259694284886298, Median: 0.2501660869034285, Num: 111
Epoch: 174, Loss: 58.53161235028003, Learning Rate: 0.0005
Epoch: 175, Loss: 65.3447557472321, Learning Rate: 0.0005
Mean: 0.2615668414422993, Median: 0.2556026206525205, Num: 111
Epoch: 176, Loss: 64.35762947725962, Learning Rate: 0.0005
Epoch: 177, Loss: 60.7878750720656, Learning Rate: 0.0005
Mean: 0.25675762517907585, Median: 0.24383366107992488, Num: 111
Epoch: 178, Loss: 61.62658467924739, Learning Rate: 0.0005
Epoch: 179, Loss: 67.67096352289958, Learning Rate: 0.0005
Mean: 0.2665537518625195, Median: 0.2670170951409674, Num: 111
Epoch: 180, Loss: 62.78845715810017, Learning Rate: 0.0005
Epoch: 181, Loss: 70.46697425842285, Learning Rate: 0.0005
Mean: 0.26504349568445823, Median: 0.26341649980601595, Num: 111
Epoch: 182, Loss: 68.23866468452546, Learning Rate: 0.0005
Epoch: 183, Loss: 65.06210957952293, Learning Rate: 0.0005
Mean: 0.26196095082306026, Median: 0.267117041559202, Num: 111
Epoch: 184, Loss: 64.57900449454067, Learning Rate: 0.0005
Epoch: 185, Loss: 59.74066757294069, Learning Rate: 0.0005
Mean: 0.2595056803022254, Median: 0.24021495926751382, Num: 111
Epoch: 186, Loss: 56.28979005009295, Learning Rate: 0.0005
Epoch: 187, Loss: 53.51446278123971, Learning Rate: 0.0005
Mean: 0.2609752856966279, Median: 0.2547363774751613, Num: 111
Epoch: 188, Loss: 68.56593619197248, Learning Rate: 0.0005
Epoch: 189, Loss: 63.834817449730565, Learning Rate: 0.0005
Mean: 0.26207387185534914, Median: 0.24446403995850682, Num: 111
Epoch: 190, Loss: 66.70261048696128, Learning Rate: 0.0005
Epoch: 191, Loss: 60.36511016753783, Learning Rate: 0.0005
Mean: 0.26611303900512895, Median: 0.26628983050311406, Num: 111
Epoch: 192, Loss: 63.71640331773873, Learning Rate: 0.0005
Epoch: 193, Loss: 65.26245544617434, Learning Rate: 0.0005
Mean: 0.25987836700370776, Median: 0.26110817537625564, Num: 111
Epoch: 194, Loss: 61.42796378537833, Learning Rate: 0.0005
Epoch: 195, Loss: 64.93848512258874, Learning Rate: 0.0005
Mean: 0.26118517792535606, Median: 0.2414366609708138, Num: 111
Epoch: 196, Loss: 61.081254890166136, Learning Rate: 0.0005
Epoch: 197, Loss: 63.78929220911968, Learning Rate: 0.0005
Mean: 0.2645879290553512, Median: 0.26975231765762175, Num: 111
Epoch: 198, Loss: 59.562276380607884, Learning Rate: 0.0005
Epoch: 199, Loss: 73.76741163414646, Learning Rate: 0.0005
Mean: 0.2610380931311909, Median: 0.24837963177935787, Num: 111
Epoch: 200, Loss: 60.911834889147656, Learning Rate: 0.0005
Epoch: 201, Loss: 68.77853889924934, Learning Rate: 0.0005
Mean: 0.2639312053114691, Median: 0.2572893521167221, Num: 111
Epoch: 202, Loss: 59.021683164389735, Learning Rate: 0.0005
Epoch: 203, Loss: 62.07877427985869, Learning Rate: 0.0005
Mean: 0.2686289563659835, Median: 0.2616143690541902, Num: 111
Epoch: 204, Loss: 57.801886397671986, Learning Rate: 0.0005
Epoch: 205, Loss: 59.42774005108569, Learning Rate: 0.0005
Mean: 0.26034402292944614, Median: 0.257073571828492, Num: 111
Epoch: 206, Loss: 60.688858721629686, Learning Rate: 0.0005
Epoch: 207, Loss: 62.900925969503014, Learning Rate: 0.0005
Mean: 0.26746354875605, Median: 0.26634341573567133, Num: 111
Epoch: 208, Loss: 65.83470395099685, Learning Rate: 0.0005
Epoch: 209, Loss: 66.02556918615318, Learning Rate: 0.0005
Mean: 0.2584187220552254, Median: 0.2537211846677226, Num: 111
Epoch: 210, Loss: 54.79599714853678, Learning Rate: 0.0005
Epoch: 211, Loss: 60.968411686908766, Learning Rate: 0.0005
Mean: 0.26482757088169395, Median: 0.26645148136676, Num: 111
Epoch: 212, Loss: 67.77733676404839, Learning Rate: 0.0005
Epoch: 213, Loss: 53.231421597032664, Learning Rate: 0.0005
Mean: 0.26571484560548575, Median: 0.2598904868714217, Num: 111
Epoch: 214, Loss: 60.639759822064136, Learning Rate: 0.0005
Epoch: 215, Loss: 59.69861446518496, Learning Rate: 0.0005
Mean: 0.2618803791912675, Median: 0.2514340177999155, Num: 111
Epoch: 216, Loss: 64.54383997170322, Learning Rate: 0.0005
Epoch: 217, Loss: 61.053142984229396, Learning Rate: 0.0005
Mean: 0.26799029569499083, Median: 0.25973822650379297, Num: 111
Epoch: 218, Loss: 56.192352225981566, Learning Rate: 0.0005
Epoch: 219, Loss: 60.818494268210536, Learning Rate: 0.0005
Mean: 0.2676340424889108, Median: 0.2648254664745931, Num: 111
Epoch: 220, Loss: 62.33953126654568, Learning Rate: 0.0005
Epoch: 221, Loss: 63.53062497564109, Learning Rate: 0.0005
Mean: 0.2691286247019896, Median: 0.2705823716891021, Num: 111
Epoch: 222, Loss: 63.00604173361537, Learning Rate: 0.0005
Epoch: 223, Loss: 63.51041642154556, Learning Rate: 0.0005
Mean: 0.2698857619220951, Median: 0.2675263895572801, Num: 111
Epoch: 224, Loss: 65.39820039128682, Learning Rate: 0.0005
Epoch: 225, Loss: 62.17972255614867, Learning Rate: 0.0005
Mean: 0.2665290093460921, Median: 0.26062709754736213, Num: 111
Epoch: 226, Loss: 68.53808570769895, Learning Rate: 0.0005
Epoch: 227, Loss: 57.442795696028746, Learning Rate: 0.0005
Mean: 0.25926897952634714, Median: 0.24998464125047695, Num: 111
Epoch: 228, Loss: 66.86425562939012, Learning Rate: 0.0005
Epoch: 229, Loss: 53.24784189247223, Learning Rate: 0.0005
Mean: 0.2648500572593416, Median: 0.2704735467357444, Num: 111
Epoch: 230, Loss: 61.681834760918676, Learning Rate: 0.0005
Epoch: 231, Loss: 55.59165573120117, Learning Rate: 0.0005
Mean: 0.26638125250902095, Median: 0.25923596180784036, Num: 111
Epoch: 232, Loss: 66.60765117047781, Learning Rate: 0.0005
Epoch: 233, Loss: 58.731691613254775, Learning Rate: 0.0005
Mean: 0.26730670302084447, Median: 0.26931874488754654, Num: 111
Epoch: 234, Loss: 60.297379872885095, Learning Rate: 0.0005
Epoch: 235, Loss: 60.96650495873877, Learning Rate: 0.0005
Mean: 0.26973782477109987, Median: 0.2695948940320421, Num: 111
Epoch: 236, Loss: 51.29281202569065, Learning Rate: 0.0005
Epoch: 237, Loss: 58.27221555594938, Learning Rate: 0.0005
Mean: 0.26403206944274976, Median: 0.26106290085686157, Num: 111
Epoch: 238, Loss: 64.17115126460432, Learning Rate: 0.0005
Epoch: 239, Loss: 57.95617023146296, Learning Rate: 0.0005
Mean: 0.2661594772418276, Median: 0.2506305150890356, Num: 111
Epoch: 240, Loss: 51.710764115115246, Learning Rate: 0.0005
Epoch: 241, Loss: 63.782616982977075, Learning Rate: 0.0005
Mean: 0.2634099672888111, Median: 0.2607011547310797, Num: 111
Epoch: 242, Loss: 62.09928692392556, Learning Rate: 0.0005
Epoch: 243, Loss: 55.07034100681902, Learning Rate: 0.0005
Mean: 0.2644739951682343, Median: 0.26132464000889194, Num: 111
Epoch: 244, Loss: 55.38941880881068, Learning Rate: 0.0005
Epoch: 245, Loss: 63.16245509917478, Learning Rate: 0.0005
Mean: 0.2663695771421858, Median: 0.2799249158999349, Num: 111
Epoch: 246, Loss: 57.82780881674893, Learning Rate: 0.0005
Epoch: 247, Loss: 65.64470075124717, Learning Rate: 0.0005
Mean: 0.271327263099799, Median: 0.2745687586325584, Num: 111
Epoch: 248, Loss: 57.24144442684679, Learning Rate: 0.0005
Epoch: 249, Loss: 64.32981654247605, Learning Rate: 0.0005
Mean: 0.26040183941624784, Median: 0.23881105272361358, Num: 111
Epoch: 250, Loss: 59.34781225043607, Learning Rate: 0.0005
Epoch: 251, Loss: 60.0601909591491, Learning Rate: 0.0005
Mean: 0.2645297287199363, Median: 0.24756102574310307, Num: 111
Epoch: 252, Loss: 62.61838336163257, Learning Rate: 0.0005
Epoch: 253, Loss: 70.73206205253142, Learning Rate: 0.0005
Mean: 0.26192184631924004, Median: 0.24995598035755953, Num: 111
Epoch: 254, Loss: 63.98032045938883, Learning Rate: 0.0005
Epoch: 255, Loss: 60.019433814359, Learning Rate: 0.0005
Mean: 0.26475758012135336, Median: 0.26899777403896374, Num: 111
Epoch: 256, Loss: 58.16115717141025, Learning Rate: 0.0005
Epoch: 257, Loss: 53.85380595563406, Learning Rate: 0.0005
Mean: 0.2693964629937347, Median: 0.2710504648632921, Num: 111
Epoch: 258, Loss: 62.93755194652511, Learning Rate: 0.0005
Epoch: 259, Loss: 59.7757210214454, Learning Rate: 0.0005
Mean: 0.26615989995679035, Median: 0.26072514419161874, Num: 111
Epoch: 260, Loss: 56.96640297303717, Learning Rate: 0.0005
Epoch: 261, Loss: 63.324246550180824, Learning Rate: 0.0005
Mean: 0.2657376104931236, Median: 0.26471676023904445, Num: 111
Epoch: 262, Loss: 64.44695855336018, Learning Rate: 0.0005
Epoch: 263, Loss: 55.09755328764398, Learning Rate: 0.0005
Mean: 0.2675444954509861, Median: 0.2673427786356531, Num: 111
Epoch: 264, Loss: 63.28568566563618, Learning Rate: 0.0005
Epoch: 265, Loss: 57.64992231346039, Learning Rate: 0.0005
Mean: 0.2638067595375803, Median: 0.25179890218516915, Num: 111
Epoch: 266, Loss: 56.226823163319786, Learning Rate: 0.0005
Epoch: 267, Loss: 65.13232804493732, Learning Rate: 0.0005
Mean: 0.26919726200548444, Median: 0.28137004428010026, Num: 111
Epoch: 268, Loss: 64.37526620152485, Learning Rate: 0.0005
Epoch: 269, Loss: 57.84694171813597, Learning Rate: 0.0005
Mean: 0.2678522066128728, Median: 0.2719033339138553, Num: 111
Epoch: 270, Loss: 57.168005334325585, Learning Rate: 0.0005
Epoch: 271, Loss: 58.20752239227295, Learning Rate: 0.0005
Mean: 0.2705972473397064, Median: 0.2648394724883998, Num: 111
Epoch: 272, Loss: 66.3553930305573, Learning Rate: 0.0005
Epoch: 273, Loss: 60.38306949799319, Learning Rate: 0.0005
Mean: 0.2641715069613782, Median: 0.2484692158423054, Num: 111
Epoch: 274, Loss: 64.03409043277603, Learning Rate: 0.0005
Epoch: 275, Loss: 57.93131695023502, Learning Rate: 0.0005
Mean: 0.2718670280071849, Median: 0.26669462329567634, Num: 111
Epoch: 276, Loss: 58.97711484977998, Learning Rate: 0.0005
Epoch: 277, Loss: 62.27508940179664, Learning Rate: 0.0005
Mean: 0.2742109811738083, Median: 0.2848204845697954, Num: 111
Epoch: 278, Loss: 64.2857531696917, Learning Rate: 0.0005
Epoch: 279, Loss: 56.83454694518124, Learning Rate: 0.0005
Mean: 0.2713695901403387, Median: 0.25666901290465094, Num: 111
Epoch: 280, Loss: 64.43155203669905, Learning Rate: 0.0005
Epoch: 281, Loss: 68.83882603013372, Learning Rate: 0.0005
Mean: 0.269065512427715, Median: 0.2748027984056598, Num: 111
Epoch: 282, Loss: 61.485197837094226, Learning Rate: 0.0005
Epoch: 283, Loss: 54.85632620662092, Learning Rate: 0.0005
Mean: 0.2736380368250455, Median: 0.25670572682666803, Num: 111
Epoch: 284, Loss: 61.90603444662439, Learning Rate: 0.0005
Epoch: 285, Loss: 62.82876878761383, Learning Rate: 0.0005
Mean: 0.26886267488431304, Median: 0.2541624087872692, Num: 111
Epoch: 286, Loss: 53.58342947442848, Learning Rate: 0.0005
Epoch: 287, Loss: 67.27073108719055, Learning Rate: 0.0005
Mean: 0.26951802210211506, Median: 0.26777819525919294, Num: 111
Epoch: 288, Loss: 64.1915349041123, Learning Rate: 0.0005
Epoch: 289, Loss: 60.95701218800372, Learning Rate: 0.0005
Mean: 0.2673867346486483, Median: 0.25673603672721756, Num: 111
Epoch: 290, Loss: 57.67387686580061, Learning Rate: 0.0005
Epoch: 291, Loss: 60.235040182090664, Learning Rate: 0.0005
Mean: 0.2772177702818726, Median: 0.2668716482042798, Num: 111
Epoch: 292, Loss: 57.27348709106445, Learning Rate: 0.0005
Epoch: 293, Loss: 54.72243012003152, Learning Rate: 0.0005
Mean: 0.27001908596196045, Median: 0.26175572623789606, Num: 111
Epoch: 294, Loss: 58.03725423008562, Learning Rate: 0.0005
Epoch: 295, Loss: 61.7345559740641, Learning Rate: 0.0005
Mean: 0.2686162832422444, Median: 0.25534989576396866, Num: 111
Epoch: 296, Loss: 63.828798638768944, Learning Rate: 0.0005
Epoch: 297, Loss: 57.56792058714901, Learning Rate: 0.0005
Mean: 0.27172082924572705, Median: 0.26073590821921905, Num: 111
Epoch: 298, Loss: 60.80597466158579, Learning Rate: 0.0005
Epoch: 299, Loss: 60.641900568123326, Learning Rate: 0.0005
Mean: 0.26774799519035153, Median: 0.2628898193429109, Num: 111
Epoch: 300, Loss: 57.0908489686897, Learning Rate: 0.0005
Epoch: 301, Loss: 63.33425179447036, Learning Rate: 0.0005
Mean: 0.27518589130507, Median: 0.2742181420193274, Num: 111
Epoch: 302, Loss: 57.320002860333545, Learning Rate: 0.0005
Epoch: 303, Loss: 59.322967529296875, Learning Rate: 0.0005
Mean: 0.2687002988906761, Median: 0.26774162037618504, Num: 111
Epoch: 304, Loss: 62.593100530555446, Learning Rate: 0.0005
Epoch: 305, Loss: 59.71342579715223, Learning Rate: 0.0005
Mean: 0.271478013469101, Median: 0.2744673581086581, Num: 111
Epoch: 306, Loss: 57.993157455720095, Learning Rate: 0.0005
Epoch: 307, Loss: 59.72814350817577, Learning Rate: 0.0005
Mean: 0.27482764901736617, Median: 0.27915697759095065, Num: 111
Epoch: 308, Loss: 58.497095263147926, Learning Rate: 0.0005
Epoch: 309, Loss: 55.179128060857934, Learning Rate: 0.0005
Mean: 0.2634815460687663, Median: 0.2606762340974112, Num: 111
Epoch: 310, Loss: 66.28173669562283, Learning Rate: 0.0005
Epoch: 311, Loss: 58.139800623238806, Learning Rate: 0.0005
Mean: 0.272671345276809, Median: 0.26967956455602266, Num: 111
Epoch: 312, Loss: 58.49624530378595, Learning Rate: 0.0005
Epoch: 313, Loss: 58.12532066437135, Learning Rate: 0.0005
Mean: 0.26927232378885324, Median: 0.2798586323271284, Num: 111
Epoch: 314, Loss: 62.33197698248438, Learning Rate: 0.0005
Epoch: 315, Loss: 53.61262863802622, Learning Rate: 0.0005
Mean: 0.2645624988761754, Median: 0.2455487309269719, Num: 111
Epoch: 316, Loss: 55.478081875536816, Learning Rate: 0.0005
Epoch: 317, Loss: 61.869325080549864, Learning Rate: 0.0005
Mean: 0.2727839013798825, Median: 0.2873709491682603, Num: 111
Epoch: 318, Loss: 59.74941081311329, Learning Rate: 0.0005
Epoch: 319, Loss: 58.56208665686918, Learning Rate: 0.0005
Mean: 0.270537486050337, Median: 0.26700652274158837, Num: 111
Epoch: 320, Loss: 59.821086826094664, Learning Rate: 0.0005
Epoch: 321, Loss: 63.15895755032459, Learning Rate: 0.0005
Mean: 0.26900817210921874, Median: 0.2569370735875138, Num: 111
Epoch: 322, Loss: 60.614986764379296, Learning Rate: 0.0005
Epoch: 323, Loss: 60.516762917300305, Learning Rate: 0.0005
Mean: 0.27130836662375796, Median: 0.26881563776109074, Num: 111
Epoch: 324, Loss: 51.151356576436974, Learning Rate: 0.0005
Epoch: 325, Loss: 59.409431727535754, Learning Rate: 0.0005
Mean: 0.26813167269816496, Median: 0.2575882220215512, Num: 111
Epoch: 326, Loss: 55.85447545798428, Learning Rate: 0.0005
Epoch: 327, Loss: 56.3698945217822, Learning Rate: 0.0005
Mean: 0.2772911205809035, Median: 0.2732724138325768, Num: 111
Epoch: 328, Loss: 64.36818198698113, Learning Rate: 0.0005
Epoch: 329, Loss: 58.94162145867405, Learning Rate: 0.0005
Mean: 0.26657162415143437, Median: 0.25723190567965587, Num: 111
Epoch: 330, Loss: 56.369588817458556, Learning Rate: 0.0005
Epoch: 331, Loss: 63.24032271051981, Learning Rate: 0.0005
Mean: 0.2701017971215204, Median: 0.25832999507613114, Num: 111
Epoch: 332, Loss: 59.55610899178379, Learning Rate: 0.0005
Epoch: 333, Loss: 58.83735923307488, Learning Rate: 0.0005
Mean: 0.2664749090777058, Median: 0.264416005590945, Num: 111
Epoch: 334, Loss: 62.019270759030995, Learning Rate: 0.0005
Epoch: 335, Loss: 56.59044270343091, Learning Rate: 0.0005
Mean: 0.27047793725630737, Median: 0.2578616271820645, Num: 111
Epoch: 336, Loss: 52.45472477143069, Learning Rate: 0.0005
Epoch: 337, Loss: 59.2279368251203, Learning Rate: 0.0005
Mean: 0.2729714438051729, Median: 0.26562811679786225, Num: 111
Epoch: 338, Loss: 60.92894724190953, Learning Rate: 0.0005
Epoch: 339, Loss: 52.45868789718812, Learning Rate: 0.0005
Mean: 0.2745286851210028, Median: 0.2646971462392772, Num: 111
Epoch: 340, Loss: 62.188315288130056, Learning Rate: 0.0005
Epoch: 341, Loss: 56.61260554876672, Learning Rate: 0.0005
Mean: 0.27155099624328427, Median: 0.27090662547053046, Num: 111
Epoch: 342, Loss: 56.03539368043463, Learning Rate: 0.0005
Epoch: 343, Loss: 57.63368068832949, Learning Rate: 0.0005
Mean: 0.27634403995522816, Median: 0.28554682904654616, Num: 111
Epoch: 344, Loss: 55.210312372230625, Learning Rate: 0.0005
Epoch: 345, Loss: 58.52636832501515, Learning Rate: 0.0005
Mean: 0.2685095547045439, Median: 0.267181709266287, Num: 111
Epoch: 346, Loss: 62.832019484186745, Learning Rate: 0.0005
Epoch: 347, Loss: 55.50802353204015, Learning Rate: 0.0005
Mean: 0.2742397103229911, Median: 0.26551278627198144, Num: 111
Epoch: 348, Loss: 61.70984854181129, Learning Rate: 0.0005
Epoch: 349, Loss: 64.06874532584685, Learning Rate: 0.0005
Mean: 0.2711359598838822, Median: 0.2659236688712949, Num: 111
Epoch: 350, Loss: 59.06406751885471, Learning Rate: 0.0005
Epoch: 351, Loss: 54.556648116513905, Learning Rate: 0.0005
Mean: 0.26841672199425476, Median: 0.2615362252501322, Num: 111
Epoch: 352, Loss: 57.8289794921875, Learning Rate: 0.0005
Epoch: 353, Loss: 56.704939049410534, Learning Rate: 0.0005
Mean: 0.27376560304840936, Median: 0.29123067409539666, Num: 111
Epoch: 354, Loss: 53.97409682676017, Learning Rate: 0.0005
Epoch: 355, Loss: 59.126614972769495, Learning Rate: 0.0005
Mean: 0.2733007070202878, Median: 0.26757526814739196, Num: 111
Epoch: 356, Loss: 57.96896925317236, Learning Rate: 0.0005
Epoch: 357, Loss: 63.21335545505386, Learning Rate: 0.0005
Mean: 0.27664220940876316, Median: 0.2644556585407874, Num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
Epoch: 358, Loss: 58.586422621485696, Learning Rate: 0.0005
Epoch: 359, Loss: 55.609525301370276, Learning Rate: 0.0005
Mean: 0.27010930372278075, Median: 0.26675487434006445, Num: 111
Epoch: 360, Loss: 54.15536954029497, Learning Rate: 0.0005
Epoch: 361, Loss: 57.724789240274085, Learning Rate: 0.0005
Mean: 0.2719270549566606, Median: 0.2652081190336658, Num: 111
Epoch: 362, Loss: 58.33718260799546, Learning Rate: 0.0005
Epoch: 363, Loss: 58.65693671444812, Learning Rate: 0.0005
Mean: 0.2745839510749978, Median: 0.252387504669229, Num: 111
Epoch: 364, Loss: 64.0553837109761, Learning Rate: 0.0005
Epoch: 365, Loss: 67.20022751911577, Learning Rate: 0.0005
Mean: 0.2805565372368211, Median: 0.27368786730418226, Num: 111
Epoch: 366, Loss: 57.076445728899486, Learning Rate: 0.0005
Epoch: 367, Loss: 59.63953991970384, Learning Rate: 0.0005
Mean: 0.2743605929161331, Median: 0.26329942932904665, Num: 111
Epoch: 368, Loss: 61.06059816659215, Learning Rate: 0.0005
Epoch: 369, Loss: 57.038514441754444, Learning Rate: 0.0005
Mean: 0.2732909388603651, Median: 0.2725173806548494, Num: 111
Epoch: 370, Loss: 54.78043066737163, Learning Rate: 0.0005
Epoch: 371, Loss: 61.357032936739635, Learning Rate: 0.0005
Mean: 0.2807907667348714, Median: 0.27373506537330283, Num: 111
Epoch: 372, Loss: 57.47015254468803, Learning Rate: 0.0005
Epoch: 373, Loss: 58.93440450530454, Learning Rate: 0.0005
Mean: 0.2773850603015798, Median: 0.2799048447887989, Num: 111
Epoch: 374, Loss: 48.033099036618886, Learning Rate: 0.0005
Epoch: 375, Loss: 60.737814248326316, Learning Rate: 0.0005
Mean: 0.2718200030823115, Median: 0.25705048727882973, Num: 111
Epoch: 376, Loss: 53.128327059458535, Learning Rate: 0.0005
Epoch: 377, Loss: 62.455729312207325, Learning Rate: 0.0005
Mean: 0.27196012239954004, Median: 0.27798509312338693, Num: 111
Epoch: 378, Loss: 63.475984378033374, Learning Rate: 0.0005
Epoch: 379, Loss: 64.19132673884013, Learning Rate: 0.0005
Mean: 0.27696973894726595, Median: 0.27372231084248977, Num: 111
Epoch: 380, Loss: 55.10432774187571, Learning Rate: 0.0005
Epoch: 381, Loss: 56.831393184432066, Learning Rate: 0.0005
Mean: 0.2695331898337917, Median: 0.2511410487816439, Num: 111
Epoch: 382, Loss: 60.642534899424355, Learning Rate: 0.0005
Epoch: 383, Loss: 62.83159534040704, Learning Rate: 0.0005
Mean: 0.2755095734276977, Median: 0.26889856917239646, Num: 111
Epoch: 384, Loss: 58.26492213053876, Learning Rate: 0.0005
Epoch: 385, Loss: 61.59411940517196, Learning Rate: 0.0005
Mean: 0.2780611545707901, Median: 0.26294838125880454, Num: 111
Epoch: 386, Loss: 65.81311878526067, Learning Rate: 0.0005
Epoch: 387, Loss: 61.07340029061559, Learning Rate: 0.0005
Mean: 0.27201922951854635, Median: 0.2641961132681022, Num: 111
Epoch: 388, Loss: 67.60484890765454, Learning Rate: 0.0005
Epoch: 389, Loss: 57.48063432739442, Learning Rate: 0.0005
Mean: 0.27264353227710536, Median: 0.27456148714625667, Num: 111
Epoch: 390, Loss: 58.290004534893725, Learning Rate: 0.0005
Epoch: 391, Loss: 62.540611267089844, Learning Rate: 0.0005
Mean: 0.27276429702992566, Median: 0.2717097047031109, Num: 111
Epoch: 392, Loss: 59.145337782710435, Learning Rate: 0.0005
Epoch: 393, Loss: 54.19798474139478, Learning Rate: 0.0005
Mean: 0.27423996941550843, Median: 0.2849396715020962, Num: 111
Epoch: 394, Loss: 60.955899227096374, Learning Rate: 0.0005
Epoch: 395, Loss: 54.8849031609225, Learning Rate: 0.0005
Mean: 0.27756336866563225, Median: 0.26983418698567374, Num: 111
Epoch: 396, Loss: 67.61223563228745, Learning Rate: 0.0005
Epoch: 397, Loss: 56.46031950180789, Learning Rate: 0.0005
Mean: 0.2765578768775464, Median: 0.2593770308730654, Num: 111
Epoch: 398, Loss: 50.95268134036696, Learning Rate: 0.0005
Epoch: 399, Loss: 56.33604471367526, Learning Rate: 0.0005
Mean: 0.28073076152511145, Median: 0.2751938800492099, Num: 111
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
/home/warmachine/Desktop/Code_for_publication/deep_ensemble/deep_ensemble_Graph.py:145: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/aten/src/ATen/native/TensorShape.cpp:3697.)
  log_likelihood = -0.5 * torch.matmul(y.T, torch.matmul(inverse_mat, y))
num_encoder: 2, methods: GraphGCN, dataset: fsmol, NCL: False
training tasks num: 4938, valid tasks num: 40, test tasks num: 111
10
Epoch: 0, Loss: 240.59131622314453, Learning Rate: 0.0005
Epoch: 1, Loss: 192.07928016386836, Learning Rate: 0.0005
Mean: 0.060911052025546565, Median: 0.01586155834699193, Num: 111
Epoch: 2, Loss: 173.66532576227763, Learning Rate: 0.0005
Epoch: 3, Loss: 172.58688101711044, Learning Rate: 0.0005
Mean: 0.09127935086144587, Median: 0.03847764508270179, Num: 111
Epoch: 4, Loss: 156.49693385664239, Learning Rate: 0.0005
Epoch: 5, Loss: 157.30777896743223, Learning Rate: 0.0005
Mean: 0.10790130899212377, Median: 0.058998144468235945, Num: 111
Epoch: 6, Loss: 152.81858664822866, Learning Rate: 0.0005
Epoch: 7, Loss: 149.007005622588, Learning Rate: 0.0005
Mean: 0.1279801276477546, Median: 0.08176104585651472, Num: 111
Epoch: 8, Loss: 131.9304077654, Learning Rate: 0.0005
Epoch: 9, Loss: 140.37755589312818, Learning Rate: 0.0005
Mean: 0.13807471908072863, Median: 0.09746323974969157, Num: 111
Epoch: 10, Loss: 132.8151402301099, Learning Rate: 0.0005
Epoch: 11, Loss: 135.46200947589185, Learning Rate: 0.0005
Mean: 0.15603715871432475, Median: 0.12298485497868084, Num: 111
Epoch: 12, Loss: 132.98116477139024, Learning Rate: 0.0005
Epoch: 13, Loss: 126.794588916273, Learning Rate: 0.0005
Mean: 0.15195684906139273, Median: 0.1252164705342945, Num: 111
Epoch: 14, Loss: 130.0249143623444, Learning Rate: 0.0005
Epoch: 15, Loss: 121.64545270621059, Learning Rate: 0.0005
Mean: 0.17579175421676732, Median: 0.14815628597169359, Num: 111
Epoch: 16, Loss: 123.40579848691641, Learning Rate: 0.0005
Epoch: 17, Loss: 119.96591324404062, Learning Rate: 0.0005
Mean: 0.18480921762483346, Median: 0.16278747524403903, Num: 111
Epoch: 18, Loss: 124.8968941791948, Learning Rate: 0.0005
Epoch: 19, Loss: 110.28711305181665, Learning Rate: 0.0005
Mean: 0.18799438247962738, Median: 0.150786397961604, Num: 111
Epoch: 20, Loss: 112.22053778315166, Learning Rate: 0.0005
Epoch: 21, Loss: 106.95758353083967, Learning Rate: 0.0005
Mean: 0.19550094950523797, Median: 0.15361097445713948, Num: 111
Epoch: 22, Loss: 113.05388795324119, Learning Rate: 0.0005
Epoch: 23, Loss: 108.86958386524614, Learning Rate: 0.0005
Mean: 0.19538386343015612, Median: 0.16922182485767948, Num: 111
Epoch: 24, Loss: 108.35531827627895, Learning Rate: 0.0005
Epoch: 25, Loss: 105.16636774913374, Learning Rate: 0.0005
Mean: 0.19267670339809545, Median: 0.17629322968122235, Num: 111
Epoch: 26, Loss: 103.13346796150667, Learning Rate: 0.0005
Epoch: 27, Loss: 102.7003838872335, Learning Rate: 0.0005
Mean: 0.19747270045564488, Median: 0.17570856050508424, Num: 111
Epoch: 28, Loss: 104.28586872514472, Learning Rate: 0.0005
Epoch: 29, Loss: 108.89634654033615, Learning Rate: 0.0005
Mean: 0.20012205472718986, Median: 0.17904635617632053, Num: 111
Epoch: 30, Loss: 99.77447247792439, Learning Rate: 0.0005
Epoch: 31, Loss: 100.61177563954548, Learning Rate: 0.0005
Mean: 0.20123944188662107, Median: 0.18996768781994555, Num: 111
Epoch: 32, Loss: 97.07689138205654, Learning Rate: 0.0005
Epoch: 33, Loss: 98.45267380863787, Learning Rate: 0.0005
Mean: 0.20453318690115893, Median: 0.19627187041106373, Num: 111
Epoch: 34, Loss: 95.72735122312983, Learning Rate: 0.0005
Epoch: 35, Loss: 91.76792905416833, Learning Rate: 0.0005
Mean: 0.2062311057096222, Median: 0.1955327132324259, Num: 111
Epoch: 36, Loss: 103.52751035575407, Learning Rate: 0.0005
Epoch: 37, Loss: 93.72082781504436, Learning Rate: 0.0005
Mean: 0.20054206251182902, Median: 0.1867707696568282, Num: 111
Epoch: 38, Loss: 89.83740133262542, Learning Rate: 0.0005
Epoch: 39, Loss: 92.2225788530097, Learning Rate: 0.0005
Mean: 0.21485601177647778, Median: 0.20480223473285528, Num: 111
Epoch: 40, Loss: 100.45983555805252, Learning Rate: 0.0005
Epoch: 41, Loss: 97.66286328327224, Learning Rate: 0.0005
Mean: 0.21483270579812275, Median: 0.19615798521595726, Num: 111
Epoch: 42, Loss: 90.42029010818665, Learning Rate: 0.0005
Epoch: 43, Loss: 88.59013077149908, Learning Rate: 0.0005
Mean: 0.21961566743838395, Median: 0.1939027026197183, Num: 111
Epoch: 44, Loss: 92.13311960610999, Learning Rate: 0.0005
Epoch: 45, Loss: 85.71375348194536, Learning Rate: 0.0005
Mean: 0.21667620097859575, Median: 0.21070142201581385, Num: 111
Epoch: 46, Loss: 88.22588272554329, Learning Rate: 0.0005
Epoch: 47, Loss: 87.30133920692536, Learning Rate: 0.0005
Mean: 0.21620155472861946, Median: 0.19898440413305438, Num: 111
Epoch: 48, Loss: 90.41116912106433, Learning Rate: 0.0005
Epoch: 49, Loss: 90.28070285521358, Learning Rate: 0.0005
Mean: 0.22516864294879288, Median: 0.20904833241319842, Num: 111
Epoch: 50, Loss: 90.70265542455466, Learning Rate: 0.0005
Epoch: 51, Loss: 87.99097745964326, Learning Rate: 0.0005
Mean: 0.22414278066200427, Median: 0.2243904286514157, Num: 111
Epoch: 52, Loss: 91.35509206013508, Learning Rate: 0.0005
Epoch: 53, Loss: 81.59431760856904, Learning Rate: 0.0005
Mean: 0.2301676216700511, Median: 0.21074243790942457, Num: 111
Epoch: 54, Loss: 84.68477331873882, Learning Rate: 0.0005
Epoch: 55, Loss: 88.92155677151968, Learning Rate: 0.0005
Mean: 0.22796048767929977, Median: 0.21271422718866614, Num: 111
Epoch: 56, Loss: 81.9840581043657, Learning Rate: 0.0005
Epoch: 57, Loss: 85.58619902507368, Learning Rate: 0.0005
Mean: 0.2316248546588065, Median: 0.2277486804566787, Num: 111
Epoch: 58, Loss: 85.67308506333684, Learning Rate: 0.0005
Epoch: 59, Loss: 89.6404490643237, Learning Rate: 0.0005
Mean: 0.2172639063063414, Median: 0.21298647186822356, Num: 111
Epoch: 60, Loss: 84.10712772966868, Learning Rate: 0.0005
Epoch: 61, Loss: 82.80791353892131, Learning Rate: 0.0005
Mean: 0.23248228317714495, Median: 0.2351439350143981, Num: 111
Epoch: 62, Loss: 86.97172293605574, Learning Rate: 0.0005
Epoch: 63, Loss: 74.30137965190842, Learning Rate: 0.0005
Mean: 0.2401804565370767, Median: 0.21665840553819862, Num: 111
Epoch: 64, Loss: 82.40466777387871, Learning Rate: 0.0005
Epoch: 65, Loss: 79.17119182448789, Learning Rate: 0.0005
Mean: 0.2319623991969599, Median: 0.22524959346640414, Num: 111
Epoch: 66, Loss: 75.43240441471697, Learning Rate: 0.0005
Epoch: 67, Loss: 78.96296079474759, Learning Rate: 0.0005
Mean: 0.2324656910961063, Median: 0.22658810925731826, Num: 111
Epoch: 68, Loss: 75.25859324903374, Learning Rate: 0.0005
Epoch: 69, Loss: 85.95605029830013, Learning Rate: 0.0005
Mean: 0.23789083053213508, Median: 0.23758067044724984, Num: 111
Epoch: 70, Loss: 78.76825757773526, Learning Rate: 0.0005
Epoch: 71, Loss: 81.27451301482786, Learning Rate: 0.0005
Mean: 0.24370443253115015, Median: 0.23667206798628337, Num: 111
Epoch: 72, Loss: 79.73818997302688, Learning Rate: 0.0005
Epoch: 73, Loss: 77.5867151696998, Learning Rate: 0.0005
Mean: 0.23791477332560487, Median: 0.2334783027353706, Num: 111
Epoch: 74, Loss: 84.0793565956943, Learning Rate: 0.0005
Epoch: 75, Loss: 82.24760073926075, Learning Rate: 0.0005
Mean: 0.23834326827899135, Median: 0.2416036405718831, Num: 111
Epoch: 76, Loss: 78.01513701749136, Learning Rate: 0.0005
Epoch: 77, Loss: 81.26128989529897, Learning Rate: 0.0005
Mean: 0.2376123300829131, Median: 0.23237385430446492, Num: 111
Epoch: 78, Loss: 77.51906193882586, Learning Rate: 0.0005
Epoch: 79, Loss: 75.40345628577542, Learning Rate: 0.0005
Mean: 0.237886181910019, Median: 0.2484269978037945, Num: 111
Epoch: 80, Loss: 72.9588003503271, Learning Rate: 0.0005
Epoch: 81, Loss: 72.15823373449854, Learning Rate: 0.0005
Mean: 0.23922349872627457, Median: 0.23522726996392962, Num: 111
Epoch: 82, Loss: 74.05745085750718, Learning Rate: 0.0005
Epoch: 83, Loss: 74.12677348952695, Learning Rate: 0.0005
Mean: 0.2421856419377925, Median: 0.24073931850515412, Num: 111
Epoch: 84, Loss: 71.36600191047393, Learning Rate: 0.0005
Epoch: 85, Loss: 67.1699837834002, Learning Rate: 0.0005
Mean: 0.24327141763458363, Median: 0.25213474350658727, Num: 111
Epoch: 86, Loss: 78.0683135526726, Learning Rate: 0.0005
Epoch: 87, Loss: 91.81707780906953, Learning Rate: 0.0005
Mean: 0.24524710245413947, Median: 0.26270996335662955, Num: 111
Epoch: 88, Loss: 68.45559368363347, Learning Rate: 0.0005
Epoch: 89, Loss: 74.72035104682647, Learning Rate: 0.0005
Mean: 0.2459080070335692, Median: 0.2448299747853235, Num: 111
Epoch: 90, Loss: 68.69500254435711, Learning Rate: 0.0005
Epoch: 91, Loss: 62.398978911250474, Learning Rate: 0.0005
Mean: 0.24836962425897927, Median: 0.24180852149895696, Num: 111
Epoch: 92, Loss: 68.26621639297669, Learning Rate: 0.0005
Epoch: 93, Loss: 76.04437425912144, Learning Rate: 0.0005
Mean: 0.2488465920587423, Median: 0.25071723962561027, Num: 111
Epoch: 94, Loss: 67.37692350364593, Learning Rate: 0.0005
Epoch: 95, Loss: 64.64070078838303, Learning Rate: 0.0005
Mean: 0.2455006459223427, Median: 0.22131880004091953, Num: 111
Epoch: 96, Loss: 70.61194138354566, Learning Rate: 0.0005
Epoch: 97, Loss: 74.69519746159932, Learning Rate: 0.0005
Mean: 0.2496452495968815, Median: 0.2628463924607494, Num: 111
Epoch: 98, Loss: 71.5439146455512, Learning Rate: 0.0005
Epoch: 99, Loss: 72.87036845195725, Learning Rate: 0.0005
Mean: 0.25190284904805654, Median: 0.273344007831398, Num: 111
Epoch: 100, Loss: 74.29883615654636, Learning Rate: 0.0005
Epoch: 101, Loss: 63.49372018101704, Learning Rate: 0.0005
Mean: 0.24506779167577172, Median: 0.22928089691915263, Num: 111
Epoch: 102, Loss: 76.25924090879509, Learning Rate: 0.0005
Epoch: 103, Loss: 72.02952998517507, Learning Rate: 0.0005
Mean: 0.2517712630831876, Median: 0.24044051575042388, Num: 111
Epoch: 104, Loss: 68.78967319626406, Learning Rate: 0.0005
Epoch: 105, Loss: 64.37226923977036, Learning Rate: 0.0005
Mean: 0.24565796251790492, Median: 0.2429323475630745, Num: 111
Epoch: 106, Loss: 71.82764508350786, Learning Rate: 0.0005
Epoch: 107, Loss: 66.69221390873552, Learning Rate: 0.0005
Mean: 0.24967215915352445, Median: 0.25129117892866243, Num: 111
Epoch: 108, Loss: 69.29956392495029, Learning Rate: 0.0005
Epoch: 109, Loss: 70.32699837742082, Learning Rate: 0.0005
Mean: 0.247127339120571, Median: 0.2338079129502111, Num: 111
Epoch: 110, Loss: 59.883980222495204, Learning Rate: 0.0005
Epoch: 111, Loss: 64.79210228517832, Learning Rate: 0.0005
Mean: 0.25174275434522164, Median: 0.25889637367059853, Num: 111
Epoch: 112, Loss: 65.63774902849312, Learning Rate: 0.0005
Epoch: 113, Loss: 67.94029662120774, Learning Rate: 0.0005
Mean: 0.2501640243654886, Median: 0.24324030855646006, Num: 111
Epoch: 114, Loss: 65.26098198488535, Learning Rate: 0.0005
Epoch: 115, Loss: 73.7534362838929, Learning Rate: 0.0005
Mean: 0.25684186170531365, Median: 0.2496355157693344, Num: 111
Epoch: 116, Loss: 65.51804413852922, Learning Rate: 0.0005
Epoch: 117, Loss: 65.00726072472263, Learning Rate: 0.0005
Mean: 0.2526388829467851, Median: 0.2502569162689854, Num: 111
Epoch: 118, Loss: 73.70689909142185, Learning Rate: 0.0005
Epoch: 119, Loss: 73.96156368485417, Learning Rate: 0.0005
Mean: 0.24877640370382897, Median: 0.23660331893071748, Num: 111
Epoch: 120, Loss: 61.35959817128009, Learning Rate: 0.0005
Epoch: 121, Loss: 78.7920388486012, Learning Rate: 0.0005
Mean: 0.24562240996878382, Median: 0.23715769994430497, Num: 111
Epoch: 122, Loss: 71.98532387147466, Learning Rate: 0.0005
Epoch: 123, Loss: 63.65490051637213, Learning Rate: 0.0005
Mean: 0.25471869377541867, Median: 0.2363398530717039, Num: 111
Epoch: 124, Loss: 71.35307852044163, Learning Rate: 0.0005
Epoch: 125, Loss: 61.751043526523084, Learning Rate: 0.0005
Mean: 0.24920215713203464, Median: 0.21483689274670137, Num: 111
Epoch: 126, Loss: 68.21542358398438, Learning Rate: 0.0005
Epoch: 127, Loss: 66.54971645539065, Learning Rate: 0.0005
Mean: 0.2533923887936143, Median: 0.24105327413411715, Num: 111
Epoch: 128, Loss: 67.21130407861916, Learning Rate: 0.0005
Epoch: 129, Loss: 70.02153348348227, Learning Rate: 0.0005
Mean: 0.25387756053604793, Median: 0.2358100027355907, Num: 111
Epoch: 130, Loss: 68.58615871797124, Learning Rate: 0.0005
Epoch: 131, Loss: 60.232267632541884, Learning Rate: 0.0005
Mean: 0.25674803052606676, Median: 0.2518591696265346, Num: 111
Epoch: 132, Loss: 69.26053784841514, Learning Rate: 0.0005
Epoch: 133, Loss: 60.156822779092444, Learning Rate: 0.0005
Mean: 0.25604134373710974, Median: 0.2428442642829357, Num: 111
Epoch: 134, Loss: 63.838502412819004, Learning Rate: 0.0005
Epoch: 135, Loss: 64.24486812913274, Learning Rate: 0.0005
Mean: 0.25493114082416385, Median: 0.25878393124037086, Num: 111
Epoch: 136, Loss: 63.5976338673787, Learning Rate: 0.0005
Epoch: 137, Loss: 75.78370920434055, Learning Rate: 0.0005
Mean: 0.25520721273330627, Median: 0.2442916197667823, Num: 111
Epoch: 138, Loss: 62.71541944756565, Learning Rate: 0.0005
Epoch: 139, Loss: 65.20102758292693, Learning Rate: 0.0005
Mean: 0.25713700884388113, Median: 0.2527350597119185, Num: 111
Epoch: 140, Loss: 63.169308444103564, Learning Rate: 0.0005
Epoch: 141, Loss: 63.91988294670381, Learning Rate: 0.0005
Mean: 0.2583086838432542, Median: 0.24874855061464066, Num: 111
Epoch: 142, Loss: 68.72166839277888, Learning Rate: 0.0005
Epoch: 143, Loss: 61.82493734934244, Learning Rate: 0.0005
Mean: 0.25650284982903965, Median: 0.23731691652720446, Num: 111
Epoch: 144, Loss: 68.42925108484475, Learning Rate: 0.0005
Epoch: 145, Loss: 63.90981000877289, Learning Rate: 0.0005
Mean: 0.25932392196676995, Median: 0.253587877683638, Num: 111
Epoch: 146, Loss: 69.99620350297675, Learning Rate: 0.0005
Epoch: 147, Loss: 67.92598086667348, Learning Rate: 0.0005
Mean: 0.25160831311012954, Median: 0.24476657693716283, Num: 111
Epoch: 148, Loss: 70.54709181728133, Learning Rate: 0.0005
Epoch: 149, Loss: 64.33294259496482, Learning Rate: 0.0005
Mean: 0.2571558393253864, Median: 0.2520933700794683, Num: 111
Epoch: 150, Loss: 59.44862215777478, Learning Rate: 0.0005
Epoch: 151, Loss: 70.82585827701062, Learning Rate: 0.0005
Mean: 0.2623755000051934, Median: 0.2477888550135654, Num: 111
Epoch: 152, Loss: 68.95653867147055, Learning Rate: 0.0005
Epoch: 153, Loss: 61.05605500577444, Learning Rate: 0.0005
Mean: 0.26168881319666104, Median: 0.27128177765773154, Num: 111
Epoch: 154, Loss: 62.21062879677279, Learning Rate: 0.0005
Epoch: 155, Loss: 60.311152101999305, Learning Rate: 0.0005
Mean: 0.2515824053863457, Median: 0.24673078839224732, Num: 111
Epoch: 156, Loss: 66.12101784671646, Learning Rate: 0.0005
Epoch: 157, Loss: 76.77411798683994, Learning Rate: 0.0005
Mean: 0.2552432388818432, Median: 0.2504341168610027, Num: 111
Epoch: 158, Loss: 62.523033383380934, Learning Rate: 0.0005
Epoch: 159, Loss: 61.621137779879284, Learning Rate: 0.0005
Mean: 0.26353468872238006, Median: 0.2564394621570918, Num: 111
Epoch: 160, Loss: 65.80952841977039, Learning Rate: 0.0005
Epoch: 161, Loss: 63.81165679104357, Learning Rate: 0.0005
Mean: 0.2630408186937818, Median: 0.25069908608421576, Num: 111
Epoch: 162, Loss: 63.77390126147902, Learning Rate: 0.0005
Epoch: 163, Loss: 62.0763767541173, Learning Rate: 0.0005
Mean: 0.2547505809051283, Median: 0.23884328033093832, Num: 111
Epoch: 164, Loss: 67.88483665650149, Learning Rate: 0.0005
Epoch: 165, Loss: 62.73252421689321, Learning Rate: 0.0005
Mean: 0.2618937728820136, Median: 0.24079546927687812, Num: 111
Epoch: 166, Loss: 62.76223210254347, Learning Rate: 0.0005
Epoch: 167, Loss: 64.4439066231969, Learning Rate: 0.0005
Mean: 0.2622106278892501, Median: 0.24889724157788254, Num: 111
Epoch: 168, Loss: 58.985944253852566, Learning Rate: 0.0005
Epoch: 169, Loss: 61.641275704625144, Learning Rate: 0.0005
Mean: 0.26472085793541206, Median: 0.24608424702996143, Num: 111
Epoch: 170, Loss: 64.37597892945071, Learning Rate: 0.0005
Epoch: 171, Loss: 63.62608867093741, Learning Rate: 0.0005
Mean: 0.25779890274117556, Median: 0.24269066084185079, Num: 111
Epoch: 172, Loss: 62.22435022836708, Learning Rate: 0.0005
Epoch: 173, Loss: 65.32502677641719, Learning Rate: 0.0005
Mean: 0.25919681488800195, Median: 0.2454093166675355, Num: 111
Epoch: 174, Loss: 58.53323035642325, Learning Rate: 0.0005
Epoch: 175, Loss: 65.36727038923516, Learning Rate: 0.0005
Mean: 0.26396485483670123, Median: 0.2603476143167559, Num: 111
Epoch: 176, Loss: 64.3736548825919, Learning Rate: 0.0005
Epoch: 177, Loss: 60.81124064434005, Learning Rate: 0.0005
Mean: 0.2609207407787684, Median: 0.24407143149924101, Num: 111
Epoch: 178, Loss: 61.63405442525105, Learning Rate: 0.0005
Epoch: 179, Loss: 67.70015313826411, Learning Rate: 0.0005
